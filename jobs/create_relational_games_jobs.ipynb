{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"relational_games\"\n",
    "out_dir = f'{job_directory}/.out'\n",
    "time_str = '00-04:00:00'\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_task = 8\n",
    "mem_per_cpu = 2\n",
    "n_gpus = 1\n",
    "# gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # all gpus are pretty good now\n",
    "project_dir = \"/home/ma2393/project/abstract_transformer/experiments/relational_games\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "model_params = [\n",
    "    # dict(d_model=128, dff=256, n_layers=1, sa=0, ra=4, ra_type='relational_attention', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=1, sa=2, ra=2, ra_type='relational_attention', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=1, sa=0, ra=4, ra_type='rca', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=1, sa=2, ra=2, ra_type='rca', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=1, sa=4, ra=0, ra_type='NA', symbol_type='NA'),\n",
    "    # dict(d_model=128, dff=256, n_layers=2, sa=0, ra=4, ra_type='relational_attention', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=2, sa=2, ra=2, ra_type='relational_attention', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=2, sa=0, ra=4, ra_type='rca', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=128, dff=256, n_layers=2, sa=2, ra=2, ra_type='rca', symbol_type='positional_symbols'),\n",
    "    # dict(d_model=144, dff=144*2, n_layers=2, sa=2, ra=0, ra_type='NA', symbol_type='NA'),\n",
    "    # dict(d_model=144, dff=144*2, n_layers=2, sa=4, ra=0, ra_type='NA', symbol_type='NA'),\n",
    "    # dict(d_model=144, dff=144*2, n_layers=2, sa=8, ra=0, ra_type='NA', symbol_type='NA'),\n",
    "    dict(d_model=128, dff=256, n_layers=2, sa=0, ra=2, n_relations=8, symmetric_rels=1, ra_type='relational_attention', symbol_type='positional_symbols'),\n",
    "    dict(d_model=128, dff=256, n_layers=2, sa=0, ra=2, n_relations=8, symmetric_rels=1, ra_type='relational_attention', symbol_type='symbolic_attention'),\n",
    "    # dict(d_model=128, dff=256, n_layers=2, sa=2, ra=2, ra_type='relational_attention', symbol_type='positional_symbols'),\n",
    "]\n",
    "\n",
    "# global config parameters\n",
    "patch_size = 12\n",
    "n_epochs = 50\n",
    "# max_steps = -1\n",
    "log_to_wandb = 1\n",
    "\n",
    "# tasks\n",
    "tasks = ['same', 'occurs', 'xoccurs', '1task_between', '1task_match_patt']\n",
    "train_sizes = {'1task_match_patt': '10_000 12_500 15_000 17_500 20_000',\n",
    "    **{task: '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500' for task in tasks if task != '1task_match_patt'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_params = []\n",
    "for task in tasks:\n",
    "    for mparams in model_params:\n",
    "        jobs_params.append({'task': task, 'compile': 1, 'train_sizes': train_sizes[task], **mparams})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task': 'same',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'positional_symbols'},\n",
       " {'task': 'same',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'symbolic_attention'},\n",
       " {'task': 'occurs',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'positional_symbols'},\n",
       " {'task': 'occurs',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'symbolic_attention'},\n",
       " {'task': 'xoccurs',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'positional_symbols'},\n",
       " {'task': 'xoccurs',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'symbolic_attention'},\n",
       " {'task': '1task_between',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'positional_symbols'},\n",
       " {'task': '1task_between',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '250 500 750 1_000 1_250 1_500 1_750 2_000 2_250 2_500',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'symbolic_attention'},\n",
       " {'task': '1task_match_patt',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '10_000 12_500 15_000 17_500 20_000',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'positional_symbols'},\n",
       " {'task': '1task_match_patt',\n",
       "  'compile': 1,\n",
       "  'train_sizes': '10_000 12_500 15_000 17_500 20_000',\n",
       "  'd_model': 128,\n",
       "  'dff': 256,\n",
       "  'n_layers': 2,\n",
       "  'sa': 0,\n",
       "  'ra': 2,\n",
       "  'n_relations': 8,\n",
       "  'symmetric_rels': 1,\n",
       "  'ra_type': 'relational_attention',\n",
       "  'symbol_type': 'symbolic_attention'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jobs\n",
    "created_jobs = []\n",
    "for params in jobs_params:\n",
    "\n",
    "    job_name = (f\"relational_games-{params['task']}-d{params['d_model']}-sa{params['sa']}-ra{params['ra']}-L{params['n_layers']}\"\n",
    "        f\"-ra_type_{params['ra_type']}-symbol_type_{params['symbol_type']}\")\n",
    "\n",
    "    job_file = os.path.join(job_directory, f\"{job_name}.job\")\n",
    "\n",
    "    with open(job_file, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={job_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{job_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-task={cpu_per_task}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={n_gpus}\\n\")\n",
    "        # fh.writelines(f\"#SBATCH --reservation=h100\\n\") # NOTE: using h100 reservation for noow\n",
    "        # fh.writelines(f\"#SBATCH -C {gpus_constraints}\\n\")# --gpus={n_gpus}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate abstract_transformer\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        # run python script\n",
    "        fh.writelines(f\"python eval_relational_games_learning_curve.py --task {params['task']} \")\n",
    "        fh.writelines(f\"--d_model {params['d_model']} --dff {params['dff']} --sa {params['sa']} --ra {params['ra']} --n_layers {params['n_layers']} \")\n",
    "        if 'n_relations' in params:\n",
    "            fh.writelines(f\"--n_relations {params['n_relations']} \")\n",
    "        if 'symmetric_rels' in params:\n",
    "            fh.writelines(f\"--symmetric_rels {params['symmetric_rels']} \")\n",
    "        fh.writelines(f\"--ra_type {params['ra_type']} --symbol_type {params['symbol_type']} --patch_size {patch_size} \")\n",
    "        fh.writelines(f\"--train_sizes {params['train_sizes']} --n_epochs {n_epochs} --log_to_wandb {log_to_wandb} --compile {params['compile']} \\n\")\n",
    "\n",
    "    created_jobs.append(job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relational_games/relational_games-same-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-same-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job',\n",
       " 'relational_games/relational_games-occurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-occurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job',\n",
       " 'relational_games/relational_games-xoccurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-xoccurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job',\n",
       " 'relational_games/relational_games-1task_between-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-1task_between-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job',\n",
       " 'relational_games/relational_games-1task_match_patt-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-1task_match_patt-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relational_games/relational_games-same-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-same-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job',\n",
       " 'relational_games/relational_games-occurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-occurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job',\n",
       " 'relational_games/relational_games-xoccurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_positional_symbols.job',\n",
       " 'relational_games/relational_games-xoccurs-d128-sa0-ra2-L2-ra_type_relational_attention-symbol_type_symbolic_attention.job']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for j in created_jobs if (any((task in j) for task in ['occurs', 'same', 'xoccurs']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 25387\n",
      "Submitted batch job 25388\n",
      "Submitted batch job 25389\n",
      "Submitted batch job 25390\n",
      "Submitted batch job 25391\n",
      "Submitted batch job 25392\n",
      "Submitted batch job 25393\n",
      "Submitted batch job 25394\n",
      "Submitted batch job 25395\n",
      "Submitted batch job 25396\n",
      "Submitted batch job 25397\n",
      "Submitted batch job 25398\n",
      "Submitted batch job 25399\n",
      "Submitted batch job 25400\n",
      "Submitted batch job 25401\n",
      "Submitted batch job 25402\n",
      "Submitted batch job 25403\n",
      "Submitted batch job 25404\n",
      "Submitted batch job 25405\n",
      "Submitted batch job 25406\n",
      "Submitted batch job 25407\n",
      "Submitted batch job 25408\n",
      "Submitted batch job 25409\n",
      "Submitted batch job 25410\n",
      "Submitted batch job 25411\n",
      "Submitted batch job 25412\n",
      "Submitted batch job 25413\n",
      "Submitted batch job 25414\n",
      "Submitted batch job 25415\n",
      "Submitted batch job 25416\n",
      "Submitted batch job 25417\n",
      "Submitted batch job 25418\n",
      "Submitted batch job 25419\n",
      "Submitted batch job 25420\n",
      "Submitted batch job 25421\n",
      "Submitted batch job 25422\n",
      "Submitted batch job 25423\n",
      "Submitted batch job 25424\n",
      "Submitted batch job 25425\n",
      "Submitted batch job 25426\n",
      "Submitted batch job 25427\n",
      "Submitted batch job 25428\n",
      "Submitted batch job 25429\n",
      "Submitted batch job 25430\n",
      "Submitted batch job 25431\n",
      "Submitted batch job 25432\n",
      "Submitted batch job 25433\n",
      "Submitted batch job 25434\n",
      "Submitted batch job 25435\n",
      "Submitted batch job 25436\n"
     ]
    }
   ],
   "source": [
    "confirm = input(\"CONTINUE TO RUN ALL JOBS? (enter 'Y' or 'y')\")\n",
    "if confirm in (\"Y\", \"y\"):\n",
    "    for trial in range(n_trials):\n",
    "        for job in created_jobs:\n",
    "            os.system(f'sbatch {job}')\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print(\"did not run jobs since you did not confirm.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
