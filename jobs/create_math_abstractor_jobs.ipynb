{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"math\"\n",
    "out_dir = f'{job_directory}/.out'\n",
    "time_str = '00-12:00:00'\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_task = 8\n",
    "mem_per_cpu = 5\n",
    "n_gpus = 1\n",
    "# gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # all gpus are pretty good now\n",
    "project_dir = \"/home/ma2393/project/abstract_transformer/experiments/math\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params of individual jobs\n",
    "\n",
    "model_params = [\n",
    "    dict(n_layers=1, d_model=128, n_heads=8, dff=128*2, symbol_type='symbolic_attention'),\n",
    "    dict(n_layers=2, d_model=128, n_heads=8, dff=128*2, symbol_type='symbolic_attention')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks =  ['algebra__linear_1d', 'polynomials__add', 'polynomials__expand', 'calculus__differentiate', 'algebra__sequence_next_term']\n",
    "n_epochs = 100\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n_layers': 1, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 1, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 1, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 1, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 1, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 2, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 2, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 2, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 2, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'n_layers': 2, 'd_model': 128, 'n_heads': 8, 'dff': 256, 'symbol_type': 'symbolic_attention', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "jobs_params = []\n",
    "for model_p, task in itertools.product(model_params, tasks):\n",
    "    jobs_params.append({**model_p, 'task': task, 'n_epochs': n_epochs, 'batch_size': batch_size})\n",
    "print(jobs_params)\n",
    "print(len(jobs_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jobs\n",
    "created_jobs = []\n",
    "for params in jobs_params:\n",
    "\n",
    "    job_name = f\"math-abstractor--{params['task']}-n_layers={params['n_layers']}\"\n",
    "\n",
    "    job_file = os.path.join(job_directory, f\"{job_name}.job\")\n",
    "\n",
    "    with open(job_file, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={job_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{job_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-task={cpu_per_task}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={n_gpus}\\n\")\n",
    "        # fh.writelines(f\"#SBATCH -C {gpus_constraints}\\n\")# --gpus={n_gpus}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate abstract_transformer\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        # run python script\n",
    "        fh.writelines(f\"python train_abstractor_model.py \")\n",
    "        fh.writelines(f\"--task {params['task']} --n_epochs {params['n_epochs']} --batch_size {params['batch_size']} \")\n",
    "        fh.writelines(f\"--d_model {params['d_model']} --dff {params['dff']} --symbol_type {params['symbol_type']} \")\n",
    "        fh.writelines(f\"--n_layers {params['n_layers']} --n_heads {params['n_heads']} \")\n",
    "        fh.writelines('\\n')\n",
    "    created_jobs.append(job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trails = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 25814\n",
      "Submitted batch job 25815\n",
      "Submitted batch job 25816\n",
      "Submitted batch job 25817\n",
      "Submitted batch job 25818\n",
      "Submitted batch job 25819\n",
      "Submitted batch job 25820\n",
      "Submitted batch job 25821\n",
      "Submitted batch job 25822\n",
      "Submitted batch job 25823\n",
      "Submitted batch job 25824\n",
      "Submitted batch job 25825\n",
      "Submitted batch job 25826\n",
      "Submitted batch job 25827\n",
      "Submitted batch job 25828\n",
      "Submitted batch job 25829\n",
      "Submitted batch job 25830\n",
      "Submitted batch job 25831\n",
      "Submitted batch job 25832\n",
      "Submitted batch job 25833\n",
      "Submitted batch job 25834\n",
      "Submitted batch job 25835\n",
      "Submitted batch job 25836\n",
      "Submitted batch job 25837\n",
      "Submitted batch job 25838\n",
      "Submitted batch job 25839\n",
      "Submitted batch job 25840\n",
      "Submitted batch job 25841\n",
      "Submitted batch job 25842\n",
      "Submitted batch job 25843\n",
      "Submitted batch job 25844\n",
      "Submitted batch job 25845\n",
      "Submitted batch job 25846\n",
      "Submitted batch job 25847\n",
      "Submitted batch job 25848\n",
      "Submitted batch job 25849\n",
      "Submitted batch job 25850\n",
      "Submitted batch job 25851\n",
      "Submitted batch job 25852\n",
      "Submitted batch job 25853\n"
     ]
    }
   ],
   "source": [
    "proceed = input(\"Run all jobs? [Y/y or N/n]\")\n",
    "if proceed in ('Y', 'y'):\n",
    "    for i in range(n_trails):\n",
    "        for job in created_jobs:\n",
    "            os.system(f'sbatch {job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['math/math-abstractor--algebra__linear_1d-n_layers=1.job',\n",
       " 'math/math-abstractor--polynomials__add-n_layers=1.job',\n",
       " 'math/math-abstractor--polynomials__expand-n_layers=1.job',\n",
       " 'math/math-abstractor--calculus__differentiate-n_layers=1.job',\n",
       " 'math/math-abstractor--algebra__sequence_next_term-n_layers=1.job',\n",
       " 'math/math-abstractor--algebra__linear_1d-n_layers=2.job',\n",
       " 'math/math-abstractor--polynomials__add-n_layers=2.job',\n",
       " 'math/math-abstractor--polynomials__expand-n_layers=2.job',\n",
       " 'math/math-abstractor--calculus__differentiate-n_layers=2.job',\n",
       " 'math/math-abstractor--algebra__sequence_next_term-n_layers=2.job']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_jobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
