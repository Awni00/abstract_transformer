{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"math\"\n",
    "out_dir = f'{job_directory}/.out'\n",
    "time_str = '00-24:00:00'\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_task = 8\n",
    "mem_per_cpu = 2\n",
    "n_gpus = 1\n",
    "# gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # all gpus are pretty good now\n",
    "project_dir = \"/home/ma2393/project/abstract_transformer/experiments/math\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params of individual jobs\n",
    "\n",
    "model_params = [\n",
    "    # dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=144, dff=144*2, symbol_type='NA', ra_type='NA'),\n",
    "    # dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=128*2, symbol_type='NA', ra_type='NA'),\n",
    "    # dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=128*2, symbol_type='NA', ra_type='NA'),\n",
    "    # dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=144, dff=144*2, symbol_type='NA', ra_type='NA'),\n",
    "    # dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=144, dff=144*2, symbol_type='NA', ra_type='NA'),\n",
    "\n",
    "    dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=128, dff=256, symbol_type='symbolic_attention', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=128, dff=256, symbol_type='symbolic_attention', ra_type='relational_attention'),\n",
    "    # dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    # dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=256, symbol_type='symbolic_attention', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=256, symbol_type='symbolic_attention', ra_type='relational_attention'),\n",
    "    # dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    # dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=256, symbol_type='symbolic_attention', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=256, symbol_type='symbolic_attention', ra_type='relational_attention'),\n",
    "    # dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    # dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "]\n",
    "\n",
    "for model_param in model_params:\n",
    "    model_param['update_symbols_each_layer'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define params of individual jobs\n",
    "\n",
    "# # variant with pos_enc = RoPE\n",
    "model_params = [\n",
    "    dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=144, dff=144*2, symbol_type='NA', ra_type='NA', pos_enc_type='RoPE'),\n",
    "    dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=128*2, symbol_type='NA', ra_type='NA'),\n",
    "    dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=128*2, symbol_type='NA', ra_type='NA'),\n",
    "#     dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=144, dff=144*2, symbol_type='NA', ra_type='NA', pos_enc_type='RoPE'),\n",
    "#     dict(e_sa=8, e_ra=0, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=144, dff=144*2, symbol_type='NA', ra_type='NA', pos_enc_type='RoPE'),\n",
    "\n",
    "    dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention', pos_enc_type='RoPE'),\n",
    "#     # dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=2, d_n_layers=2, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention', pos_enc_type='RoPE'),\n",
    "#     # dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=3, d_n_layers=3, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "    dict(e_sa=4, e_ra=4, d_sa=4, d_ra=4, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention', pos_enc_type='RoPE'),\n",
    "#     # dict(e_sa=4, e_ra=4, d_sa=8, d_ra=0, d_cross=8, e_n_layers=4, d_n_layers=4, d_model=128, dff=256, symbol_type='position_relative', ra_type='relational_attention'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks =  ['algebra__linear_1d', 'polynomials__add', 'polynomials__expand', 'calculus__differentiate', 'algebra__sequence_next_term']\n",
    "n_epochs = 100\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 144, 'dff': 288, 'symbol_type': 'NA', 'ra_type': 'NA', 'pos_enc_type': 'RoPE', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 144, 'dff': 288, 'symbol_type': 'NA', 'ra_type': 'NA', 'pos_enc_type': 'RoPE', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 144, 'dff': 288, 'symbol_type': 'NA', 'ra_type': 'NA', 'pos_enc_type': 'RoPE', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 144, 'dff': 288, 'symbol_type': 'NA', 'ra_type': 'NA', 'pos_enc_type': 'RoPE', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 144, 'dff': 288, 'symbol_type': 'NA', 'ra_type': 'NA', 'pos_enc_type': 'RoPE', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 8, 'e_ra': 0, 'd_sa': 8, 'd_ra': 0, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'NA', 'ra_type': 'NA', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 2, 'd_n_layers': 2, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 3, 'd_n_layers': 3, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'algebra__linear_1d', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'polynomials__add', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'polynomials__expand', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'calculus__differentiate', 'n_epochs': 100, 'batch_size': 512}, {'e_sa': 4, 'e_ra': 4, 'd_sa': 4, 'd_ra': 4, 'd_cross': 8, 'e_n_layers': 4, 'd_n_layers': 4, 'd_model': 128, 'dff': 256, 'symbol_type': 'position_relative', 'ra_type': 'relational_attention', 'pos_enc_type': 'RoPE', 'task': 'algebra__sequence_next_term', 'n_epochs': 100, 'batch_size': 512}]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "jobs_params = []\n",
    "for model_p, task in itertools.product(model_params, tasks):\n",
    "    jobs_params.append({**model_p, 'task': task, 'n_epochs': n_epochs, 'batch_size': batch_size})\n",
    "print(jobs_params)\n",
    "print(len(jobs_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jobs\n",
    "created_jobs = []\n",
    "for params in jobs_params:\n",
    "\n",
    "    job_name = f\"math--{params['task']}-e_sa{params['e_sa']}-e_ra{params['e_ra']}-d_sa{params['d_sa']}-d_ra{params['d_ra']}-el{params['e_n_layers']}-dl{params['d_n_layers']}-ra_type_{params['ra_type']}-symbol_type_{params['symbol_type']}\"\n",
    "    if 'update_symbols_each_layer' in params:\n",
    "        job_name += f\"-update_symbols_each_layer{params['update_symbols_each_layer']}\"\n",
    "    if 'pos_enc_type' in params:\n",
    "        job_name += f\"-pos_enc{params['pos_enc_type']}\"\n",
    "\n",
    "    job_file = os.path.join(job_directory, f\"{job_name}.job\")\n",
    "\n",
    "    with open(job_file, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={job_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{job_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-task={cpu_per_task}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={n_gpus}\\n\")\n",
    "        # fh.writelines(f\"#SBATCH -C {gpus_constraints}\\n\")# --gpus={n_gpus}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate abstract_transformer\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        # run python script\n",
    "        fh.writelines(f\"python train_model.py \")\n",
    "        fh.writelines(f\"--task {params['task']} --n_epochs {params['n_epochs']} --batch_size {params['batch_size']} \")\n",
    "        fh.writelines(f\"--e_sa {params['e_sa']} --e_ra {params['e_ra']} --d_sa {params['d_sa']} --d_ra {params['d_ra']} --d_cross {params['d_cross']} \")\n",
    "        fh.writelines(f\"--d_model {params['d_model']} --dff {params['dff']} --ra_type {params['ra_type']} --symbol_type {params['symbol_type']} \")\n",
    "        if 'pos_enc_type' in params:\n",
    "            fh.writelines(f\"--pos_enc_type {params['pos_enc_type']} \")\n",
    "        if 'update_symbols_each_layer' in params:\n",
    "            fh.writelines(f\"--update_symbols_each_layer {params['update_symbols_each_layer']} \")\n",
    "        fh.writelines(f\"--e_n_layers {params['e_n_layers']} --d_n_layers {params['d_n_layers']}\\n\")\n",
    "\n",
    "    created_jobs.append(job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 77415\n",
      "Submitted batch job 77416\n",
      "Submitted batch job 77417\n",
      "Submitted batch job 77418\n",
      "Submitted batch job 77419\n",
      "Submitted batch job 77420\n",
      "Submitted batch job 77421\n",
      "Submitted batch job 77422\n",
      "Submitted batch job 77423\n",
      "Submitted batch job 77424\n",
      "Submitted batch job 77425\n",
      "Submitted batch job 77426\n",
      "Submitted batch job 77427\n",
      "Submitted batch job 77428\n",
      "Submitted batch job 77429\n",
      "Submitted batch job 77430\n",
      "Submitted batch job 77431\n",
      "Submitted batch job 77432\n",
      "Submitted batch job 77433\n",
      "Submitted batch job 77434\n",
      "Submitted batch job 77435\n",
      "Submitted batch job 77436\n",
      "Submitted batch job 77437\n",
      "Submitted batch job 77438\n",
      "Submitted batch job 77439\n",
      "Submitted batch job 77440\n",
      "Submitted batch job 77441\n",
      "Submitted batch job 77442\n",
      "Submitted batch job 77443\n",
      "Submitted batch job 77444\n"
     ]
    }
   ],
   "source": [
    "n_trials = 1\n",
    "\n",
    "proceed = input(\"Run all jobs? [Y/y or N/n]\")\n",
    "if proceed in ('Y', 'y'):\n",
    "    for job in created_jobs:\n",
    "        for _ in range(n_trials):\n",
    "            os.system(f'sbatch {job}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['math/math--algebra__linear_1d-e_sa8-e_ra0-d_sa8-d_ra0-el2-dl2-ra_type_NA-symbol_type_NA-pos_encRoPE.job',\n",
       " 'math/math--polynomials__add-e_sa8-e_ra0-d_sa8-d_ra0-el2-dl2-ra_type_NA-symbol_type_NA-pos_encRoPE.job',\n",
       " 'math/math--polynomials__expand-e_sa8-e_ra0-d_sa8-d_ra0-el2-dl2-ra_type_NA-symbol_type_NA-pos_encRoPE.job',\n",
       " 'math/math--calculus__differentiate-e_sa8-e_ra0-d_sa8-d_ra0-el2-dl2-ra_type_NA-symbol_type_NA-pos_encRoPE.job',\n",
       " 'math/math--algebra__sequence_next_term-e_sa8-e_ra0-d_sa8-d_ra0-el2-dl2-ra_type_NA-symbol_type_NA-pos_encRoPE.job',\n",
       " 'math/math--algebra__linear_1d-e_sa8-e_ra0-d_sa8-d_ra0-el3-dl3-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--polynomials__add-e_sa8-e_ra0-d_sa8-d_ra0-el3-dl3-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--polynomials__expand-e_sa8-e_ra0-d_sa8-d_ra0-el3-dl3-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--calculus__differentiate-e_sa8-e_ra0-d_sa8-d_ra0-el3-dl3-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--algebra__sequence_next_term-e_sa8-e_ra0-d_sa8-d_ra0-el3-dl3-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--algebra__linear_1d-e_sa8-e_ra0-d_sa8-d_ra0-el4-dl4-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--polynomials__add-e_sa8-e_ra0-d_sa8-d_ra0-el4-dl4-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--polynomials__expand-e_sa8-e_ra0-d_sa8-d_ra0-el4-dl4-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--calculus__differentiate-e_sa8-e_ra0-d_sa8-d_ra0-el4-dl4-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--algebra__sequence_next_term-e_sa8-e_ra0-d_sa8-d_ra0-el4-dl4-ra_type_NA-symbol_type_NA.job',\n",
       " 'math/math--algebra__linear_1d-e_sa4-e_ra4-d_sa4-d_ra4-el2-dl2-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--polynomials__add-e_sa4-e_ra4-d_sa4-d_ra4-el2-dl2-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--polynomials__expand-e_sa4-e_ra4-d_sa4-d_ra4-el2-dl2-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--calculus__differentiate-e_sa4-e_ra4-d_sa4-d_ra4-el2-dl2-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--algebra__sequence_next_term-e_sa4-e_ra4-d_sa4-d_ra4-el2-dl2-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--algebra__linear_1d-e_sa4-e_ra4-d_sa4-d_ra4-el3-dl3-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--polynomials__add-e_sa4-e_ra4-d_sa4-d_ra4-el3-dl3-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--polynomials__expand-e_sa4-e_ra4-d_sa4-d_ra4-el3-dl3-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--calculus__differentiate-e_sa4-e_ra4-d_sa4-d_ra4-el3-dl3-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--algebra__sequence_next_term-e_sa4-e_ra4-d_sa4-d_ra4-el3-dl3-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--algebra__linear_1d-e_sa4-e_ra4-d_sa4-d_ra4-el4-dl4-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--polynomials__add-e_sa4-e_ra4-d_sa4-d_ra4-el4-dl4-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--polynomials__expand-e_sa4-e_ra4-d_sa4-d_ra4-el4-dl4-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--calculus__differentiate-e_sa4-e_ra4-d_sa4-d_ra4-el4-dl4-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job',\n",
       " 'math/math--algebra__sequence_next_term-e_sa4-e_ra4-d_sa4-d_ra4-el4-dl4-ra_type_relational_attention-symbol_type_position_relative-pos_encRoPE.job']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_jobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
