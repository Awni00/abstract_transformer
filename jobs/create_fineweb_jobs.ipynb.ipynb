{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"fineweb_edu\"\n",
    "out_dir = f'{job_directory}/.out'\n",
    "job_duration = '00-48:00:00'\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_gpu = 8\n",
    "mem_per_cpu = 16\n",
    "# n_gpus = 1\n",
    "# gpus_constraints = '\"h100|a100\"' # all gpus are pretty good now\n",
    "project_dir = \"/home/ma2393/project/abstract_transformer/experiments/fineweb\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "T = 1024\n",
    "total_batch_size = 524_288\n",
    "\n",
    "model_params = [\n",
    "    # dict(d_model=1024, n_layers=24, sa=8, ra=8,\n",
    "    #     sym_attn_n_symbols=1024, sym_attn_n_heads=8,\n",
    "    #     n_kv_heads=1, B=8,\n",
    "    #     n_gpus=2, gpus_constraints= '\"h100|a100\"',\n",
    "    #     param_ct_string='325M'),\n",
    "    # dict(d_model=1024, n_layers=24, sa=8, ra=8,\n",
    "    #     sym_attn_n_symbols=1024, sym_attn_n_heads=8,\n",
    "    #     n_kv_heads=2, B=8,\n",
    "    #     n_gpus=2, gpus_constraints= '\"h100|a100\"',\n",
    "    #     param_ct_string='330M'),\n",
    "    # dict(d_model=1024, n_layers=24, sa=8, ra=8,\n",
    "    #     sym_attn_n_symbols=1024, sym_attn_n_heads=8,\n",
    "    #     n_kv_heads=4, B=8,\n",
    "    #     n_gpus=2, gpus_constraints= '\"h100|a100\"',\n",
    "    #     param_ct_string='343M'),\n",
    "    dict(d_model=1024, n_layers=24, sa=8, ra=8,\n",
    "        sym_attn_n_symbols=1024, sym_attn_n_heads=8,\n",
    "        share_attn_params=1, B=8,\n",
    "        n_gpus=2, gpus_constraints= '\"h100|a100\"',\n",
    "        param_ct_string='343M'),\n",
    "]\n",
    "\n",
    "jobs_params = []\n",
    "for mparams in model_params:\n",
    "    # compute run name\n",
    "    if mparams['ra'] > 0:\n",
    "        run_name = f\"DAT-sa{mparams['sa']}-ra{mparams['ra']}\"\n",
    "        if 'n_relations' in mparams:\n",
    "            run_name += f\"-nr{mparams['n_relations']}\"\n",
    "        if 'share_attn_params' in mparams:\n",
    "            run_name += f\"-sharedattn{mparams['share_attn_params']}\"\n",
    "        if 'sym_attn_n_symbols' in mparams:\n",
    "            run_name += f\"-ns{mparams['sym_attn_n_symbols']}\"\n",
    "        if 'sym_attn_n_heads' in mparams:\n",
    "            run_name += f\"-sh{mparams['sym_attn_n_heads']}\"\n",
    "        if 'shared_symbol_retriever' in mparams:\n",
    "            run_name += f\"-ssr{mparams['shared_symbol_retriever']}\"\n",
    "        if 'weight_tie_symbol_library' in mparams:\n",
    "            run_name += f\"-wt{mparams['weight_tie_symbol_library']}\"\n",
    "        if 'trainable_symbols' in mparams:\n",
    "            run_name += f\"-ts{mparams['trainable_symbols']}\"\n",
    "    else:\n",
    "        run_name = f'T-sa{mparams[\"sa\"]}'\n",
    "    if 'n_kv_heads' in mparams:\n",
    "        run_name += f'-nkvh{mparams[\"n_kv_heads\"]}'\n",
    "    if 'param_ct_string' in mparams:\n",
    "        run_name += f'-{mparams[\"param_ct_string\"]}'\n",
    "\n",
    "    jobs_params.append({**mparams, 'run_name': run_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'d_model': 1024,\n",
       "  'n_layers': 24,\n",
       "  'sa': 8,\n",
       "  'ra': 8,\n",
       "  'sym_attn_n_symbols': 1024,\n",
       "  'sym_attn_n_heads': 8,\n",
       "  'share_attn_params': 1,\n",
       "  'B': 8,\n",
       "  'n_gpus': 2,\n",
       "  'gpus_constraints': '\"h100|a100\"',\n",
       "  'param_ct_string': '343M',\n",
       "  'run_name': 'DAT-sa8-ra8-sharedattn1-ns1024-sh8-343M'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global config parameters\n",
    "n_epochs = 1\n",
    "max_steps = -1\n",
    "log_to_wandb = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jobs\n",
    "created_jobs = []\n",
    "for params in jobs_params:\n",
    "\n",
    "    job_file = os.path.join(job_directory, f\"{params['run_name']}.job\")\n",
    "\n",
    "    with open(job_file, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={params['run_name']}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{params['run_name']}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-gpu={cpu_per_gpu}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={job_duration}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={params['n_gpus']}\\n\")\n",
    "        if 'gpus_constraints' in params:\n",
    "            fh.writelines(f\"#SBATCH --constraint={params['gpus_constraints']}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate abstract_transformer\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # run python script\n",
    "        if params['n_gpus'] > 1:\n",
    "            fh.writelines(f\"torchrun --standalone --nproc_per_node={params['n_gpus']} pretrain.py \\\\\\n\")\n",
    "        else:\n",
    "            fh.writelines(f\"python pretrain.py \\\\\\n\")\n",
    "\n",
    "        fh.writelines(f\"\\t--d_model {params['d_model']} --sa {params['sa']} --ra {params['ra']} --n_layers {params['n_layers']} \\\\\\n\")\n",
    "        if 'n_relations' in params:\n",
    "            fh.writelines(f\"\\t--n_relations {params['n_relations']} \\\\\\n\")\n",
    "        if 'sym_attn_n_symbols' in params:\n",
    "            fh.writelines(f\"\\t--sym_attn_n_symbols {params['sym_attn_n_symbols']} --sym_attn_n_heads {params['sym_attn_n_heads']} \\\\\\n\")\n",
    "        if 'n_kv_heads' in params:\n",
    "            fh.writelines(f\"\\t--n_kv_heads {params['n_kv_heads']} \\\\\\n\")\n",
    "        if 'shared_symbol_retriever' in params:\n",
    "            fh.writelines(f\"\\t--shared_symbol_retriever {params['shared_symbol_retriever']} --weight_tie_symbol_library {params['weight_tie_symbol_library']} \")\n",
    "            fh.writelines(f\"--trainable_symbols {params['trainable_symbols']} \\\\\\n\")\n",
    "        fh.writelines(f\"\\t--T {T} --B {params['B']} --total_batch_size {total_batch_size} \\\\\\n\")\n",
    "        fh.writelines(f\"\\t--wandb_log 1 --run_name {params['run_name']} --job_duration {job_duration} \\\\\\n\")\n",
    "\n",
    "    created_jobs.append(job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fineweb_edu/DAT-sa8-ra8-sharedattn1-ns1024-sh8-343M.job']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 22918\n"
     ]
    }
   ],
   "source": [
    "confirm = input(\"CONTINUE TO RUN ALL JOBS?\")\n",
    "if confirm == 'y':\n",
    "    for job in created_jobs:\n",
    "        os.system(f'sbatch {job}')\n",
    "else:\n",
    "    print('JOBS NOT SUBMITTED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
