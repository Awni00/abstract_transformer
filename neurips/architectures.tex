
\section{Architectures: Encoder, Decoder, Causality, etc.}
\aanote{is ``abstract Transformer'' a good name? or should we stick with ``Abstractor''?}

Define algorithms compactly side by side.

\textbf{Sample complexity.} Note that sample complexity scales the same as self-attention with the same dependence on sequence length. Mention difference in dependence on 

\aanote[inline, nomargin]{TODO: in disucssion or ``limations'' section: mention that our current implementation is significantly slower than existing implementations of the Transformer. This is partly because DisRCAv2 involves additional computational steps, but is also because our implementation is unable to use hardware-aware optimizations such as Flash Attention. Considerable effort has been spent optimizing implementations of self-attention and Transformers. Also, our implementation may be ``naive,'' and there may be some optimizations possible without needing to program new GPU/cuda kernels, etc. We leave optimization of the implementation, including potentially hardware-aware implementations similar FlashAttention to future work.}
