@misc{altabaa2024approximation,
      title={Approximation of relation functions and attention mechanisms}, 
      author={Awni Altabaa and John Lafferty},
      year={2024},
      eprint={2402.08856},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{altabaa2024abstractors,
title={Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers},
author={Awni Altabaa and Taylor Whittington Webb and Jonathan D. Cohen and John Lafferty},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=XNa6r6ZjoB}
}

@inproceedings{altabaaAbstractorsRelationalCrossattention2023a,
  title = {Abstractors and Relational Cross-Attention: {{An}} Inductive Bias for Explicit Relational Reasoning in {{Transformers}}},
  shorttitle = {Abstractors and Relational Cross-Attention},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Altabaa, Awni and Webb, Taylor Whittington and Cohen, Jonathan D. and Lafferty, John},
  year = {2023},
  month = oct,
  urldate = {2024-05-15},
  langid = {english}
}

@misc{altabaaApproximationRelationFunctions2024a,
  title = {Approximation of Relation Functions and Attention Mechanisms},
  author = {Altabaa, Awni and Lafferty, John},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08856},
  eprint = {2402.08856},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08856},
  urldate = {2024-05-15},
  archiveprefix = {arxiv}
}

@misc{altabaaLearningHierarchicalRelational2024a,
  title = {Learning {{Hierarchical Relational Representations}} through {{Relational Convolutions}}},
  author = {Altabaa, Awni and Lafferty, John},
  year = {2024},
  month = feb,
  number = {arXiv:2310.03240},
  eprint = {2310.03240},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03240},
  urldate = {2024-05-15},
  archiveprefix = {arxiv}
}

@article{boix-adseraWhenCanTransformers,
  title = {When Can Transformers Reason with Abstract Symbols?},
  author = {{Boix-Adser{\`a}}, Enric and Saremi, Omid and Abbe, Emmanuel and Bengio, Samy and Littwin, Etai and Susskind, Joshua},
  langid = {english}
}

@inproceedings{dosovitskiyImageWorth16x162020,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@misc{eldanTinyStoriesHowSmall2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  year = {2023},
  month = may,
  number = {arXiv:2305.07759},
  eprint = {2305.07759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07759},
  urldate = {2024-05-16},
  archiveprefix = {arxiv}
}

@misc{kazemnejadImpactPositionalEncoding2023,
  title = {The {{Impact}} of {{Positional Encoding}} on {{Length Generalization}} in {{Transformers}}},
  author = {Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
  year = {2023},
  month = may,
  number = {arXiv:2305.19466},
  eprint = {2305.19466},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-17},
  archiveprefix = {arxiv}
}

@misc{kergNeuralArchitectureInductive2022,
  title = {On {{Neural Architecture Inductive Biases}} for {{Relational Tasks}}},
  author = {Kerg, Giancarlo and Mittal, Sarthak and Rolnick, David and Bengio, Yoshua and Richards, Blake and Lajoie, Guillaume},
  year = {2022},
  month = jun,
  number = {arXiv:2206.05056},
  eprint = {2206.05056},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.05056},
  urldate = {2022-09-13},
  archiveprefix = {arxiv}
}

@misc{locatelloObjectCentricLearningSlot2020,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  month = oct,
  number = {arXiv:2006.15055},
  eprint = {2006.15055},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-18},
  archiveprefix = {arxiv}
}

@inproceedings{santoroRelationalRecurrentNeural2018,
  title = {Relational Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-11-11}
}

@misc{santoroSimpleNeuralNetwork2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  month = jun,
  number = {arXiv:1706.01427},
  eprint = {1706.01427},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-11-11},
  archiveprefix = {arxiv}
}

@inproceedings{saxtonAnalyzingMathematicalReasoning2019,
  title={Analysing Mathematical Reasoning Abilities of Neural Models},
  author={David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=H1gR5iR5FX},
}

@misc{schlagEnhancingTransformerExplicit2020,
  title = {Enhancing the {{Transformer}} with {{Explicit Relational Encoding}} for {{Math Problem Solving}}},
  author = {Schlag, Imanol and Smolensky, Paul and Fernandez, Roland and Jojic, Nebojsa and Schmidhuber, J{\"u}rgen and Gao, Jianfeng},
  year = {2020},
  month = nov,
  number = {arXiv:1910.06611},
  eprint = {1910.06611},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1910.06611},
  urldate = {2023-01-16},
  archiveprefix = {arxiv}
}


@inproceedings{shanahanExplicitlyRelationalNeurala,
  title = 	 {An Explicitly Relational Neural Network Architecture},
  author =       {Shanahan, Murray and Nikiforou, Kyriacos and Creswell, Antonia and Kaplanis, Christos and Barrett, David and Garnelo, Marta},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = 	 {2020},
}


@inproceedings{shawSelfAttentionRelativePosition2018b,
  title = {Self-{{Attention}} with {{Relative Position Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year = {2018},
  month = jun,
  pages = {464--468},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-2074},
  urldate = {2024-05-16}
}

@misc{shazeerGLUVariantsImprove2020,
  title = {{{GLU Variants Improve Transformer}}},
  author = {Shazeer, Noam},
  year = {2020},
  month = feb,
  number = {arXiv:2002.05202},
  eprint = {2002.05202},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-16},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{smolenskyNeurocompositionalComputingCentral2022,
  title = {Neurocompositional Computing: {{From}} the {{Central Paradox}} of {{Cognition}} to a New Generation of {{AI}} Systems},
  shorttitle = {Neurocompositional Computing},
  author = {Smolensky, Paul and McCoy, R. Thomas and Fernandez, Roland and Goldrick, Matthew and Gao, Jianfeng},
  year = {2022},
  month = may,
  number = {arXiv:2205.01128},
  eprint = {2205.01128},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01128},
  urldate = {2023-01-13},
  archiveprefix = {arxiv}
}

@misc{suRoFormerEnhancedTransformer2023,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-14},
  archiveprefix = {arxiv}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-12-06},
  archiveprefix = {arxiv}
}

@article{vaswani2017attention,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@misc{webbEmergentSymbolsBinding2021,
  title = {Emergent {{Symbols}} through {{Binding}} in {{External Memory}}},
  author = {Webb, Taylor W. and Sinha, Ishan and Cohen, Jonathan D.},
  year = {2021},
  month = mar,
  number = {arXiv:2012.14601},
  eprint = {2012.14601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.14601},
  urldate = {2022-09-13},
  archiveprefix = {arxiv}
}

@article{webbRelationalBottleneckInductive2024,
  title = {The Relational Bottleneck as an Inductive Bias for Efficient Abstraction},
  author = {Webb, Taylor W. and Frankland, Steven M. and Altabaa, Awni and Segert, Simon and Krishnamurthy, Kamesh and Campbell, Declan and Russin, Jacob and Giallanza, Tyler and O'Reilly, Randall and Lafferty, John and Cohen, Jonathan D.},
  year = {2024},
  month = may,
  journal = {Trends in Cognitive Sciences},
  pages = {S1364661324000809},
  issn = {13646613},
  doi = {10.1016/j.tics.2024.04.001},
  urldate = {2024-05-15},
  langid = {english}
}

@misc{whittingtonRelatingTransformersModels2022,
  title = {Relating Transformers to Models and Neural Representations of the Hippocampal Formation},
  author = {Whittington, James C. R. and Warren, Joseph and Behrens, Timothy E. J.},
  year = {2022},
  month = mar,
  number = {arXiv:2112.04035},
  eprint = {2112.04035},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-01-05},
  archiveprefix = {arxiv}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}


@InProceedings{duGLaM2022,
  title = 	 {{GL}a{M}: Efficient Scaling of Language Models with Mixture-of-Experts},
  author =       {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5547--5569},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/du22c/du22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/du22c.html},
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/N19-1423},
  urldate = {2024-05-17}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {OpenAI}
}

@inproceedings{inanTyingWordVectors2016,
  title={Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling},
  author={Hakan Inan and Khashayar Khosravi and Richard Socher},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=r1aPbsFle}
}