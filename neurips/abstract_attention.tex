\section{Abstract Attention: Mixture of specialized attention heads}
\aanote{is ``abstract attention'' a good name?}

\begin{enumerate}
  \item The core component of standard Transformers is self-attention (and cross-attention, in encoder-decoder Transformers). Intuitively, what self-attention does is to route information between elements in the sequence. A Transformer block is simply composing self-attention with a feedforward network to perform independent element-wise processing (modulo technicalities such as layer normalization, residual connections, etc).
  \item Simply stacking these simple blocks leads to remarkable scalability in various domains, including language, vision, etc.
  \item At its core, all self-attention does is route information. By contrast, relational cross-attention is focused on extracting \textit{relational information}. Both aspects are crucial for general sequence-modeling.
  \item We hypothesize that imbuing a Transformer with additional more specialize ``attention heads'' will enable the model to learn more robust, generalizable representations in a more sample-efficient manner.
  \item The intuitive reason for this hypothesis is that self-attention is a single type of computation: it can only select and retrieve information. Adding more and more self-attention heads will saturate: the mode already did all it can with this type of computation. If we instead add a different kind of attention head (i.e., RCA) the model will now have a different kind of computation available to it, which will be useful in new and different ways. By having both kinds of computation available to the model, it can learn to use both and select between them depending on the current context/etc. [this is the basic idea, but TO-DO: revise sentence and make it flow better. improve argument. etc.]
\end{enumerate}