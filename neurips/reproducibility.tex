\section*{Code and Reproducibility}

Our implementation of the Dual Attention Transformer architecture is open-sourced at \url{https://github.com/Awni00/dual-attention} and published as a Python package. Pre-trained model weights (e.g., the 1.3B-parameter \textit{DAT} language model) are made publicly available, and can be loaded directly through the package. Additionally, we provide code for running the experiments described in this paper, along with instructions for reproducing our results and access to the experimental logs. Links to all this can be found through the github repository.