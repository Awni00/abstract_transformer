\begin{abstract}
  The Transformer architecture processes sequences by implementing a form of neural message-passing that consists of iterative information retrieval (attention), followed by local processing (position-wise MLP).
  There are two types of information that are relevant/crucial under this general processing paradigm: 1) ``sensory'' information about individual objects, and 2) ``relational'' information describing the relations between object-level features  Standard attention naturally encodes the former, but does not explicitly encode the latter.
  In this paper, we present an extension of Transformers where multi-head attention now consists of two distinct types of attention heads, each routing information of a different type.
  The first is the standard attention mechanism of Transformers, which captures object-level features, while the second is a novel attention mechanism we propose to explicitly capture relational information.
  The two types of attention heads each possess different inductive biases, giving the resulting architecture greater efficiency and versatility.
  The promise of this approach is demonstrated empirically across a range of tasks, including visual relational reasoning, mathematical problem-solving, and language modeling.
\end{abstract}
