\section{Discussion}\label{sec:discussion}

\textbf{Summary.} The standard attention mechanism of Transformers provides a versatile mechanism for retrieval of sensory information in any given context, but does not explicitly support retrieval of relational information. In this work, we proposed an extension of the Transformer architecture that disentangles sensory and relational information by introducing a new type of attention mechanism designed for retrieval of relational information, introducing an architecture combining the two types of attention heads. We empirically evaluate this architecture and find that it yields performance improvements across a range of tasks.

\textbf{Limitations.} The proposed architecture introduces several hyperparameters and possible configurations. Although we carried out ablations on the major configuration choices (e.g., composition of head types, symmetry, symbol assignment mechanism), a more thorough empirical examination would help develop an improved understanding of the behavior of this architecture under different configurations. Such a systematic study may also enable the discovery of further tweaks to improve the architecture. We also note that our implementation of the Dual-Attention Transformer is somewhat slower compared to hardware-aware implementations of Transformers~\citep{dao2022flashattention}.