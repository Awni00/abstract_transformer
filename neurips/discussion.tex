
\textbf{Limitations.} Much work has gone into optimizing training and inference in Transformers. For example, hardware-aware implementations of attention~\citep{flashattention}. By comparison, our implementation is slower because it lacks these hardware optimizations. If the research community finds this model interesting, similar optimizations can be made. Architecture also has several hyperparameters and possible configurations. A more thorough empirical examination would enable discovery of good ``default choices'' and the ``right scaling'', as well as tweaking and improving the Architecture. ... Due to computational limitations, we performed very limited hyperparameter tuning. More time and effort on hyperparameter tuning may reveal further performance improvements. Scaling up, seeing whether benefits persist.

Mention that we didn't scale language modeling experiments too large / weren't able to observe benefits at very large scales yet, though this may be due to limited hyperparam tuning.