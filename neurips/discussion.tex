\section{Discussion}\label{sec:discussion}

\textbf{Limitations.} Much work has gone into optimizing training and inference in Transformers. For example, hardware-aware implementations of attention~\citep{flashattention}. By comparison, our implementation is slower because it lacks these hardware optimizations. If the research community finds this model interesting, similar optimizations can be made. Architecture also has several hyperparameters and possible configurations. A more thorough empirical examination would enable discovery of good ``default choices'' and the ``right scaling'', as well as tweaking and improving the Architecture. ... Due to computational limitations, we performed very limited hyperparameter tuning. More time and effort on hyperparameter tuning may reveal further performance improvements. Scaling up, seeing whether benefits persist.

Mention that we didn't scale language modeling experiments too large / weren't able to observe benefits at very large scales yet, though this may be due to limited hyperparam tuning.

\aanote[inline, nomargin]{TODO: in disucssion or ``limations'' section: mention that our current implementation is significantly slower than existing implementations of the Transformer. This is partly because DisRCAv2 involves additional computational steps, but is also because our implementation is unable to use hardware-aware optimizations such as Flash Attention. Considerable effort has been spent optimizing implementations of self-attention and Transformers. Also, our implementation may be ``naive,'' and there may be some optimizations possible without needing to program new GPU/cuda kernels, etc. We leave optimization of the implementation, including potentially hardware-aware implementations similar FlashAttention to future work.}
