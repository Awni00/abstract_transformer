\section{Function Class of Relational Attention: a universal approximation result}\label{sec:approx}

Recall that relational attention is a mapping on $\reals^d \times \reals^{n \times d} \to \reals^{d_{\mathrm{out}}}$, where $d$ is the dimension of the input objects and $d_{\mathrm{out}}$ is the output dimension. For convenience, we denote the ``query space'' by $\calX$ and the ``key space'' by $\calY$, though both are $\reals^d$ in this setting. Relational attention takes as input a query $x \in \calX$ and a collection of objects $\y = \ys \in \calY^n$ and computes the following
\begin{align}
  \RA(x, \y) &= \sum_{i=1}^{n} \alpha_i(x; \y) \bigparen{W_r \, r(x, y_i) + W_s \, s_i}, \\
  \alpha(x; \y) &= \Softmax\Bigparen{\bigbra{\iprod{\phiqattn(x)}{\phikattn(y_i)}}_{i=1}^{n}} \in \Delta^n, \\
  r(x, y_i) &= \paren{\iprod{\phi_{q,\ell}^{\rel}(x)}{\phi_{k,\ell}^{\rel}(y_i)}}_{\ell \in [d_r]} \in \reals^{d_r}, \\
  (s_1, \ldots, s_n) &= \SymbolRetriever\paren{\y;\, \Slib} \in \reals^{n \times d_{\mathrm{out}}},
\end{align}
where $\phiqattn, \phikattn, \phi_{q,\ell}^{\rel}, \phi_{k,\ell}^{\rel}: \reals^{d} \to \reals^{d_k}$ are the feature maps defining the attention mechanism and the relation, respectively. For this section, these are multi-layer perceptrons. Note that in~\Cref{alg:dual_head_attn} these are linear maps, but they are preceded by multi-layer perceptron in~\Cref{alg:dh_encoder,alg:dh_decoder}, which makes the overall function class the same. Moreover, for this analysis we will take $W_r = I, d_{\mathrm{out}} = 0$ and $W_s = 0$. We will then discuss how the role of symbols fits within the message of th

The following result states that relational attention can approximate any function of the form: 1) select an object in $\ys$ by an arbitrary query-dependent selection criterion, and 2) compute an arbitrary relation $r: \calX \times \calY \to \reals^{d_r}$ with the selected object. This is formalized below.

To formalize (1), we adopt an abstract and very general formulation of a ``selection criterion'' in terms of family of preference preorders, $\sset{\preceq_x}_x$: for each possible query $x$, the preorder $\preceq_x$ defines a preference over objects in $\calY$ to be selected. Intuitively, ``$y_1 \preceq_x y_2$'' means that $y_2$ is more relevant to the query $x$ than $y_1$.

More precisely, for each query $x \in \calX$, $\preceq_x$ is a complete (for each $y_1, y_2 \in \calY$, either $y_1 \preceq y_2$ or $y_2 \preceq_x y_1$), reflexive ($y \preceq_x y$ for all $y \in \calY$), and transitive ($y_1 \preceq_x y_2$ and $y_2 \preceq_x y_3$ implies $y_1 \preceq_x y_3$) relation. For each $x \in \calX$, $\preceq_x$ induces a preordered space $(\calY, \preceq_x)$. This implicitly defines two additional relations: $\prec_x$ and $\sim_x$. We will write $y_1 \prec_x y_2$ if ``$y_1 \preceq_x y_2$ and not $y_2 \preceq_x y_1$'', and $y_1 \sim y_2$ if ``$y_1 \preceq_x y_2$ and $y_2 \preceq_x y_1$''.

For a collection of objects $\y = \ys \in \calY^n$ and a query $x \in \calX$, the preorder $\preceq_x$ defines a selection function
\begin{equation}\label{eq:def_selection}
  \mathrm{Select}(x, \ys) \coloneq \max\paren{\ys, \mathtt{key}=\preceq_x}.
\end{equation}
That is, $\mathrm{Select}(x, \y)$ returns the most relevant element with respect to the query $x$. In particular, it returns $y_i$ when $y_i \succ_x y_j, \ \forall j \neq i$ (and may return an arbitrary element if no unique maximal element exists in $\ys$).

We will assume some regularity conditions on the family of preorders $\sset{\preceq_x}_x$ which essentially stipulate that, 1) nearby elements in $\calY$ have a similar preference with respect to each $x$, and 2) nearby queries in $\calX$ induce similar preference preorders.

\begin{assumption}[Selection criterion is query-continuous and key-continuous]\label{ass:qk_cts}
  The family of preorder relations $\sset{\preceq_x}_{x \in \calX}$ satisfies the following:
  \begin{enumerate}
    \item \textbf{Key-continuity.} For each $x \in \calX$, $\preceq_x$ is continuous. That is, for any sequence $(y_i)_i$ such that $y_i \preceq_x z$ and $y_i \to y_{\infty}$, we have $y_\infty \preceq_x z$. Equivalently, for any $y \in \calY$, $\sset{z \in \calY : z \preceq_x y}$ and $\sset{z \in \calY : y \preceq_x z}$ are closed sets in $\calY$.
    \item \textbf{Query-continuity.} Under key-continuity, \citet{debreu1954representation} shows that for each $x \in \calX$, there exists a continuous in utility function $u_x: \calY \to \reals$ for $\preceq_x$ such that $y_1 \preceq_x y_2 \iff u_x(y_1) \leq u_x(y_2)$. For query-continuity, we make the further assumption that there exists a family of utility functions $\sset{u_x: \calY \to \reals}_{x \in \calX}$ such that  $u(x, y) \coloneq u_x(y)$ is also continuous in its first argument.
  \end{enumerate}
\end{assumption}

For technical reasons, for \Cref{eq:def_selection} to make sense, we must assume that there exists a unique element to be selected. We formulate this in terms of an assumption on the data distribution of the space $\calX \times \calY^n$. This is a technical assumption, and different forms of such an assumption would be possible (e.g., instead condition on this event).
\begin{assumption}[Selection is unique almost always]\label{ass:select_unique}
  Let $(x, \y) \sim \bbP_{x,\y}$. For each $\epsilon > 0$, there exists $\eta_\epsilon > 0$ such that $\min_{j \neq i} \abs{u_x(y_i) - u_x(y_j)} > \eta_\epsilon$ with probability at least $1 - \epsilon$.
\end{assumption}

\aanote{is $\calX$ and $\calY$ confusing? switch to $\calQ$ and $\calX$ (or $\calK$)?}

\begin{theorem}[Function class of relational attention]\label{theorem:func_class}
  Let $\calX, \calY$ be compact euclidean spaces. Let $\sset{\preceq_x}_{x \in \calX}$ be an arbitrary family of relevance preorders on $\calY$ which are query-continuous and key-continuous (\Cref{ass:qk_cts}). Let $\mathrm{Select}(x, \ys) = \max(\ys, \mathtt{key}=\preceq_x)$ be the selection function associated with $\sset{\preceq_x}_{x}$. Let $R: \calX \times \calY \to \reals^{d_r}$ be an arbitrary continuous relation function. Suppose $x, \y \sim \bbP_{x, \y}$ and that \Cref{ass:select_unique} holds (i.e., the data distribution is such that there exists a unique most-relevant element). For any $\epsilon > 0$, there exists multi-layer perceptrons $\phiqattn, \phikattn, \phiqrel, \phikrel$ and a choice of symbols such that,
  \begin{equation*}
    \infnorm{\RA(x, \ys) - R(x, \mathrm{Select}(x, \ys))} < \epsilon
  \end{equation*}
\end{theorem}

\begin{proof}
  Condition on the event $\calE := \sset{(x, \y) \in \calX \times \calY^n \,\colon\, \min_{j \neq i} \abs{u_x(y_i) - u_x(y_j)} > \eta_\epsilon}$. Let $i^* = \argmax(\ys, \mathtt{key} = \preceq_x) = \argmax(u_x(y_1), \ldots, u_x(y_n))$. By~\citep[Theorem 5.1]{altabaaApproximationRelationFunctions2024}, for any $\epsilon_1 > 0$, there exists MLPs $\phiqattn, \phikattn$ such that $\alpha_{i^*}(x, \y) > 1 - \epsilon_1$ for any $(x, \y) \in \calE$. That is, the attention score is nearly $1$ for the $\preceq_x$-selected element \textit{uniformly} over inputs in $\calE$.

  Similarly, by~\citep[Theorem 3.1]{altabaaApproximationRelationFunctions2024}, for any $\epsilon_2 > 0$, there exists MLPs $(\phiqrell{\ell}, \phikrell{\ell})_{\ell \in [d_r]}$ such that $r(x, y) \coloneq (\iiprod{\phiqrell{\ell}(x)}{\phikrell{\ell}(y)})_{\ell \in [d_r]}$ approximates the target relation $R$ uniformly within an error of $\epsilon_2$,
  \begin{equation*}
    \infnorm{R(x, y) - r(x, y)} < \epsilon_2, \quad \text{Lebesgue almost every } (x, y) \in \calX \times \calY.
  \end{equation*}

  Thus, we have
  \begin{align*}
    &\infnorm{\RA(x, \ys) - R(x, \mathrm{Select}(x, \ys))} \\
    &= \infnorm{\sum_{i=1}^{n} \alpha_i(x; \y) \,r(x, y_i) - R(x, y_{i^*})} \\
    &\leq \sum_{i=1}^{n} \infnorm{\alpha_i(x; \y) \,r(x, y_i) - R(x, y_{i^*})} \\
    &\leq \alpha_{i^*}(x, \y) \infnorm{r(x, y_{i^*}) - R(x, y_{i^*})} + \sum_{j \neq i^*} \alpha_i(x; \y) \infnorm{r(x, y_i) - R(x, y_{i^*})}\\
    &\leq (1 - \epsilon_1) \epsilon_2 + \epsilon_1 \max_{x, y, y^*} \infnorm{r(x, y) - R(x, y^{*})}.
  \end{align*}
  Note that $\max_{x, y, y^*} \infnorm{r(x, y) - R(x, y^*)}$ is finite since $\calX, \calY$ are compact and $r, R$ are continuous. Letting $\epsilon_1, \epsilon_2$ be small enough completes the proof.
\end{proof}

To summarize the analysis in this section, we showed that relational attention can approximate any computation composed of selecting an object from a collection followed by computing a relation with that object. We can approximate any well-behaved selection criterion by formulating it in terms of an abstract preference preorder, and approximating the corresponding utility function (given by a Debreu representation theorem) by inner products of query and key feature maps. We can then approximate the target relation function similarly by inner products of a different set of query and key feature maps.

In the analysis above, we set aside the role of the symbols. Note that the function class this approximation result proves involves retrieving a relation from a selected object, but does not explicitly encoded the identity of the selected object. Informally, the receiver knows that it has a particular relation with one of the objects in its context, and knows that this relation is with an object that was selected according to a particular selection criterion, but does not know the identity of the object beyond that. This is the purpose of adding symbols to relational attention---the retrieved relation is tagged with a symbol identifying the sender.
