{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import sys; sys.path.append('..')\n",
    "from language_models import DualAttnTransformerLM, TransformerLM\n",
    "from hf import DualAttnTransformerLM_HFHub\n",
    "from huggingface_hub import ModelCard, ModelCardData\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_from_ckpt(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model_config = ckpt['config']\n",
    "\n",
    "    model_state_dict = ckpt['model']\n",
    "    model_state_dict = {k.replace('_orig_mod.', ''): v for k, v in model_state_dict.items()}\n",
    "\n",
    "    if 'n_heads_ra' in model_config:\n",
    "        model = DualAttnTransformerLM(**model_config)\n",
    "    else:\n",
    "        model = TransformerLM(**model_config)\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_from_ckpt_hf(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model_config = ckpt['config']\n",
    "\n",
    "    model_state_dict = ckpt['model']\n",
    "    model_state_dict = {k.replace('_orig_mod.', ''): v for k, v in model_state_dict.items()}\n",
    "\n",
    "    if 'n_heads_ra' in model_config:\n",
    "        model = DualAttnTransformerLM_HFHub(**model_config)\n",
    "    else:\n",
    "        model = TransformerLM(**model_config)\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../experiments/fineweb/log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    f'{base_path}/DAT-sa8-ra8-ns1024-sh8-nkvh4-343M_2024_07_19_13_50_14_resumed_2024_07_26_18_49_04/model_19073.pt',\n",
    "    f'{base_path}/DAT-sa8-ra8-nr64-ns1024-sh8-nkvh4-343M_2024_07_30_13_58_00_resumed_2024_08_14_19_34_08/model_19073.pt',\n",
    "    f'{base_path}/DAT-sa8-ra8-nr32-ns1024-sh8-nkvh4-343M_2024_07_30_16_55_13_resumed_2024_08_14_19_34_16/model_19073.pt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_card(model, model_name):\n",
    "    n_layers = model.n_layers\n",
    "    block_size = model.block_size\n",
    "    d_model = model.d_model\n",
    "    n_heads_sa = model.n_heads_sa\n",
    "    n_heads_ra = model.n_heads_ra\n",
    "    rel_dim = model.layers.blocks[0].dual_attn.relational_attention.n_relations\n",
    "    training_tokens = '10B'\n",
    "    tokenizer = 'GPT-2 BPE tokenizer'\n",
    "    dataset = 'HuggingFaceFW/fineweb-edu'\n",
    "    msize = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    if msize > 1e9:\n",
    "        msize = f'{msize/1e9:.0f}B'\n",
    "    elif msize > 1e6:\n",
    "        msize = f'{msize/1e6:.0f}M'\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    template = dict(\n",
    "        model_name=model_name,\n",
    "        n_layers=n_layers,\n",
    "        block_size=block_size,\n",
    "        d_model=d_model,\n",
    "        n_heads_sa=n_heads_sa,\n",
    "        n_heads_ra=n_heads_ra,\n",
    "        rel_dim=rel_dim,\n",
    "        training_tokens=training_tokens,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=dataset,\n",
    "        msize=msize,\n",
    "        date=datetime.now().strftime('%B, %Y')\n",
    "    )\n",
    "\n",
    "    card_data = ModelCardData(\n",
    "        language=\"en\", license=\"mit\", dataset=dataset, pipeline_tag=\"text-generation\", tags=[\"model_hub_mixin\", \"pytorch_model_hub_mixin\"])\n",
    "\n",
    "\n",
    "    model_card = ModelCard.from_template(card_data, 'readme_template.md', **template)\n",
    "    return model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "model_name: DAT-sa8-ra8-ns1024-sh8-nkvh4-343M\n",
      "model_path: ../experiments/fineweb/log/DAT-sa8-ra8-ns1024-sh8-nkvh4-343M_2024_07_19_13_50_14_resumed_2024_07_26_18_49_04/model_19073.pt\n",
      "================================================================================\n",
      "model_name: DAT-sa8-ra8-nr64-ns1024-sh8-nkvh4-343M\n",
      "model_path: ../experiments/fineweb/log/DAT-sa8-ra8-nr64-ns1024-sh8-nkvh4-343M_2024_07_30_13_58_00_resumed_2024_08_14_19_34_08/model_19073.pt\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: DAT-sa8-ra8-nr32-ns1024-sh8-nkvh4-343M\n",
      "model_path: ../experiments/fineweb/log/DAT-sa8-ra8-nr32-ns1024-sh8-nkvh4-343M_2024_07_30_16_55_13_resumed_2024_08_14_19_34_16/model_19073.pt\n"
     ]
    }
   ],
   "source": [
    "for model_path in model_paths:\n",
    "    print('='*80)\n",
    "    model = load_from_ckpt_hf(model_path)\n",
    "    model_name = model_path.split('/')[-2]\n",
    "    model_name = model_name.split('_')[0]\n",
    "    print(f'model_name: {model_name}')\n",
    "    print(f'model_path: {model_path}')\n",
    "\n",
    "    confirm = input('Confirm? (y/n): ')\n",
    "    if confirm == 'y':\n",
    "        model.push_to_hub(model_name)\n",
    "        create_model_card(model, model_name).push_to_hub(f'awni00/{model_name}')\n",
    "    else:\n",
    "        print('Model not pushed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
