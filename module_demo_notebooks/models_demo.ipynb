{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "import torchinfo\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DualAttnTransformerLM                                   --\n",
       "├─ModuleDict: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   16,384,000\n",
       "│    │    └─Linear: 3-1                                 16,416,000\n",
       "│    └─Dropout: 2-2                                     --\n",
       "│    └─SymbolicAttention: 2-3                           524,288\n",
       "│    │    └─Linear: 3-2                                 262,656\n",
       "│    └─ModuleList: 2-4                                  --\n",
       "│    │    └─DualAttnEncoderBlock: 3-3                   4,595,200\n",
       "│    │    └─DualAttnEncoderBlock: 3-4                   4,595,200\n",
       "│    │    └─DualAttnEncoderBlock: 3-5                   4,595,200\n",
       "│    │    └─DualAttnEncoderBlock: 3-6                   4,595,200\n",
       "│    │    └─DualAttnEncoderBlock: 3-7                   4,595,200\n",
       "│    │    └─DualAttnEncoderBlock: 3-8                   4,595,200\n",
       "│    └─Linear: 2-5                                      (recursive)\n",
       "================================================================================\n",
       "Total params: 61,158,144\n",
       "Trainable params: 61,158,144\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from language_models import DualAttnTransformerLM\n",
    "\n",
    "dat_lm = DualAttnTransformerLM(\n",
    "    vocab_size=32_000,    # vocabulary size\n",
    "    d_model=512,          # model dimension\n",
    "    n_layers=6,           # number of layers\n",
    "    n_heads_sa=4,         # number of self-attention heads\n",
    "    n_heads_ra=4,         # number of relational attention headsd\n",
    "    dff=2048,             # feedforward intermediate dimension\n",
    "    dropout_rate=0.1,     # dropout rate\n",
    "    activation='swiglu',  # activation function of feedforward block\n",
    "    norm_first=True,      # whether to use pre-norm or post-norm\n",
    "    max_block_size=1024,  # max context length\n",
    "    symbol_retrieval='symbolic_attention', # type of symbol assignment mechanism\n",
    "    symbol_retrieval_kwargs=dict(d_model=512, n_heads=4, n_symbols=512), # kwargs for symbol assignment mechanism\n",
    "    pos_enc_type='RoPE'   # type of positional encoding to use\n",
    ")\n",
    "\n",
    "torchinfo.summary(dat_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.randint(0, 32_000, (1, 129))\n",
    "x, y = idx[:, :-1], idx[:, 1:]\n",
    "logits, loss = dat_lm(x, y)\n",
    "logits.shape # shape: (1, 128, 32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "VisionDualAttnTransformer                          101,376\n",
       "├─PositionRelativeSymbolRetriever: 1-1             --\n",
       "│    └─RelativePositionalEncoding: 2-1             202,240\n",
       "├─Sequential: 1-2                                  --\n",
       "│    └─Rearrange: 2-2                              --\n",
       "│    └─LayerNorm: 2-3                              1,536\n",
       "│    └─Linear: 2-4                                 393,728\n",
       "│    └─LayerNorm: 2-5                              1,024\n",
       "├─Dropout: 1-3                                     --\n",
       "├─ModuleList: 1-4                                  --\n",
       "│    └─DualAttnEncoderBlock: 2-6                   --\n",
       "│    │    └─Dropout: 3-1                           --\n",
       "│    │    └─LayerNorm: 3-2                         1,024\n",
       "│    │    └─DualAttention: 3-3                     1,180,672\n",
       "│    │    └─LayerNorm: 3-4                         1,024\n",
       "│    │    └─FeedForwardBlock: 3-5                  3,150,336\n",
       "│    └─DualAttnEncoderBlock: 2-7                   --\n",
       "│    │    └─Dropout: 3-6                           --\n",
       "│    │    └─LayerNorm: 3-7                         1,024\n",
       "│    │    └─DualAttention: 3-8                     1,180,672\n",
       "│    │    └─LayerNorm: 3-9                         1,024\n",
       "│    │    └─FeedForwardBlock: 3-10                 3,150,336\n",
       "│    └─DualAttnEncoderBlock: 2-8                   --\n",
       "│    │    └─Dropout: 3-11                          --\n",
       "│    │    └─LayerNorm: 3-12                        1,024\n",
       "│    │    └─DualAttention: 3-13                    1,180,672\n",
       "│    │    └─LayerNorm: 3-14                        1,024\n",
       "│    │    └─FeedForwardBlock: 3-15                 3,150,336\n",
       "│    └─DualAttnEncoderBlock: 2-9                   --\n",
       "│    │    └─Dropout: 3-16                          --\n",
       "│    │    └─LayerNorm: 3-17                        1,024\n",
       "│    │    └─DualAttention: 3-18                    1,180,672\n",
       "│    │    └─LayerNorm: 3-19                        1,024\n",
       "│    │    └─FeedForwardBlock: 3-20                 3,150,336\n",
       "│    └─DualAttnEncoderBlock: 2-10                  --\n",
       "│    │    └─Dropout: 3-21                          --\n",
       "│    │    └─LayerNorm: 3-22                        1,024\n",
       "│    │    └─DualAttention: 3-23                    1,180,672\n",
       "│    │    └─LayerNorm: 3-24                        1,024\n",
       "│    │    └─FeedForwardBlock: 3-25                 3,150,336\n",
       "│    └─DualAttnEncoderBlock: 2-11                  --\n",
       "│    │    └─Dropout: 3-26                          --\n",
       "│    │    └─LayerNorm: 3-27                        1,024\n",
       "│    │    └─DualAttention: 3-28                    1,180,672\n",
       "│    │    └─LayerNorm: 3-29                        1,024\n",
       "│    │    └─FeedForwardBlock: 3-30                 3,150,336\n",
       "├─Linear: 1-5                                      513,000\n",
       "===========================================================================\n",
       "Total params: 27,211,240\n",
       "Trainable params: 27,211,240\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vision_models import VisionDualAttnTransformer\n",
    "\n",
    "img_shape = (3, 224, 224)\n",
    "patch_size = (16, 16)\n",
    "n_patches = (img_shape[1] // patch_size[0]) * (img_shape[2] // patch_size[1])\n",
    "\n",
    "dat_vision = VisionDualAttnTransformer(\n",
    "    image_shape=img_shape,     # shape of input image\n",
    "    patch_size=patch_size,     # size of patch\n",
    "    num_classes=1000,          # number of classes\n",
    "    d_model=512,               # model dimension\n",
    "    n_layers=6,                # number of layers\n",
    "    n_heads_sa=4,              # number of self-attention heads\n",
    "    n_heads_ra=4,              # number of relational attention heads\n",
    "    dff=2048,                  # feedforward intermediate dimension\n",
    "    dropout_rate=0.1,          # dropout rate\n",
    "    activation='swiglu',       # activation function of feedforward block\n",
    "    norm_first=True,           # whether to use pre-norm or post-norm\n",
    "    symbol_retrieval='position_relative',\n",
    "    symbol_retrieval_kwargs=dict(symbol_dim=512, max_rel_pos=n_patches+1),\n",
    "    ra_kwargs=dict(symmetric_rels=True, use_relative_positional_symbols=True),\n",
    "    pool='cls',                # type of pooling (class token)\n",
    ")\n",
    "\n",
    "torchinfo.summary(dat_vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn(1, *img_shape)\n",
    "logits = dat_vision(img)\n",
    "logits.shape # shape: (1, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstract_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
