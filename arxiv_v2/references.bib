@inproceedings{altabaa2024abstractors,
title={Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers},
author={Awni Altabaa and Taylor Whittington Webb and Jonathan D. Cohen and John Lafferty},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=XNa6r6ZjoB}
}


@misc{altabaaApproximationRelationFunctions2024,
  title = {Approximation of Relation Functions and Attention Mechanisms},
  author = {Altabaa, Awni and Lafferty, John},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08856},
  eprint = {2402.08856},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08856},
  urldate = {2024-05-15},
  archiveprefix = {arxiv}
}

@misc{altabaaLearningHierarchicalRelational2024,
  title = {Learning {{Hierarchical Relational Representations}} through {{Relational Convolutions}}},
  author = {Altabaa, Awni and Lafferty, John},
  year = {2024},
  month = feb,
  number = {arXiv:2310.03240},
  eprint = {2310.03240},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03240},
  urldate = {2024-05-15},
  archiveprefix = {arxiv}
}

@article{boix-adseraWhenCanTransformers,
  title = {When Can Transformers Reason with Abstract Symbols?},
  author = {{Boix-Adser{\`a}}, Enric and Saremi, Omid and Abbe, Emmanuel and Bengio, Samy and Littwin, Etai and Susskind, Joshua},
  langid = {english}
}

@inproceedings{dosovitskiyImageWorth16x162020,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@misc{eldanTinyStoriesHowSmall2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  year = {2023},
  month = may,
  number = {arXiv:2305.07759},
  eprint = {2305.07759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07759},
  urldate = {2024-05-16},
  archiveprefix = {arxiv}
}

@misc{kazemnejadImpactPositionalEncoding2023,
  title = {The {{Impact}} of {{Positional Encoding}} on {{Length Generalization}} in {{Transformers}}},
  author = {Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
  year = {2023},
  month = may,
  number = {arXiv:2305.19466},
  eprint = {2305.19466},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-17},
  archiveprefix = {arxiv}
}

@misc{kergNeuralArchitectureInductive2022,
  title = {On {{Neural Architecture Inductive Biases}} for {{Relational Tasks}}},
  author = {Kerg, Giancarlo and Mittal, Sarthak and Rolnick, David and Bengio, Yoshua and Richards, Blake and Lajoie, Guillaume},
  year = {2022},
  month = jun,
  number = {arXiv:2206.05056},
  eprint = {2206.05056},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.05056},
  urldate = {2022-09-13},
  archiveprefix = {arxiv}
}

@misc{locatelloObjectCentricLearningSlot2020,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  month = oct,
  number = {arXiv:2006.15055},
  eprint = {2006.15055},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-11-18},
  archiveprefix = {arxiv}
}

@inproceedings{santoroRelationalRecurrentNeural2018,
  title = {Relational Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-11-11}
}

@misc{santoroSimpleNeuralNetwork2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  month = jun,
  number = {arXiv:1706.01427},
  eprint = {1706.01427},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-11-11},
  archiveprefix = {arxiv}
}

@inproceedings{saxtonAnalyzingMathematicalReasoning2019,
  title={Analysing Mathematical Reasoning Abilities of Neural Models},
  author={David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=H1gR5iR5FX},
}

@misc{schlagEnhancingTransformerExplicit2020,
  title = {Enhancing the {{Transformer}} with {{Explicit Relational Encoding}} for {{Math Problem Solving}}},
  author = {Schlag, Imanol and Smolensky, Paul and Fernandez, Roland and Jojic, Nebojsa and Schmidhuber, J{\"u}rgen and Gao, Jianfeng},
  year = {2020},
  month = nov,
  number = {arXiv:1910.06611},
  eprint = {1910.06611},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1910.06611},
  urldate = {2023-01-16},
  archiveprefix = {arxiv}
}


@inproceedings{shanahanExplicitlyRelationalNeurala,
  title = 	 {An Explicitly Relational Neural Network Architecture},
  author =       {Shanahan, Murray and Nikiforou, Kyriacos and Creswell, Antonia and Kaplanis, Christos and Barrett, David and Garnelo, Marta},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = 	 {2020},
}


@inproceedings{shawSelfAttentionRelativePosition2018b,
  title = {Self-{{Attention}} with {{Relative Position Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year = {2018},
  month = jun,
  pages = {464--468},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-2074},
  urldate = {2024-05-16}
}

@misc{shazeerGLUVariantsImprove2020,
  title = {{{GLU Variants Improve Transformer}}},
  author = {Shazeer, Noam},
  year = {2020},
  month = feb,
  number = {arXiv:2002.05202},
  eprint = {2002.05202},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-16},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{smolenskyNeurocompositionalComputingCentral2022,
  title = {Neurocompositional Computing: {{From}} the {{Central Paradox}} of {{Cognition}} to a New Generation of {{AI}} Systems},
  shorttitle = {Neurocompositional Computing},
  author = {Smolensky, Paul and McCoy, R. Thomas and Fernandez, Roland and Goldrick, Matthew and Gao, Jianfeng},
  year = {2022},
  month = may,
  number = {arXiv:2205.01128},
  eprint = {2205.01128},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01128},
  urldate = {2023-01-13},
  archiveprefix = {arxiv}
}

@misc{suRoFormerEnhancedTransformer2023,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-14},
  archiveprefix = {arxiv}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-12-06},
  archiveprefix = {arxiv}
}

@article{vaswani2017attention,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@misc{webbEmergentSymbolsBinding2021,
  title = {Emergent {{Symbols}} through {{Binding}} in {{External Memory}}},
  author = {Webb, Taylor W. and Sinha, Ishan and Cohen, Jonathan D.},
  year = {2021},
  month = mar,
  number = {arXiv:2012.14601},
  eprint = {2012.14601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.14601},
  urldate = {2022-09-13},
  archiveprefix = {arxiv}
}

@article{webbRelationalBottleneckInductive2024,
  title = {The Relational Bottleneck as an Inductive Bias for Efficient Abstraction},
  author = {Webb, Taylor W. and Frankland, Steven M. and Altabaa, Awni and Segert, Simon and Krishnamurthy, Kamesh and Campbell, Declan and Russin, Jacob and Giallanza, Tyler and O'Reilly, Randall and Lafferty, John and Cohen, Jonathan D.},
  year = {2024},
  month = may,
  journal = {Trends in Cognitive Sciences},
  pages = {S1364661324000809},
  issn = {13646613},
  doi = {10.1016/j.tics.2024.04.001},
  urldate = {2024-05-15},
  langid = {english}
}

@misc{whittingtonRelatingTransformersModels2022,
  title = {Relating Transformers to Models and Neural Representations of the Hippocampal Formation},
  author = {Whittington, James C. R. and Warren, Joseph and Behrens, Timothy E. J.},
  year = {2022},
  month = mar,
  number = {arXiv:2112.04035},
  eprint = {2112.04035},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-01-05},
  archiveprefix = {arxiv}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=B1ckMDqlg}
}


@InProceedings{duGLaM2022,
  title = 	 {{GL}a{M}: Efficient Scaling of Language Models with Mixture-of-Experts},
  author =       {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5547--5569},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/du22c/du22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/du22c.html},
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/N19-1423},
  urldate = {2024-05-17}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {OpenAI}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@inproceedings{inanTyingWordVectors2016,
  title={Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling},
  author={Hakan Inan and Khashayar Khosravi and Richard Socher},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=r1aPbsFle}
}

@article{debreu1954representation,
  title={Representation of a preference ordering by a numerical function},
  author={Debreu, Gerard and others},
  journal={Decision processes},
  volume={3},
  pages={159--165},
  year={1954},
  publisher={Wiley New York}
}

@article{imagenet,
  Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  Title = {{ImageNet Large Scale Visual Recognition Challenge}},
  Year = {2015},
  journal   = {International Journal of Computer Vision (IJCV)},
  doi = {10.1007/s11263-015-0816-y},
  volume={115},
  number={3},
  pages={211-252}
}

@misc{ba2016layer,
  title={Layer Normalization}, 
  author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  year={2016},
  eprint={1607.06450},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@misc{hendrycks2016gaussian,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2016},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with {IO}-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@inproceedings{dao2023flashattention,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Tri Dao},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=mZn2Xyh9Ec}
}

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}

@misc{yue2024mmmumassivemultidisciplinemultimodal,
      title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI}, 
      author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
      year={2024},
      eprint={2311.16502},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.16502}, 
}

@techreport{cifar_dataset,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@inproceedings{zhang2018mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@misc{cubuk2019autoaugmentlearningaugmentationpolicies,
      title={AutoAugment: Learning Augmentation Policies from Data}, 
      author={Ekin D. Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V. Le},
      year={2019},
      eprint={1805.09501},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1805.09501}, 
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@software{lozhkov2024fineweb-edu,
  author = {Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and Wolf, Thomas},
  title = {FineWeb-Edu},
  month = May,
  year = 2024,
  doi = { 10.57967/hf/2497 },
  url = {https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu}
}

@misc{clark2019doesbertlookat,
      title={What Does BERT Look At? An Analysis of BERT's Attention}, 
      author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
      year={2019},
      eprint={1906.04341},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.04341}, 
}

@misc{htut2019attentionheadsberttrack,
      title={Do Attention Heads in BERT Track Syntactic Dependencies?}, 
      author={Phu Mon Htut and Jason Phang and Shikha Bordia and Samuel R. Bowman},
      year={2019},
      eprint={1911.12246},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.12246}, 
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{zerroug2022benchmark,
  title={A benchmark for compositional visual reasoning},
  author={Zerroug, Aimen and Vaishnav, Mohit and Colin, Julien and Musslick, Sebastian and Serre, Thomas},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={29776--29788},
  year={2022}
}

@article{zhao2024benchmarking,
  title={Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning},
  author={Zhao, Bingchen and Zong, Yongshuo and Zhang, Letian and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2406.12742},
  year={2024}
}

@inproceedings{johnson2017clevr,
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},
  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {July},
  year = {2017}
}

@inproceedings{ainslie-etal-2023-gqa,
  title = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author = {Ainslie, Joshua  and
    Lee-Thorp, James  and
    de Jong, Michiel  and
    Zemlyanskiy, Yury  and
    Lebron, Federico  and
    Sanghai, Sumit},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  month = {dec},
  year = {2023},
  address = {Singapore},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2023.emnlp-main.298},
  doi = {10.18653/v1/2023.emnlp-main.298},
  pages = {4895--4901},
}

@misc{penedo2024finewebdatasetsdecantingweb,
    title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
    author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
    year={2024},
    eprint={2406.17557},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.17557},
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
  title={Training Compute-Optimal Large Language Models}, 
  author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  year={2022},
  eprint={2203.15556},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2203.15556}, 
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
}

@article{baxter2000model,
  title={A model of inductive bias learning},
  author={Baxter, Jonathan},
  journal={Journal of artificial intelligence research},
  volume={12},
  pages={149--198},
  year={2000}
}

@techreport{wolpert1995no,
  title={No free lunch theorems for search},
  author={Wolpert, David H and Macready, William G and others},
  year={1995},
  institution={Citeseer}
}

@article{goyal2022inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2266},
  pages={20210068},
  year={2022},
  publisher={The Royal Society}
}

@book{marcus2003algebraic,
  title={The algebraic mind: Integrating connectionism and cognitive science},
  author={Marcus, Gary F},
  year={2003},
  publisher={MIT press}
}

@article{newman1997neural,
  title={A neural global workspace model for conscious attention},
  author={Newman, James and Baars, Bernard J and Cho, Sung-Bae},
  journal={Neural Networks},
  volume={10},
  number={7},
  pages={1195--1206},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}

@article{kemp2008discovery,
  title={The discovery of structural form},
  author={Kemp, Charles and Tenenbaum, Joshua B},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={31},
  pages={10687--10692},
  year={2008},
  publisher={National Acad Sciences}
}

@inproceedings{barrett2018measuring,
  title={Measuring abstract reasoning in neural networks},
  author={Barrett, David and Hill, Felix and Santoro, Adam and Morcos, Ari and Lillicrap, Timothy},
  booktitle={International conference on machine learning},
  pages={511--520},
  year={2018},
  organization={PMLR}
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@article{holyoak2012analogy,
  title={Analogy and relational reasoning},
  author={Holyoak, Keith J},
  journal={The Oxford handbook of thinking and reasoning},
  pages={234--259},
  year={2012}
}

@article{snow1984topography,
  title={The topography of ability and learning correlations},
  author={Snow, Richard E and Kyllonen, Patrick C and Marshalek, Brachia and others},
  journal={Advances in the psychology of human intelligence},
  volume={2},
  number={S 47},
  pages={103},
  year={1984}
}