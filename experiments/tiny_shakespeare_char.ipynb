{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import sys; sys.path.append('..')\n",
    "from models import TransformerLM, AbstractTransformerLM, configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'../data/tiny_shakespeare_char'\n",
    "\n",
    "# I/O\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "\n",
    "\n",
    "# system\n",
    "device = 'cuda'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "compile = True\n",
    "\n",
    "# evaluation and output\n",
    "out_dir = '../out/out-shakespeare-char'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "eval_interval = 250 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "log_interval = 10 # don't print too too often\n",
    "\n",
    "# we expect to overfit on this small dataset, so only save when val improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = 'shakespeare-char'\n",
    "wandb_run_name = 'mini-gpt'\n",
    "\n",
    "# optimization hyperparams\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "max_iters = 5000\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "weight_decay = 1e-1\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "warmup_iters = 100\n",
    "gradient_accumulation_steps = 1 # accumulate gradients over this many steps. simulates larger batch size\n",
    "\n",
    "# batch size and block size\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "# DDP (distributed data parallel) training\n",
    "ddp = False\n",
    "master_process = True\n",
    "\n",
    "# TODO: set up DDP for future experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_data = np.memmap(f'{data_path}_train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(f'{data_path}_val.bin', dtype=np.uint16, mode='r')\n",
    "test_data = np.memmap(f'{data_path}_test.bin', dtype=np.uint16, mode='r')\n",
    "\n",
    "def get_batch(split, batch_size=batch_size, block_size=256):\n",
    "    data = train_data if split == 'train' else val_data if split == 'val' else test_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in idx])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in idx])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "meta_data = pickle.load(open(f'../data/shakespeare_char_meta.pkl', 'rb'))\n",
    "vocab_size = meta_data['vocab_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=vocab_size, d_model=384, n_layers=6, n_heads=6, dff=None,\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True)\n",
    "model = transformer_lm = TransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerLM                                 [1, 1, 65]                --\n",
       "├─ModuleDict: 1-1                             --                        --\n",
       "│    └─Embedding: 2-1                         [1, 256, 384]             24,960\n",
       "│    └─Embedding: 2-2                         [256, 384]                98,304\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─EncoderBlock: 3-1                 [1, 256, 384]             1,774,464\n",
       "│    │    └─EncoderBlock: 3-2                 [1, 256, 384]             1,774,464\n",
       "│    │    └─EncoderBlock: 3-3                 [1, 256, 384]             1,774,464\n",
       "│    │    └─EncoderBlock: 3-4                 [1, 256, 384]             1,774,464\n",
       "│    │    └─EncoderBlock: 3-5                 [1, 256, 384]             1,774,464\n",
       "│    │    └─EncoderBlock: 3-6                 [1, 256, 384]             1,774,464\n",
       "│    └─Linear: 2-4                            [1, 1, 65]                25,025\n",
       "===============================================================================================\n",
       "Total params: 10,795,073\n",
       "Trainable params: 10,795,073\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 32.31\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 34.60\n",
       "Params size (MB): 28.99\n",
       "Estimated Total Size (MB): 63.59\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,256)), device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 27, with 10,765,056 parameters\n",
      "num non-decayed parameter tensors: 49, with 30,017 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "if compile:\n",
    "    print('compiling model...')\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5564, val loss 4.5639\n",
      "iter 0: loss 4.5732, time 33891.68ms, mfu -100.00%\n",
      "iter 10: loss 3.4819, time 46.14ms, mfu 8.11%\n",
      "iter 20: loss 2.9775, time 46.12ms, mfu 8.11%\n",
      "iter 30: loss 2.7137, time 45.98ms, mfu 8.11%\n",
      "iter 40: loss 2.6052, time 45.87ms, mfu 8.12%\n",
      "iter 50: loss 2.5684, time 45.92ms, mfu 8.12%\n",
      "iter 60: loss 2.5493, time 45.89ms, mfu 8.12%\n",
      "iter 70: loss 2.4961, time 46.19ms, mfu 8.12%\n",
      "iter 80: loss 2.5003, time 46.07ms, mfu 8.12%\n",
      "iter 90: loss 2.5080, time 46.19ms, mfu 8.12%\n",
      "iter 100: loss 2.4887, time 46.21ms, mfu 8.12%\n",
      "iter 110: loss 2.4660, time 46.40ms, mfu 8.11%\n",
      "iter 120: loss 2.4318, time 46.19ms, mfu 8.11%\n",
      "iter 130: loss 2.4129, time 46.16ms, mfu 8.11%\n",
      "iter 140: loss 2.3724, time 46.22ms, mfu 8.11%\n",
      "iter 150: loss 2.3654, time 46.19ms, mfu 8.11%\n",
      "iter 160: loss 2.3205, time 45.86ms, mfu 8.11%\n",
      "iter 170: loss 2.2785, time 45.93ms, mfu 8.12%\n",
      "iter 180: loss 2.2060, time 45.97ms, mfu 8.12%\n",
      "iter 190: loss 2.1525, time 46.01ms, mfu 8.12%\n",
      "iter 200: loss 2.1044, time 46.16ms, mfu 8.12%\n",
      "iter 210: loss 2.0523, time 46.21ms, mfu 8.12%\n",
      "iter 220: loss 2.0201, time 46.34ms, mfu 8.11%\n",
      "iter 230: loss 2.0130, time 46.22ms, mfu 8.11%\n",
      "iter 240: loss 1.9540, time 46.33ms, mfu 8.11%\n",
      "step 250: train loss 1.8679, val loss 1.9715\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 250: loss 1.9132, time 10226.07ms, mfu 7.30%\n",
      "iter 260: loss 1.8760, time 45.85ms, mfu 7.39%\n",
      "iter 270: loss 1.8805, time 45.94ms, mfu 7.46%\n",
      "iter 280: loss 1.8340, time 45.93ms, mfu 7.53%\n",
      "iter 290: loss 1.8204, time 45.98ms, mfu 7.59%\n",
      "iter 300: loss 1.8280, time 45.97ms, mfu 7.65%\n",
      "iter 310: loss 1.8159, time 46.08ms, mfu 7.69%\n",
      "iter 320: loss 1.8060, time 46.14ms, mfu 7.74%\n",
      "iter 330: loss 1.7789, time 46.24ms, mfu 7.77%\n",
      "iter 340: loss 1.7397, time 46.17ms, mfu 7.80%\n",
      "iter 350: loss 1.7276, time 46.02ms, mfu 7.84%\n",
      "iter 360: loss 1.7245, time 46.13ms, mfu 7.86%\n",
      "iter 370: loss 1.7150, time 46.13ms, mfu 7.89%\n",
      "iter 380: loss 1.6753, time 46.20ms, mfu 7.91%\n",
      "iter 390: loss 1.7135, time 46.16ms, mfu 7.93%\n",
      "iter 400: loss 1.6692, time 46.05ms, mfu 7.95%\n",
      "iter 410: loss 1.6680, time 46.04ms, mfu 7.97%\n",
      "iter 420: loss 1.6686, time 46.03ms, mfu 7.98%\n",
      "iter 430: loss 1.6682, time 46.04ms, mfu 8.00%\n",
      "iter 440: loss 1.6391, time 46.06ms, mfu 8.01%\n",
      "iter 450: loss 1.6268, time 45.92ms, mfu 8.02%\n",
      "iter 460: loss 1.6071, time 46.08ms, mfu 8.03%\n",
      "iter 470: loss 1.5788, time 46.21ms, mfu 8.04%\n",
      "iter 480: loss 1.6190, time 46.25ms, mfu 8.05%\n",
      "iter 490: loss 1.5723, time 46.30ms, mfu 8.05%\n",
      "step 500: train loss 1.5141, val loss 1.6815\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 500: loss 1.5825, time 10204.39ms, mfu 7.25%\n",
      "iter 510: loss 1.5506, time 46.24ms, mfu 7.33%\n",
      "iter 520: loss 1.5780, time 46.22ms, mfu 7.41%\n",
      "iter 530: loss 1.5295, time 46.31ms, mfu 7.48%\n",
      "iter 540: loss 1.5340, time 46.42ms, mfu 7.53%\n",
      "iter 550: loss 1.5309, time 46.34ms, mfu 7.59%\n",
      "iter 560: loss 1.5252, time 46.40ms, mfu 7.64%\n",
      "iter 570: loss 1.5217, time 46.05ms, mfu 7.69%\n",
      "iter 580: loss 1.5376, time 46.09ms, mfu 7.73%\n",
      "iter 590: loss 1.5018, time 46.29ms, mfu 7.76%\n",
      "iter 600: loss 1.4993, time 46.63ms, mfu 7.79%\n",
      "iter 610: loss 1.5255, time 46.43ms, mfu 7.82%\n",
      "iter 620: loss 1.5155, time 46.35ms, mfu 7.84%\n",
      "iter 630: loss 1.4742, time 46.11ms, mfu 7.87%\n",
      "iter 640: loss 1.5265, time 46.20ms, mfu 7.89%\n",
      "iter 650: loss 1.5393, time 46.32ms, mfu 7.91%\n",
      "iter 660: loss 1.4964, time 46.31ms, mfu 7.93%\n",
      "iter 670: loss 1.4887, time 46.37ms, mfu 7.94%\n",
      "iter 680: loss 1.4812, time 46.22ms, mfu 7.96%\n",
      "iter 690: loss 1.4665, time 46.21ms, mfu 7.97%\n",
      "iter 700: loss 1.4453, time 46.36ms, mfu 7.98%\n",
      "iter 710: loss 1.4561, time 46.57ms, mfu 7.99%\n",
      "iter 720: loss 1.4542, time 46.31ms, mfu 8.00%\n",
      "iter 730: loss 1.4701, time 46.45ms, mfu 8.00%\n",
      "iter 740: loss 1.4694, time 46.14ms, mfu 8.01%\n",
      "step 750: train loss 1.3809, val loss 1.5857\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 750: loss 1.4399, time 10254.08ms, mfu 7.22%\n",
      "iter 760: loss 1.4371, time 46.23ms, mfu 7.30%\n",
      "iter 770: loss 1.4196, time 46.28ms, mfu 7.38%\n",
      "iter 780: loss 1.4447, time 46.53ms, mfu 7.45%\n",
      "iter 790: loss 1.4414, time 46.18ms, mfu 7.51%\n",
      "iter 800: loss 1.4302, time 46.32ms, mfu 7.57%\n",
      "iter 810: loss 1.3924, time 46.52ms, mfu 7.62%\n",
      "iter 820: loss 1.4253, time 46.37ms, mfu 7.66%\n",
      "iter 830: loss 1.4316, time 46.46ms, mfu 7.70%\n",
      "iter 840: loss 1.3933, time 46.14ms, mfu 7.74%\n",
      "iter 850: loss 1.4249, time 46.22ms, mfu 7.78%\n",
      "iter 860: loss 1.4232, time 46.55ms, mfu 7.80%\n",
      "iter 870: loss 1.3936, time 46.44ms, mfu 7.83%\n",
      "iter 880: loss 1.4078, time 46.47ms, mfu 7.85%\n",
      "iter 890: loss 1.3880, time 46.21ms, mfu 7.88%\n",
      "iter 900: loss 1.4067, time 46.50ms, mfu 7.89%\n",
      "iter 910: loss 1.3765, time 46.58ms, mfu 7.91%\n",
      "iter 920: loss 1.3675, time 46.40ms, mfu 7.92%\n",
      "iter 930: loss 1.4168, time 46.25ms, mfu 7.94%\n",
      "iter 940: loss 1.3712, time 46.34ms, mfu 7.95%\n",
      "iter 950: loss 1.3642, time 46.59ms, mfu 7.96%\n",
      "iter 960: loss 1.3775, time 46.36ms, mfu 7.97%\n",
      "iter 970: loss 1.4120, time 46.25ms, mfu 7.98%\n",
      "iter 980: loss 1.3348, time 46.30ms, mfu 7.99%\n",
      "iter 990: loss 1.3867, time 46.48ms, mfu 8.00%\n",
      "step 1000: train loss 1.3009, val loss 1.5193\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 1000: loss 1.3841, time 10314.66ms, mfu 7.20%\n",
      "iter 1010: loss 1.3995, time 46.91ms, mfu 7.28%\n",
      "iter 1020: loss 1.3889, time 46.64ms, mfu 7.35%\n",
      "iter 1030: loss 1.3776, time 46.52ms, mfu 7.42%\n",
      "iter 1040: loss 1.3737, time 46.62ms, mfu 7.48%\n",
      "iter 1050: loss 1.3504, time 46.68ms, mfu 7.54%\n",
      "iter 1060: loss 1.3533, time 46.58ms, mfu 7.59%\n",
      "iter 1070: loss 1.3448, time 46.69ms, mfu 7.63%\n",
      "iter 1080: loss 1.3340, time 46.42ms, mfu 7.67%\n",
      "iter 1090: loss 1.3780, time 46.73ms, mfu 7.71%\n",
      "iter 1100: loss 1.3514, time 46.67ms, mfu 7.74%\n",
      "iter 1110: loss 1.3326, time 46.47ms, mfu 7.77%\n",
      "iter 1120: loss 1.3140, time 46.75ms, mfu 7.79%\n",
      "iter 1130: loss 1.3451, time 46.65ms, mfu 7.82%\n",
      "iter 1140: loss 1.2994, time 46.52ms, mfu 7.84%\n",
      "iter 1150: loss 1.3616, time 46.74ms, mfu 7.86%\n",
      "iter 1160: loss 1.3394, time 46.69ms, mfu 7.87%\n",
      "iter 1170: loss 1.3402, time 46.48ms, mfu 7.89%\n",
      "iter 1180: loss 1.3075, time 46.73ms, mfu 7.90%\n",
      "iter 1190: loss 1.2608, time 46.62ms, mfu 7.91%\n",
      "iter 1200: loss 1.3494, time 46.51ms, mfu 7.93%\n",
      "iter 1210: loss 1.3254, time 46.69ms, mfu 7.94%\n",
      "iter 1220: loss 1.3323, time 46.65ms, mfu 7.94%\n",
      "iter 1230: loss 1.2985, time 46.50ms, mfu 7.95%\n",
      "iter 1240: loss 1.2989, time 46.75ms, mfu 7.96%\n",
      "step 1250: train loss 1.2449, val loss 1.4967\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 1250: loss 1.2913, time 10270.19ms, mfu 7.17%\n",
      "iter 1260: loss 1.2836, time 46.59ms, mfu 7.25%\n",
      "iter 1270: loss 1.3328, time 46.65ms, mfu 7.33%\n",
      "iter 1280: loss 1.3299, time 46.48ms, mfu 7.40%\n",
      "iter 1290: loss 1.2944, time 46.72ms, mfu 7.46%\n",
      "iter 1300: loss 1.2910, time 46.68ms, mfu 7.52%\n",
      "iter 1310: loss 1.3158, time 46.50ms, mfu 7.57%\n",
      "iter 1320: loss 1.3248, time 46.70ms, mfu 7.62%\n",
      "iter 1330: loss 1.2844, time 46.69ms, mfu 7.66%\n",
      "iter 1340: loss 1.3179, time 46.54ms, mfu 7.69%\n",
      "iter 1350: loss 1.2668, time 46.68ms, mfu 7.73%\n",
      "iter 1360: loss 1.3066, time 46.66ms, mfu 7.76%\n",
      "iter 1370: loss 1.2949, time 46.50ms, mfu 7.78%\n",
      "iter 1380: loss 1.2915, time 46.73ms, mfu 7.81%\n",
      "iter 1390: loss 1.3167, time 46.68ms, mfu 7.83%\n",
      "iter 1400: loss 1.2639, time 46.52ms, mfu 7.85%\n",
      "iter 1410: loss 1.2604, time 46.71ms, mfu 7.87%\n",
      "iter 1420: loss 1.2725, time 46.67ms, mfu 7.88%\n",
      "iter 1430: loss 1.2651, time 46.47ms, mfu 7.90%\n",
      "iter 1440: loss 1.2626, time 46.73ms, mfu 7.91%\n",
      "iter 1450: loss 1.2946, time 46.65ms, mfu 7.92%\n",
      "iter 1460: loss 1.2429, time 46.50ms, mfu 7.93%\n",
      "iter 1470: loss 1.2345, time 46.72ms, mfu 7.94%\n",
      "iter 1480: loss 1.2542, time 46.69ms, mfu 7.95%\n",
      "iter 1490: loss 1.2790, time 46.51ms, mfu 7.96%\n",
      "step 1500: train loss 1.1957, val loss 1.4781\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 1500: loss 1.2879, time 10276.21ms, mfu 7.17%\n",
      "iter 1510: loss 1.2735, time 46.93ms, mfu 7.25%\n",
      "iter 1520: loss 1.2367, time 46.53ms, mfu 7.33%\n",
      "iter 1530: loss 1.2450, time 46.70ms, mfu 7.39%\n",
      "iter 1540: loss 1.2938, time 46.67ms, mfu 7.46%\n",
      "iter 1550: loss 1.2553, time 46.53ms, mfu 7.52%\n",
      "iter 1560: loss 1.2591, time 46.73ms, mfu 7.56%\n",
      "iter 1570: loss 1.2515, time 46.64ms, mfu 7.61%\n",
      "iter 1580: loss 1.2596, time 46.54ms, mfu 7.65%\n",
      "iter 1590: loss 1.2507, time 46.71ms, mfu 7.69%\n",
      "iter 1600: loss 1.2483, time 46.68ms, mfu 7.72%\n",
      "iter 1610: loss 1.2644, time 46.53ms, mfu 7.75%\n",
      "iter 1620: loss 1.2617, time 46.71ms, mfu 7.78%\n",
      "iter 1630: loss 1.2732, time 46.63ms, mfu 7.80%\n",
      "iter 1640: loss 1.2536, time 46.50ms, mfu 7.83%\n",
      "iter 1650: loss 1.2833, time 46.73ms, mfu 7.85%\n",
      "iter 1660: loss 1.2414, time 46.69ms, mfu 7.86%\n",
      "iter 1670: loss 1.2564, time 46.55ms, mfu 7.88%\n",
      "iter 1680: loss 1.2187, time 46.86ms, mfu 7.89%\n",
      "iter 1690: loss 1.2305, time 46.55ms, mfu 7.91%\n",
      "iter 1700: loss 1.2521, time 46.39ms, mfu 7.92%\n",
      "iter 1710: loss 1.2482, time 46.55ms, mfu 7.93%\n",
      "iter 1720: loss 1.2012, time 46.60ms, mfu 7.94%\n",
      "iter 1730: loss 1.2469, time 46.50ms, mfu 7.95%\n",
      "iter 1740: loss 1.2447, time 46.63ms, mfu 7.96%\n",
      "step 1750: train loss 1.1496, val loss 1.4611\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "iter 1750: loss 1.2136, time 10264.08ms, mfu 7.17%\n",
      "iter 1760: loss 1.2122, time 46.77ms, mfu 7.25%\n",
      "iter 1770: loss 1.2142, time 46.61ms, mfu 7.33%\n",
      "iter 1780: loss 1.2432, time 46.60ms, mfu 7.40%\n",
      "iter 1790: loss 1.2283, time 46.82ms, mfu 7.46%\n",
      "iter 1800: loss 1.2196, time 46.80ms, mfu 7.51%\n",
      "iter 1810: loss 1.2300, time 46.60ms, mfu 7.56%\n",
      "iter 1820: loss 1.2158, time 46.71ms, mfu 7.61%\n",
      "iter 1830: loss 1.2361, time 46.68ms, mfu 7.65%\n",
      "iter 1840: loss 1.2167, time 46.49ms, mfu 7.69%\n",
      "iter 1850: loss 1.2265, time 46.36ms, mfu 7.73%\n",
      "iter 1860: loss 1.1981, time 46.66ms, mfu 7.76%\n",
      "iter 1870: loss 1.2140, time 46.50ms, mfu 7.79%\n",
      "iter 1880: loss 1.1931, time 46.74ms, mfu 7.81%\n",
      "iter 1890: loss 1.2086, time 46.66ms, mfu 7.83%\n",
      "iter 1900: loss 1.1897, time 46.52ms, mfu 7.85%\n",
      "iter 1910: loss 1.1642, time 46.73ms, mfu 7.87%\n",
      "iter 1920: loss 1.2010, time 46.71ms, mfu 7.88%\n",
      "iter 1930: loss 1.2136, time 46.51ms, mfu 7.90%\n",
      "iter 1940: loss 1.2142, time 46.75ms, mfu 7.91%\n",
      "iter 1950: loss 1.1817, time 46.68ms, mfu 7.92%\n",
      "iter 1960: loss 1.1954, time 46.54ms, mfu 7.93%\n",
      "iter 1970: loss 1.1923, time 46.71ms, mfu 7.94%\n",
      "iter 1980: loss 1.2114, time 46.45ms, mfu 7.95%\n",
      "iter 1990: loss 1.1871, time 46.58ms, mfu 7.96%\n",
      "step 2000: train loss 1.1125, val loss 1.4668\n",
      "iter 2000: loss 1.1913, time 10141.69ms, mfu 7.17%\n",
      "iter 2010: loss 1.2043, time 46.71ms, mfu 7.25%\n",
      "iter 2020: loss 1.2031, time 46.53ms, mfu 7.33%\n",
      "iter 2030: loss 1.1904, time 46.62ms, mfu 7.40%\n",
      "iter 2040: loss 1.1694, time 46.68ms, mfu 7.46%\n",
      "iter 2050: loss 1.1955, time 46.50ms, mfu 7.52%\n",
      "iter 2060: loss 1.1889, time 46.60ms, mfu 7.57%\n",
      "iter 2070: loss 1.2006, time 46.67ms, mfu 7.62%\n",
      "iter 2080: loss 1.1700, time 46.51ms, mfu 7.66%\n",
      "iter 2090: loss 1.1986, time 46.60ms, mfu 7.70%\n",
      "iter 2100: loss 1.1697, time 46.68ms, mfu 7.73%\n",
      "iter 2110: loss 1.1862, time 46.53ms, mfu 7.76%\n",
      "iter 2120: loss 1.1574, time 17.93ms, mfu 9.07%\n",
      "iter 2130: loss 1.2007, time 46.57ms, mfu 8.97%\n",
      "iter 2140: loss 1.1590, time 46.81ms, mfu 8.87%\n",
      "iter 2150: loss 1.1671, time 46.91ms, mfu 8.78%\n",
      "iter 2160: loss 1.1606, time 46.55ms, mfu 8.71%\n",
      "iter 2170: loss 1.1863, time 46.73ms, mfu 8.64%\n",
      "iter 2180: loss 1.1726, time 46.69ms, mfu 8.57%\n",
      "iter 2190: loss 1.1602, time 46.51ms, mfu 8.52%\n",
      "iter 2200: loss 1.1847, time 46.73ms, mfu 8.47%\n",
      "iter 2210: loss 1.1744, time 46.60ms, mfu 8.43%\n",
      "iter 2220: loss 1.1732, time 46.52ms, mfu 8.39%\n",
      "iter 2230: loss 1.1676, time 46.75ms, mfu 8.35%\n",
      "iter 2240: loss 1.1465, time 46.66ms, mfu 8.32%\n",
      "step 2250: train loss 1.0738, val loss 1.4681\n",
      "iter 2250: loss 1.1703, time 10122.40ms, mfu 7.49%\n",
      "iter 2260: loss 1.1603, time 46.87ms, mfu 7.54%\n",
      "iter 2270: loss 1.1521, time 46.56ms, mfu 7.59%\n",
      "iter 2280: loss 1.1618, time 46.73ms, mfu 7.63%\n",
      "iter 2290: loss 1.1733, time 46.69ms, mfu 7.67%\n",
      "iter 2300: loss 1.1622, time 46.50ms, mfu 7.71%\n",
      "iter 2310: loss 1.2002, time 46.71ms, mfu 7.74%\n",
      "iter 2320: loss 1.1630, time 46.69ms, mfu 7.76%\n",
      "iter 2330: loss 1.1197, time 46.53ms, mfu 7.79%\n",
      "iter 2340: loss 1.1452, time 46.76ms, mfu 7.81%\n",
      "iter 2350: loss 1.1680, time 46.65ms, mfu 7.83%\n",
      "iter 2360: loss 1.1292, time 46.50ms, mfu 7.86%\n",
      "iter 2370: loss 1.1424, time 46.70ms, mfu 7.87%\n",
      "iter 2380: loss 1.1241, time 46.66ms, mfu 7.89%\n",
      "iter 2390: loss 1.1528, time 46.55ms, mfu 7.90%\n",
      "iter 2400: loss 1.1400, time 46.58ms, mfu 7.91%\n",
      "iter 2410: loss 1.1497, time 46.84ms, mfu 7.92%\n",
      "iter 2420: loss 1.1555, time 46.52ms, mfu 7.93%\n",
      "iter 2430: loss 1.1258, time 46.73ms, mfu 7.94%\n",
      "iter 2440: loss 1.1165, time 46.67ms, mfu 7.95%\n",
      "iter 2450: loss 1.1257, time 46.51ms, mfu 7.96%\n",
      "iter 2460: loss 1.1371, time 46.73ms, mfu 7.96%\n",
      "iter 2470: loss 1.1495, time 46.68ms, mfu 7.97%\n",
      "iter 2480: loss 1.1296, time 46.48ms, mfu 7.98%\n",
      "iter 2490: loss 1.1056, time 46.72ms, mfu 7.98%\n",
      "step 2500: train loss 1.0328, val loss 1.4623\n",
      "iter 2500: loss 1.1229, time 10136.59ms, mfu 7.19%\n",
      "iter 2510: loss 1.1059, time 46.54ms, mfu 7.27%\n",
      "iter 2520: loss 1.1359, time 46.88ms, mfu 7.34%\n",
      "iter 2530: loss 1.1350, time 46.94ms, mfu 7.41%\n",
      "iter 2540: loss 1.1329, time 46.52ms, mfu 7.47%\n",
      "iter 2550: loss 1.1080, time 46.76ms, mfu 7.52%\n",
      "iter 2560: loss 1.1134, time 46.69ms, mfu 7.57%\n",
      "iter 2570: loss 1.1206, time 46.65ms, mfu 7.62%\n",
      "iter 2580: loss 1.1059, time 46.70ms, mfu 7.66%\n",
      "iter 2590: loss 1.1021, time 46.68ms, mfu 7.69%\n",
      "iter 2600: loss 1.1188, time 46.58ms, mfu 7.73%\n",
      "iter 2610: loss 1.1175, time 46.63ms, mfu 7.76%\n",
      "iter 2620: loss 1.0969, time 46.67ms, mfu 7.78%\n",
      "iter 2630: loss 1.1217, time 46.55ms, mfu 7.81%\n",
      "iter 2640: loss 1.1006, time 46.76ms, mfu 7.83%\n",
      "iter 2650: loss 1.1283, time 46.60ms, mfu 7.85%\n",
      "iter 2660: loss 1.1114, time 46.55ms, mfu 7.87%\n",
      "iter 2670: loss 1.1247, time 46.66ms, mfu 7.88%\n",
      "iter 2680: loss 1.0941, time 46.67ms, mfu 7.90%\n",
      "iter 2690: loss 1.0751, time 46.56ms, mfu 7.91%\n",
      "iter 2700: loss 1.1197, time 46.73ms, mfu 7.92%\n",
      "iter 2710: loss 1.1256, time 46.65ms, mfu 7.93%\n",
      "iter 2720: loss 1.0869, time 46.52ms, mfu 7.94%\n",
      "iter 2730: loss 1.1119, time 46.62ms, mfu 7.95%\n",
      "iter 2740: loss 1.1003, time 46.62ms, mfu 7.96%\n",
      "step 2750: train loss 0.9982, val loss 1.4751\n",
      "iter 2750: loss 1.0849, time 10160.20ms, mfu 7.17%\n",
      "iter 2760: loss 1.1141, time 46.65ms, mfu 7.25%\n",
      "iter 2770: loss 1.0935, time 46.67ms, mfu 7.33%\n",
      "iter 2780: loss 1.0840, time 46.54ms, mfu 7.40%\n",
      "iter 2790: loss 1.0889, time 46.66ms, mfu 7.46%\n",
      "iter 2800: loss 1.0775, time 46.64ms, mfu 7.52%\n",
      "iter 2810: loss 1.0791, time 46.54ms, mfu 7.57%\n",
      "iter 2820: loss 1.1144, time 46.71ms, mfu 7.61%\n",
      "iter 2830: loss 1.0618, time 46.60ms, mfu 7.66%\n",
      "iter 2840: loss 1.0786, time 46.47ms, mfu 7.69%\n",
      "iter 2850: loss 1.0685, time 46.57ms, mfu 7.73%\n",
      "iter 2860: loss 1.0846, time 46.67ms, mfu 7.76%\n",
      "iter 2870: loss 1.0528, time 46.50ms, mfu 7.79%\n",
      "iter 2880: loss 1.0822, time 46.74ms, mfu 7.81%\n",
      "iter 2890: loss 1.0939, time 46.69ms, mfu 7.83%\n",
      "iter 2900: loss 1.0507, time 46.50ms, mfu 7.85%\n",
      "iter 2910: loss 1.0632, time 46.73ms, mfu 7.87%\n",
      "iter 2920: loss 1.0902, time 46.64ms, mfu 7.88%\n",
      "iter 2930: loss 1.0574, time 46.51ms, mfu 7.90%\n",
      "iter 2940: loss 1.0868, time 46.71ms, mfu 7.91%\n",
      "iter 2950: loss 1.0901, time 46.68ms, mfu 7.92%\n",
      "iter 2960: loss 1.0826, time 46.53ms, mfu 7.93%\n",
      "iter 2970: loss 1.0645, time 46.76ms, mfu 7.94%\n",
      "iter 2980: loss 1.0650, time 46.66ms, mfu 7.95%\n",
      "iter 2990: loss 1.0807, time 46.52ms, mfu 7.96%\n",
      "step 3000: train loss 0.9591, val loss 1.4763\n",
      "iter 3000: loss 1.0801, time 10136.30ms, mfu 7.17%\n",
      "iter 3010: loss 1.0792, time 46.75ms, mfu 7.25%\n",
      "iter 3020: loss 1.0950, time 46.62ms, mfu 7.33%\n",
      "iter 3030: loss 1.0841, time 46.88ms, mfu 7.39%\n",
      "iter 3040: loss 1.0548, time 46.67ms, mfu 7.46%\n",
      "iter 3050: loss 1.0535, time 46.49ms, mfu 7.51%\n",
      "iter 3060: loss 1.0719, time 46.57ms, mfu 7.57%\n",
      "iter 3070: loss 1.0677, time 46.68ms, mfu 7.61%\n",
      "iter 3080: loss 1.0622, time 46.46ms, mfu 7.66%\n",
      "iter 3090: loss 1.0551, time 46.76ms, mfu 7.69%\n",
      "iter 3100: loss 1.0545, time 46.64ms, mfu 7.72%\n",
      "iter 3110: loss 1.0869, time 46.54ms, mfu 7.76%\n",
      "iter 3120: loss 1.0609, time 46.71ms, mfu 7.78%\n",
      "iter 3130: loss 1.0465, time 46.70ms, mfu 7.80%\n",
      "iter 3140: loss 1.0580, time 46.49ms, mfu 7.83%\n",
      "iter 3150: loss 1.0227, time 46.76ms, mfu 7.85%\n",
      "iter 3160: loss 1.0794, time 46.67ms, mfu 7.86%\n",
      "iter 3170: loss 1.0677, time 46.46ms, mfu 7.88%\n",
      "iter 3180: loss 1.0303, time 46.70ms, mfu 7.90%\n",
      "iter 3190: loss 1.0266, time 46.68ms, mfu 7.91%\n",
      "iter 3200: loss 1.0285, time 46.51ms, mfu 7.92%\n",
      "iter 3210: loss 1.0478, time 46.73ms, mfu 7.93%\n",
      "iter 3220: loss 1.0334, time 46.66ms, mfu 7.94%\n",
      "iter 3230: loss 1.0441, time 46.50ms, mfu 7.95%\n",
      "iter 3240: loss 1.0753, time 46.73ms, mfu 7.96%\n",
      "step 3250: train loss 0.9256, val loss 1.4871\n",
      "iter 3250: loss 1.0545, time 10127.06ms, mfu 7.16%\n",
      "iter 3260: loss 1.0277, time 46.80ms, mfu 7.25%\n",
      "iter 3270: loss 1.0504, time 46.53ms, mfu 7.33%\n",
      "iter 3280: loss 1.0458, time 46.83ms, mfu 7.39%\n",
      "iter 3290: loss 1.0242, time 46.90ms, mfu 7.45%\n",
      "iter 3300: loss 1.0302, time 46.53ms, mfu 7.51%\n",
      "iter 3310: loss 1.0005, time 46.73ms, mfu 7.56%\n",
      "iter 3320: loss 1.0526, time 46.69ms, mfu 7.61%\n",
      "iter 3330: loss 1.0383, time 46.50ms, mfu 7.65%\n",
      "iter 3340: loss 1.0223, time 46.75ms, mfu 7.69%\n",
      "iter 3350: loss 1.0326, time 46.66ms, mfu 7.72%\n",
      "iter 3360: loss 1.0070, time 46.50ms, mfu 7.75%\n",
      "iter 3370: loss 1.0323, time 46.76ms, mfu 7.78%\n",
      "iter 3380: loss 1.0485, time 46.65ms, mfu 7.80%\n",
      "iter 3390: loss 1.0304, time 46.51ms, mfu 7.83%\n",
      "iter 3400: loss 1.0220, time 46.74ms, mfu 7.84%\n",
      "iter 3410: loss 1.0422, time 46.66ms, mfu 7.86%\n",
      "iter 3420: loss 1.0174, time 46.52ms, mfu 7.88%\n",
      "iter 3430: loss 1.0420, time 46.77ms, mfu 7.89%\n",
      "iter 3440: loss 1.0093, time 46.68ms, mfu 7.90%\n",
      "iter 3450: loss 1.0167, time 46.51ms, mfu 7.92%\n",
      "iter 3460: loss 1.0208, time 46.72ms, mfu 7.93%\n",
      "iter 3470: loss 1.0342, time 46.70ms, mfu 7.94%\n",
      "iter 3480: loss 0.9906, time 46.50ms, mfu 7.95%\n",
      "iter 3490: loss 1.0568, time 46.74ms, mfu 7.95%\n",
      "step 3500: train loss 0.8941, val loss 1.4987\n",
      "iter 3500: loss 1.0382, time 10139.41ms, mfu 7.16%\n",
      "iter 3510: loss 1.0229, time 46.64ms, mfu 7.25%\n",
      "iter 3520: loss 1.0156, time 46.48ms, mfu 7.33%\n",
      "iter 3530: loss 1.0288, time 46.74ms, mfu 7.40%\n",
      "iter 3540: loss 1.0359, time 46.62ms, mfu 7.46%\n",
      "iter 3550: loss 1.0120, time 46.47ms, mfu 7.52%\n",
      "iter 3560: loss 1.0100, time 46.73ms, mfu 7.57%\n",
      "iter 3570: loss 0.9899, time 46.66ms, mfu 7.61%\n",
      "iter 3580: loss 1.0193, time 46.50ms, mfu 7.66%\n",
      "iter 3590: loss 1.0101, time 46.75ms, mfu 7.69%\n",
      "iter 3600: loss 1.0249, time 46.66ms, mfu 7.72%\n",
      "iter 3610: loss 1.0094, time 46.53ms, mfu 7.76%\n",
      "iter 3620: loss 1.0446, time 46.77ms, mfu 7.78%\n",
      "iter 3630: loss 1.0122, time 46.66ms, mfu 7.80%\n",
      "iter 3640: loss 0.9708, time 46.52ms, mfu 7.83%\n",
      "iter 3650: loss 1.0202, time 46.71ms, mfu 7.85%\n",
      "iter 3660: loss 1.0003, time 46.67ms, mfu 7.86%\n",
      "iter 3670: loss 1.0206, time 46.53ms, mfu 7.88%\n",
      "iter 3680: loss 1.0277, time 46.76ms, mfu 7.89%\n",
      "iter 3690: loss 1.0091, time 46.65ms, mfu 7.91%\n",
      "iter 3700: loss 0.9976, time 46.53ms, mfu 7.92%\n",
      "iter 3710: loss 0.9925, time 46.77ms, mfu 7.93%\n",
      "iter 3720: loss 1.0214, time 46.68ms, mfu 7.94%\n",
      "iter 3730: loss 0.9780, time 46.46ms, mfu 7.95%\n",
      "iter 3740: loss 1.0025, time 46.74ms, mfu 7.95%\n",
      "step 3750: train loss 0.8659, val loss 1.5056\n",
      "iter 3750: loss 1.0179, time 10145.80ms, mfu 7.16%\n",
      "iter 3760: loss 0.9967, time 46.63ms, mfu 7.25%\n",
      "iter 3770: loss 1.0027, time 46.51ms, mfu 7.33%\n",
      "iter 3780: loss 0.9910, time 46.65ms, mfu 7.40%\n",
      "iter 3790: loss 1.0210, time 46.67ms, mfu 7.46%\n",
      "iter 3800: loss 0.9988, time 46.67ms, mfu 7.52%\n",
      "iter 3810: loss 0.9805, time 46.57ms, mfu 7.57%\n",
      "iter 3820: loss 0.9592, time 46.63ms, mfu 7.61%\n",
      "iter 3830: loss 0.9966, time 46.52ms, mfu 7.66%\n",
      "iter 3840: loss 0.9723, time 46.59ms, mfu 7.69%\n",
      "iter 3850: loss 1.0075, time 46.69ms, mfu 7.73%\n",
      "iter 3860: loss 0.9939, time 46.65ms, mfu 7.76%\n",
      "iter 3870: loss 0.9884, time 46.55ms, mfu 7.78%\n",
      "iter 3880: loss 0.9820, time 46.63ms, mfu 7.81%\n",
      "iter 3890: loss 0.9813, time 46.66ms, mfu 7.83%\n",
      "iter 3900: loss 0.9759, time 46.52ms, mfu 7.85%\n",
      "iter 3910: loss 0.9715, time 46.74ms, mfu 7.87%\n",
      "iter 3920: loss 0.9991, time 46.64ms, mfu 7.88%\n",
      "iter 3930: loss 1.0152, time 46.53ms, mfu 7.90%\n",
      "iter 3940: loss 0.9829, time 46.74ms, mfu 7.91%\n",
      "iter 3950: loss 0.9778, time 46.64ms, mfu 7.92%\n",
      "iter 3960: loss 0.9838, time 46.52ms, mfu 7.93%\n",
      "iter 3970: loss 0.9883, time 46.72ms, mfu 7.94%\n",
      "iter 3980: loss 0.9921, time 46.66ms, mfu 7.95%\n",
      "iter 3990: loss 0.9768, time 46.49ms, mfu 7.96%\n",
      "step 4000: train loss 0.8382, val loss 1.5201\n",
      "iter 4000: loss 0.9674, time 10120.47ms, mfu 7.17%\n",
      "iter 4010: loss 0.9842, time 46.65ms, mfu 7.25%\n",
      "iter 4020: loss 0.9735, time 46.84ms, mfu 7.33%\n",
      "iter 4030: loss 1.0014, time 46.67ms, mfu 7.39%\n",
      "iter 4040: loss 0.9932, time 46.53ms, mfu 7.46%\n",
      "iter 4050: loss 0.9776, time 46.73ms, mfu 7.51%\n",
      "iter 4060: loss 0.9810, time 46.66ms, mfu 7.56%\n",
      "iter 4070: loss 0.9805, time 46.54ms, mfu 7.61%\n",
      "iter 4080: loss 0.9712, time 46.73ms, mfu 7.65%\n",
      "iter 4090: loss 0.9909, time 46.73ms, mfu 7.69%\n",
      "iter 4100: loss 0.9679, time 46.51ms, mfu 7.72%\n",
      "iter 4110: loss 0.9602, time 46.71ms, mfu 7.75%\n",
      "iter 4120: loss 0.9887, time 46.68ms, mfu 7.78%\n",
      "iter 4130: loss 0.9791, time 46.52ms, mfu 7.80%\n",
      "iter 4140: loss 0.9769, time 46.70ms, mfu 7.83%\n",
      "iter 4150: loss 0.9407, time 46.67ms, mfu 7.84%\n",
      "iter 4160: loss 0.9617, time 46.52ms, mfu 7.86%\n",
      "iter 4170: loss 0.9654, time 46.75ms, mfu 7.88%\n",
      "iter 4180: loss 0.9805, time 46.66ms, mfu 7.89%\n",
      "iter 4190: loss 0.9496, time 46.47ms, mfu 7.91%\n",
      "iter 4200: loss 0.9467, time 46.75ms, mfu 7.92%\n",
      "iter 4210: loss 0.9829, time 46.68ms, mfu 7.93%\n",
      "iter 4220: loss 0.9823, time 46.55ms, mfu 7.94%\n",
      "iter 4230: loss 0.9951, time 46.72ms, mfu 7.95%\n",
      "iter 4240: loss 0.9863, time 46.62ms, mfu 7.95%\n",
      "step 4250: train loss 0.8161, val loss 1.5336\n",
      "iter 4250: loss 0.9658, time 10145.61ms, mfu 7.16%\n",
      "iter 4260: loss 0.9714, time 46.84ms, mfu 7.25%\n",
      "iter 4270: loss 0.9359, time 46.70ms, mfu 7.32%\n",
      "iter 4280: loss 0.9502, time 46.53ms, mfu 7.39%\n",
      "iter 4290: loss 0.9753, time 46.73ms, mfu 7.46%\n",
      "iter 4300: loss 0.9698, time 46.70ms, mfu 7.51%\n",
      "iter 4310: loss 0.9550, time 46.51ms, mfu 7.56%\n",
      "iter 4320: loss 0.9440, time 46.73ms, mfu 7.61%\n",
      "iter 4330: loss 0.9454, time 46.65ms, mfu 7.65%\n",
      "iter 4340: loss 0.9800, time 46.49ms, mfu 7.69%\n",
      "iter 4350: loss 0.9619, time 46.74ms, mfu 7.72%\n",
      "iter 4360: loss 0.9391, time 46.70ms, mfu 7.75%\n",
      "iter 4370: loss 0.9500, time 46.52ms, mfu 7.78%\n",
      "iter 4380: loss 0.9539, time 46.71ms, mfu 7.80%\n",
      "iter 4390: loss 0.9536, time 46.67ms, mfu 7.82%\n",
      "iter 4400: loss 0.9409, time 46.49ms, mfu 7.85%\n",
      "iter 4410: loss 0.9725, time 46.72ms, mfu 7.86%\n",
      "iter 4420: loss 0.9660, time 46.70ms, mfu 7.88%\n",
      "iter 4430: loss 0.9690, time 46.50ms, mfu 7.90%\n",
      "iter 4440: loss 0.9638, time 46.75ms, mfu 7.91%\n",
      "iter 4450: loss 0.9668, time 46.85ms, mfu 7.91%\n",
      "iter 4460: loss 0.9705, time 46.54ms, mfu 7.93%\n",
      "iter 4470: loss 0.9658, time 46.75ms, mfu 7.93%\n",
      "iter 4480: loss 0.9636, time 46.71ms, mfu 7.94%\n",
      "iter 4490: loss 0.9451, time 46.52ms, mfu 7.95%\n",
      "step 4500: train loss 0.7982, val loss 1.5378\n",
      "iter 4500: loss 0.9370, time 10376.89ms, mfu 7.16%\n",
      "iter 4510: loss 0.9467, time 46.93ms, mfu 7.24%\n",
      "iter 4520: loss 0.9383, time 46.55ms, mfu 7.32%\n",
      "iter 4530: loss 0.9541, time 46.73ms, mfu 7.39%\n",
      "iter 4540: loss 0.9502, time 46.50ms, mfu 7.46%\n",
      "iter 4550: loss 0.9574, time 46.69ms, mfu 7.51%\n",
      "iter 4560: loss 0.9559, time 46.62ms, mfu 7.56%\n",
      "iter 4570: loss 0.9429, time 46.77ms, mfu 7.61%\n",
      "iter 4580: loss 0.9359, time 46.51ms, mfu 7.65%\n",
      "iter 4590: loss 0.9501, time 46.66ms, mfu 7.69%\n",
      "iter 4600: loss 0.9445, time 46.59ms, mfu 7.72%\n",
      "iter 4610: loss 0.9652, time 46.55ms, mfu 7.75%\n",
      "iter 4620: loss 0.9244, time 46.65ms, mfu 7.78%\n",
      "iter 4630: loss 0.9347, time 46.66ms, mfu 7.80%\n",
      "iter 4640: loss 0.9311, time 46.55ms, mfu 7.83%\n",
      "iter 4650: loss 0.9590, time 46.68ms, mfu 7.85%\n",
      "iter 4660: loss 0.9392, time 46.63ms, mfu 7.86%\n",
      "iter 4670: loss 0.9614, time 46.42ms, mfu 7.88%\n",
      "iter 4680: loss 0.9462, time 46.75ms, mfu 7.90%\n",
      "iter 4690: loss 0.9530, time 46.67ms, mfu 7.91%\n",
      "iter 4700: loss 0.9185, time 46.50ms, mfu 7.92%\n",
      "iter 4710: loss 0.9336, time 46.73ms, mfu 7.93%\n",
      "iter 4720: loss 0.9285, time 46.70ms, mfu 7.94%\n",
      "iter 4730: loss 0.9190, time 46.50ms, mfu 7.95%\n",
      "iter 4740: loss 0.9402, time 46.72ms, mfu 7.96%\n",
      "step 4750: train loss 0.7842, val loss 1.5427\n",
      "iter 4750: loss 0.9427, time 10118.69ms, mfu 7.16%\n",
      "iter 4760: loss 0.9515, time 46.53ms, mfu 7.25%\n",
      "iter 4770: loss 0.9255, time 46.74ms, mfu 7.33%\n",
      "iter 4780: loss 0.9187, time 46.66ms, mfu 7.40%\n",
      "iter 4790: loss 0.9122, time 46.51ms, mfu 7.46%\n",
      "iter 4800: loss 0.9215, time 46.70ms, mfu 7.52%\n",
      "iter 4810: loss 0.9224, time 46.69ms, mfu 7.57%\n",
      "iter 4820: loss 0.9411, time 46.50ms, mfu 7.61%\n",
      "iter 4830: loss 0.9622, time 46.69ms, mfu 7.65%\n",
      "iter 4840: loss 0.9384, time 46.68ms, mfu 7.69%\n",
      "iter 4850: loss 0.9204, time 46.52ms, mfu 7.73%\n",
      "iter 4860: loss 0.9224, time 46.75ms, mfu 7.75%\n",
      "iter 4870: loss 0.9245, time 46.71ms, mfu 7.78%\n",
      "iter 4880: loss 0.9407, time 46.51ms, mfu 7.81%\n",
      "iter 4890: loss 0.9247, time 46.69ms, mfu 7.83%\n",
      "iter 4900: loss 0.9178, time 46.68ms, mfu 7.85%\n",
      "iter 4910: loss 0.9407, time 46.54ms, mfu 7.87%\n",
      "iter 4920: loss 0.9157, time 46.73ms, mfu 7.88%\n",
      "iter 4930: loss 0.9207, time 46.67ms, mfu 7.89%\n",
      "iter 4940: loss 0.9154, time 46.48ms, mfu 7.91%\n",
      "iter 4950: loss 0.9387, time 46.71ms, mfu 7.92%\n",
      "iter 4960: loss 0.9151, time 46.61ms, mfu 7.93%\n",
      "iter 4970: loss 0.9101, time 46.57ms, mfu 7.94%\n",
      "iter 4980: loss 0.9369, time 46.70ms, mfu 7.95%\n",
      "iter 4990: loss 0.9256, time 46.69ms, mfu 7.95%\n",
      "step 5000: train loss 0.7720, val loss 1.5664\n",
      "iter 5000: loss 0.9174, time 10139.71ms, mfu 7.16%\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "X, Y = get_batch('train', batch_size, block_size) # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    # 'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train', batch_size, block_size)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 53, 51, 43, 53]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = meta_data['stoi']\n",
    "idx_to_char = meta_data['itos']\n",
    "prompt = 'Romeo'\n",
    "prompt_idx = torch.from_numpy(np.array([char_to_idx[c] for c in prompt])).unsqueeze(0).to(device)\n",
    "prompt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo.\n",
      "\n",
      "AUTOLYCUS:\n",
      "I have done syemberding.\n",
      "\n",
      "Shepherd:\n",
      "Above brief: here's stripp'd always i' the world's death,\n",
      "bound here they did, glad one of answer.\n",
      "\n",
      "Clown:\n",
      "Nay, it may be so: he may be saying, it\n",
      "mades a special tradition of it. You'll abefall'd\n",
      "me a torth than most where may say.\n",
      "Hear most gracious true, the fearful soul\n",
      "I do not speak with my vow-fill'd blood,\n",
      "And so I dreamtly bland and I, in a tongue.\n",
      "You, lord Northumberland on Hermione,\n",
      "And may progive a craction and cut a wrongot?\n",
      "I speak not my rage; and he may live\n",
      "Can pome of her succession.\n",
      "\n",
      "KING RICHARD III:\n",
      "Should I convent notwards me now. If it could please\n",
      "The life must be cause. Go me to kell the maid\n",
      "In your country, and does tell the malady stand\n",
      "of a cready liance, that we intend you\n",
      "Much us our orisons had been an old more But\n",
      "Hath the petition of your mother shall affect\n",
      "That you are to trother.\n",
      "\n",
      "VOLUMNIA:\n",
      "Our generally,\n",
      "You please not four must be smile.\n",
      "\n",
      "VALERIA:\n",
      "And, now death; how my heart for from meaning?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_gen = model.generate(prompt_idx, max_new_tokens=1000, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = ''.join([idx_to_char[idx] for idx in np.array(sample_gen.cpu())])\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transform training loop above to re-usable training loop for any model/dataset\n",
    "# TODO: add back ddp code for training on multiple gpus (on same node)\n",
    "# TODO: use tqdm instead? would work well in-notebook but perhaps not for slurm jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=vocab_size, d_model=384, n_layers=6, n_heads_enc=4, n_heads_abs=2, dff=None,\n",
    "    symbol_retrieval='sym_attn', symbol_retrieval_kwargs=dict(num_symbols=50, n_heads=4, model_dim=384), # FIXME make names consistent: d_model, model_dim\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True)\n",
    "model = abstracttransformer_lm = AbstractTransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "AbstractTransformerLM                         [1, 1, 65]                --\n",
       "├─ModuleDict: 1-1                             --                        --\n",
       "│    └─Embedding: 2-1                         [1, 256, 384]             24,960\n",
       "│    └─Embedding: 2-2                         [256, 384]                98,304\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─AbstractEncoderBlock: 3-1         [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-3         --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-4         [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-6         --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-7         [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-9         --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-10        [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-12        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-13        [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-15        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-16        [1, 256, 384]             2,404,224\n",
       "│    └─Linear: 2-4                            [1, 1, 65]                25,025\n",
       "===============================================================================================\n",
       "Total params: 25,845,953\n",
       "Trainable params: 25,845,953\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 33.20\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 39.32\n",
       "Params size (MB): 29.58\n",
       "Estimated Total Size (MB): 68.90\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,256)), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of params 13,544,129\n"
     ]
    }
   ],
   "source": [
    "# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\n",
    "# this is the correct number (similar to TransformerLM)\n",
    "\n",
    "num_params = model.get_num_params() #sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'# of params {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can we implement in a way that torchinfo can understand? i.e., without \"recursive\" and overcounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 42, with 13,605,120 parameters\n",
      "num non-decayed parameter tensors: 62, with 37,313 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# grad scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "if compile:\n",
    "    print('compiling model...')\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6440, val loss 4.6503\n",
      "iter 0: loss 4.6687, time 33445.83ms, mfu -100.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 10: loss 3.5447, time 85.15ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 20: loss 3.0576, time 85.24ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 30: loss 2.7734, time 85.23ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 40: loss 2.6157, time 85.09ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 50: loss 2.6064, time 85.49ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 60: loss 2.5422, time 85.40ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 70: loss 2.5275, time 85.19ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 80: loss 2.5181, time 85.28ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 90: loss 2.4984, time 85.11ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 100: loss 2.4784, time 84.96ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 110: loss 2.4882, time 85.08ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 120: loss 2.4310, time 85.10ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 130: loss 2.4124, time 84.74ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 140: loss 2.3439, time 85.27ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 150: loss 2.3239, time 84.71ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 160: loss 2.2252, time 85.74ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 170: loss 2.1697, time 84.98ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 180: loss 2.1408, time 85.28ms, mfu 5.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 190: loss 2.0582, time 85.38ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 200: loss 2.0210, time 85.17ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 210: loss 1.9800, time 85.24ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 220: loss 1.9775, time 85.15ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 230: loss 1.9319, time 85.17ms, mfu 5.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 240: loss 1.8957, time 85.52ms, mfu 5.66%\n",
      "step 250: train loss 1.8150, val loss 1.9344\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 250: loss 1.8827, time 15694.62ms, mfu 5.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 260: loss 1.8691, time 85.67ms, mfu 5.15%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 270: loss 1.8347, time 85.62ms, mfu 5.20%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 280: loss 1.8219, time 85.68ms, mfu 5.24%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 290: loss 1.8369, time 85.71ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 300: loss 1.7878, time 85.80ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 310: loss 1.7384, time 85.75ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 320: loss 1.7696, time 85.69ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 330: loss 1.7380, time 85.66ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 340: loss 1.7542, time 85.65ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 350: loss 1.6572, time 85.67ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 360: loss 1.6611, time 85.78ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 370: loss 1.6678, time 85.58ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 380: loss 1.6576, time 85.59ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 390: loss 1.6755, time 85.57ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 400: loss 1.6774, time 85.98ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 410: loss 1.6586, time 85.49ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 420: loss 1.6058, time 85.70ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 430: loss 1.6480, time 85.65ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 440: loss 1.6111, time 85.37ms, mfu 5.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 450: loss 1.5668, time 85.48ms, mfu 5.57%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 460: loss 1.5885, time 85.66ms, mfu 5.58%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 470: loss 1.5943, time 85.58ms, mfu 5.58%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 480: loss 1.6032, time 85.66ms, mfu 5.59%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 490: loss 1.5746, time 85.80ms, mfu 5.59%\n",
      "step 500: train loss 1.4923, val loss 1.6612\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 500: loss 1.5376, time 15757.52ms, mfu 5.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 510: loss 1.5610, time 86.55ms, mfu 5.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 520: loss 1.5370, time 86.31ms, mfu 5.14%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 530: loss 1.5716, time 86.03ms, mfu 5.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 540: loss 1.5155, time 86.26ms, mfu 5.23%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 550: loss 1.5278, time 86.27ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 560: loss 1.4939, time 86.49ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 570: loss 1.5398, time 86.07ms, mfu 5.33%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 580: loss 1.5102, time 86.49ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 590: loss 1.4813, time 86.30ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 600: loss 1.4991, time 86.28ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 610: loss 1.5126, time 86.78ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 620: loss 1.4967, time 86.14ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 630: loss 1.4898, time 86.38ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 640: loss 1.4531, time 86.32ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 650: loss 1.4936, time 86.34ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 660: loss 1.4720, time 86.48ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 670: loss 1.4983, time 86.07ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 680: loss 1.4382, time 86.32ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 690: loss 1.4568, time 86.56ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 700: loss 1.4526, time 86.35ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 710: loss 1.4529, time 86.54ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 720: loss 1.4521, time 86.21ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 730: loss 1.4908, time 85.41ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 740: loss 1.4523, time 86.39ms, mfu 5.55%\n",
      "step 750: train loss 1.3661, val loss 1.5732\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 750: loss 1.4104, time 15772.77ms, mfu 5.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 760: loss 1.4301, time 86.33ms, mfu 5.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 770: loss 1.3984, time 86.20ms, mfu 5.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 780: loss 1.3786, time 86.74ms, mfu 5.16%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 790: loss 1.4523, time 86.07ms, mfu 5.20%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 800: loss 1.4257, time 86.19ms, mfu 5.24%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 810: loss 1.4304, time 86.66ms, mfu 5.27%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 820: loss 1.4304, time 86.55ms, mfu 5.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 830: loss 1.4210, time 86.13ms, mfu 5.33%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 840: loss 1.3667, time 86.34ms, mfu 5.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 850: loss 1.3876, time 86.60ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 860: loss 1.3908, time 86.76ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 870: loss 1.4137, time 86.13ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 880: loss 1.3817, time 86.32ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 890: loss 1.4173, time 86.33ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 900: loss 1.3603, time 86.73ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 910: loss 1.3726, time 86.15ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 920: loss 1.3447, time 86.27ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 930: loss 1.3990, time 86.47ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 940: loss 1.3543, time 86.83ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 950: loss 1.3638, time 86.41ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 960: loss 1.3513, time 86.67ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 970: loss 1.3778, time 86.64ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 980: loss 1.3555, time 86.05ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 990: loss 1.3891, time 86.31ms, mfu 5.54%\n",
      "step 1000: train loss 1.2893, val loss 1.5082\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1000: loss 1.3760, time 15771.50ms, mfu 4.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1010: loss 1.3677, time 86.40ms, mfu 5.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1020: loss 1.3605, time 86.33ms, mfu 5.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1030: loss 1.3716, time 86.63ms, mfu 5.15%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1040: loss 1.3536, time 86.80ms, mfu 5.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1050: loss 1.3273, time 86.43ms, mfu 5.23%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1060: loss 1.3558, time 86.61ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1070: loss 1.3730, time 86.76ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1080: loss 1.3318, time 85.99ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1090: loss 1.3225, time 86.21ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1100: loss 1.3532, time 86.27ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1110: loss 1.3262, time 86.97ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1120: loss 1.3383, time 86.42ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1130: loss 1.3412, time 86.43ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1140: loss 1.3427, time 86.61ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1150: loss 1.3293, time 85.79ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1160: loss 1.3444, time 86.40ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1170: loss 1.2811, time 86.63ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1180: loss 1.3196, time 86.73ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1190: loss 1.2757, time 86.48ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1200: loss 1.3213, time 86.26ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1210: loss 1.3142, time 86.34ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1220: loss 1.2956, time 86.55ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1230: loss 1.2902, time 86.48ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1240: loss 1.3114, time 86.76ms, mfu 5.53%\n",
      "step 1250: train loss 1.2394, val loss 1.4871\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1250: loss 1.3207, time 15800.13ms, mfu 4.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1260: loss 1.3094, time 86.49ms, mfu 5.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1270: loss 1.3065, time 86.44ms, mfu 5.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1280: loss 1.2987, time 86.39ms, mfu 5.14%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1290: loss 1.2991, time 86.63ms, mfu 5.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1300: loss 1.3007, time 86.78ms, mfu 5.22%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1310: loss 1.3164, time 86.44ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1320: loss 1.2446, time 86.96ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1330: loss 1.3084, time 86.74ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1340: loss 1.3008, time 86.50ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1350: loss 1.2983, time 86.67ms, mfu 5.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1360: loss 1.2968, time 86.47ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1370: loss 1.2926, time 86.07ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1380: loss 1.2973, time 86.30ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1390: loss 1.2624, time 86.68ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1400: loss 1.2658, time 86.50ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1410: loss 1.3014, time 86.15ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1420: loss 1.2560, time 86.40ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1430: loss 1.2771, time 86.61ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1440: loss 1.3164, time 86.74ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1450: loss 1.2782, time 86.49ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1460: loss 1.2974, time 86.36ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1470: loss 1.2724, time 86.37ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1480: loss 1.2690, time 86.72ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1490: loss 1.2910, time 86.20ms, mfu 5.53%\n",
      "step 1500: train loss 1.1932, val loss 1.4733\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1500: loss 1.2635, time 15767.59ms, mfu 4.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1510: loss 1.2698, time 86.54ms, mfu 5.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1520: loss 1.2249, time 86.47ms, mfu 5.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1530: loss 1.2808, time 86.66ms, mfu 5.14%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1540: loss 1.2600, time 86.69ms, mfu 5.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1550: loss 1.2400, time 86.70ms, mfu 5.22%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1560: loss 1.2585, time 86.13ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1570: loss 1.2617, time 86.13ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1580: loss 1.2249, time 86.33ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1590: loss 1.2374, time 86.57ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1600: loss 1.2666, time 86.10ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1610: loss 1.2546, time 86.39ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1620: loss 1.2517, time 86.67ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1630: loss 1.2169, time 86.72ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1640: loss 1.2313, time 86.18ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1650: loss 1.2568, time 86.19ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1660: loss 1.2469, time 86.66ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1670: loss 1.1979, time 86.77ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1680: loss 1.2343, time 86.50ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1690: loss 1.2096, time 86.67ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1700: loss 1.2347, time 86.55ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1710: loss 1.2211, time 86.09ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1720: loss 1.2135, time 86.37ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1730: loss 1.2286, time 86.43ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1740: loss 1.2211, time 86.78ms, mfu 5.53%\n",
      "step 1750: train loss 1.1478, val loss 1.4738\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1750: loss 1.2317, time 15651.58ms, mfu 4.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1760: loss 1.1970, time 86.01ms, mfu 5.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1770: loss 1.2107, time 86.27ms, mfu 5.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1780: loss 1.2204, time 86.21ms, mfu 5.15%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1790: loss 1.2468, time 86.14ms, mfu 5.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1800: loss 1.2112, time 86.13ms, mfu 5.23%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1810: loss 1.2067, time 86.06ms, mfu 5.27%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1820: loss 1.2110, time 86.41ms, mfu 5.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1830: loss 1.2156, time 86.40ms, mfu 5.33%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1840: loss 1.2108, time 86.10ms, mfu 5.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1850: loss 1.2232, time 86.08ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1860: loss 1.2246, time 85.48ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1870: loss 1.2054, time 85.75ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1880: loss 1.2091, time 85.75ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1890: loss 1.2089, time 85.71ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1900: loss 1.1855, time 85.56ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1910: loss 1.2162, time 85.74ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1920: loss 1.2013, time 85.86ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1930: loss 1.1748, time 86.00ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1940: loss 1.2429, time 85.93ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1950: loss 1.2129, time 85.94ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1960: loss 1.1965, time 84.97ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1970: loss 1.2214, time 85.90ms, mfu 5.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1980: loss 1.1809, time 85.88ms, mfu 5.57%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1990: loss 1.2120, time 85.90ms, mfu 5.57%\n",
      "step 2000: train loss 1.1120, val loss 1.4627\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2000: loss 1.1874, time 15807.79ms, mfu 5.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2010: loss 1.1852, time 86.53ms, mfu 5.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2020: loss 1.2004, time 86.54ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2030: loss 1.1810, time 86.43ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2040: loss 1.2302, time 86.43ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2050: loss 1.1915, time 86.55ms, mfu 5.25%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2060: loss 1.1886, time 86.02ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2070: loss 1.1651, time 86.67ms, mfu 5.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2080: loss 1.1707, time 86.62ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2090: loss 1.2047, time 86.72ms, mfu 5.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2100: loss 1.1865, time 86.38ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2110: loss 1.2029, time 86.48ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2120: loss 1.1718, time 86.74ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2130: loss 1.1882, time 86.51ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2140: loss 1.2011, time 86.49ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2150: loss 1.1755, time 86.76ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2160: loss 1.1556, time 86.49ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2170: loss 1.1465, time 86.61ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2180: loss 1.1739, time 86.71ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2190: loss 1.1416, time 86.46ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2200: loss 1.1673, time 86.52ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2210: loss 1.1608, time 86.79ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2220: loss 1.1590, time 86.52ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2230: loss 1.1816, time 86.43ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2240: loss 1.1529, time 86.60ms, mfu 5.53%\n",
      "step 2250: train loss 1.0751, val loss 1.4538\n",
      "saving checkpoint to ../out/out-shakespeare-char\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2250: loss 1.1771, time 15804.19ms, mfu 4.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2260: loss 1.1723, time 86.45ms, mfu 5.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2270: loss 1.1559, time 86.67ms, mfu 5.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2280: loss 1.1436, time 86.76ms, mfu 5.14%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2290: loss 1.1739, time 86.48ms, mfu 5.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2300: loss 1.1589, time 86.71ms, mfu 5.22%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2310: loss 1.1579, time 86.75ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2320: loss 1.1406, time 86.52ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2330: loss 1.1489, time 86.28ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2340: loss 1.1496, time 86.48ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2350: loss 1.1724, time 86.42ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2360: loss 1.1473, time 86.41ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2370: loss 1.1368, time 86.66ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2380: loss 1.1400, time 86.75ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2390: loss 1.1523, time 86.52ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2400: loss 1.1499, time 86.66ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2410: loss 1.1417, time 86.64ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2420: loss 1.1396, time 86.52ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2430: loss 1.1387, time 86.26ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2440: loss 1.1254, time 86.62ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2450: loss 1.1291, time 86.43ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2460: loss 1.1145, time 86.49ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2470: loss 1.1222, time 86.64ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2480: loss 1.1332, time 86.50ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2490: loss 1.1017, time 86.36ms, mfu 5.53%\n",
      "step 2500: train loss 1.0370, val loss 1.4689\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2500: loss 1.1449, time 15642.31ms, mfu 4.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2510: loss 1.1469, time 85.79ms, mfu 5.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2520: loss 1.1453, time 85.90ms, mfu 5.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2530: loss 1.1431, time 85.67ms, mfu 5.15%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2540: loss 1.1302, time 85.80ms, mfu 5.20%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2550: loss 1.1169, time 85.78ms, mfu 5.24%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2560: loss 1.1252, time 85.81ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2570: loss 1.1084, time 85.85ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2580: loss 1.1573, time 85.93ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2590: loss 1.1162, time 85.86ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2600: loss 1.1073, time 85.91ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2610: loss 1.1170, time 85.89ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2620: loss 1.1169, time 85.90ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2630: loss 1.1324, time 85.85ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2640: loss 1.1361, time 85.92ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2650: loss 1.1272, time 85.87ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2660: loss 1.1307, time 86.18ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2670: loss 1.1052, time 85.89ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2680: loss 1.1251, time 85.87ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2690: loss 1.1229, time 85.88ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2700: loss 1.0998, time 85.60ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2710: loss 1.1237, time 85.85ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2720: loss 1.1124, time 85.88ms, mfu 5.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2730: loss 1.0931, time 85.87ms, mfu 5.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2740: loss 1.1296, time 85.89ms, mfu 5.57%\n",
      "step 2750: train loss 1.0031, val loss 1.4777\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2750: loss 1.1020, time 15668.63ms, mfu 5.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2760: loss 1.0605, time 86.46ms, mfu 5.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2770: loss 1.0923, time 86.37ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2780: loss 1.1253, time 86.83ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2790: loss 1.1309, time 85.82ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2800: loss 1.0925, time 86.30ms, mfu 5.25%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2810: loss 1.1027, time 86.13ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2820: loss 1.1056, time 86.13ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2830: loss 1.1067, time 86.32ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2840: loss 1.1120, time 86.36ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2850: loss 1.0969, time 86.32ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2860: loss 1.1068, time 85.80ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2870: loss 1.1024, time 86.14ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2880: loss 1.1123, time 86.12ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2890: loss 1.1130, time 86.47ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2900: loss 1.0914, time 86.21ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2910: loss 1.0777, time 86.88ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2920: loss 1.0720, time 85.81ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2930: loss 1.0626, time 86.29ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2940: loss 1.0885, time 86.12ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2950: loss 1.0990, time 86.01ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2960: loss 1.0926, time 86.32ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2970: loss 1.0875, time 86.28ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2980: loss 1.0728, time 86.47ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2990: loss 1.0840, time 85.89ms, mfu 5.55%\n",
      "step 3000: train loss 0.9690, val loss 1.4803\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3000: loss 1.0916, time 15652.73ms, mfu 5.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3010: loss 1.0676, time 86.12ms, mfu 5.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3020: loss 1.0946, time 86.27ms, mfu 5.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3030: loss 1.0922, time 86.50ms, mfu 5.16%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3040: loss 1.0614, time 86.15ms, mfu 5.20%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3050: loss 1.0669, time 86.13ms, mfu 5.24%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3060: loss 1.0939, time 86.16ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3070: loss 1.0693, time 86.21ms, mfu 5.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3080: loss 1.0655, time 85.81ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3090: loss 1.0540, time 85.76ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3100: loss 1.0612, time 85.70ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3110: loss 1.0774, time 85.58ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3120: loss 1.0599, time 85.79ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3130: loss 1.0458, time 89.23ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3140: loss 1.0682, time 85.76ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3150: loss 1.0686, time 85.94ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3160: loss 1.0529, time 86.04ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3170: loss 1.0592, time 85.96ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3180: loss 1.0654, time 85.89ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3190: loss 1.0585, time 85.84ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3200: loss 1.0561, time 86.02ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3210: loss 1.0503, time 86.10ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3220: loss 1.0506, time 86.16ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3230: loss 1.0750, time 86.14ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3240: loss 1.0635, time 85.70ms, mfu 5.56%\n",
      "step 3250: train loss 0.9334, val loss 1.4861\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3250: loss 1.0585, time 15661.77ms, mfu 5.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3260: loss 1.0460, time 86.25ms, mfu 5.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3270: loss 1.0317, time 85.75ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3280: loss 1.0677, time 86.02ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3290: loss 1.0372, time 86.07ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3300: loss 1.0492, time 85.97ms, mfu 5.25%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3310: loss 1.0465, time 85.99ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3320: loss 1.0480, time 85.97ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3330: loss 1.0391, time 85.90ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3340: loss 1.0510, time 86.20ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3350: loss 1.0361, time 86.29ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3360: loss 1.0195, time 86.13ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3370: loss 1.0581, time 86.24ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3380: loss 1.0462, time 86.22ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3390: loss 1.0341, time 86.27ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3400: loss 1.0356, time 86.22ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3410: loss 1.0417, time 85.78ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3420: loss 1.0590, time 85.70ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3430: loss 1.0211, time 85.74ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3440: loss 1.0439, time 85.77ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3450: loss 1.0413, time 86.00ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3460: loss 1.0502, time 86.01ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3470: loss 0.9995, time 85.88ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3480: loss 1.0082, time 85.90ms, mfu 5.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3490: loss 1.0191, time 86.65ms, mfu 5.56%\n",
      "step 3500: train loss 0.9035, val loss 1.5011\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3500: loss 1.0277, time 15652.67ms, mfu 5.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3510: loss 1.0299, time 86.25ms, mfu 5.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3520: loss 1.0095, time 86.10ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3530: loss 1.0235, time 86.10ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3540: loss 1.0500, time 86.19ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3550: loss 1.0060, time 86.42ms, mfu 5.25%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3560: loss 0.9998, time 86.34ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3570: loss 1.0239, time 86.23ms, mfu 5.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3580: loss 1.0212, time 85.98ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3590: loss 1.0220, time 86.10ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3600: loss 1.0197, time 86.17ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3610: loss 1.0272, time 86.56ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3620: loss 1.0133, time 86.34ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3630: loss 1.0079, time 86.27ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3640: loss 1.0221, time 85.91ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3650: loss 1.0055, time 86.06ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3660: loss 1.0440, time 86.14ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3670: loss 1.0005, time 86.43ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3680: loss 1.0078, time 86.37ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3690: loss 1.0317, time 86.20ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3700: loss 0.9950, time 86.29ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3710: loss 1.0253, time 86.11ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3720: loss 1.0081, time 86.46ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3730: loss 1.0082, time 86.34ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3740: loss 0.9975, time 86.65ms, mfu 5.54%\n",
      "step 3750: train loss 0.8755, val loss 1.5191\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3750: loss 1.0168, time 15654.36ms, mfu 4.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3760: loss 1.0565, time 86.21ms, mfu 5.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3770: loss 0.9916, time 86.10ms, mfu 5.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3780: loss 1.0186, time 86.03ms, mfu 5.16%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3790: loss 1.0094, time 86.17ms, mfu 5.20%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3800: loss 1.0131, time 86.24ms, mfu 5.24%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3810: loss 1.0067, time 86.31ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3820: loss 0.9902, time 86.27ms, mfu 5.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3830: loss 0.9990, time 85.68ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3840: loss 1.0009, time 85.76ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3850: loss 1.0131, time 85.72ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3860: loss 0.9816, time 85.86ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3870: loss 0.9684, time 85.98ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3880: loss 0.9876, time 85.97ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3890: loss 0.9729, time 85.93ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3900: loss 0.9924, time 86.15ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3910: loss 1.0020, time 86.14ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3920: loss 0.9952, time 86.35ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3930: loss 1.0166, time 86.15ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3940: loss 1.0000, time 85.92ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3950: loss 0.9793, time 86.07ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3960: loss 1.0084, time 86.20ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3970: loss 0.9770, time 86.18ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3980: loss 0.9795, time 85.71ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3990: loss 1.0038, time 85.69ms, mfu 5.56%\n",
      "step 4000: train loss 0.8487, val loss 1.5254\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4000: loss 0.9992, time 15661.32ms, mfu 5.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4010: loss 0.9950, time 86.21ms, mfu 5.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4020: loss 0.9732, time 86.54ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4030: loss 0.9720, time 86.09ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4040: loss 0.9741, time 86.50ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4050: loss 0.9871, time 86.13ms, mfu 5.25%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4060: loss 0.9634, time 85.48ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4070: loss 0.9839, time 85.73ms, mfu 5.32%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4080: loss 0.9858, time 85.85ms, mfu 5.35%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4090: loss 0.9881, time 86.05ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4100: loss 0.9591, time 85.97ms, mfu 5.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4110: loss 0.9780, time 85.97ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4120: loss 0.9824, time 85.92ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4130: loss 0.9612, time 85.93ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4140: loss 0.9830, time 85.82ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4150: loss 0.9671, time 86.12ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4160: loss 0.9644, time 85.99ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4170: loss 0.9703, time 86.21ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4180: loss 0.9769, time 86.18ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4190: loss 0.9741, time 86.64ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4200: loss 0.9955, time 86.14ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4210: loss 0.9652, time 85.63ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4220: loss 0.9577, time 85.66ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4230: loss 0.9679, time 86.53ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4240: loss 0.9927, time 85.95ms, mfu 5.56%\n",
      "step 4250: train loss 0.8264, val loss 1.5453\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4250: loss 0.9830, time 15670.31ms, mfu 5.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4260: loss 0.9741, time 86.21ms, mfu 5.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4270: loss 0.9752, time 86.22ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4280: loss 0.9380, time 86.07ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4290: loss 0.9529, time 86.35ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4300: loss 0.9578, time 86.22ms, mfu 5.25%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4310: loss 0.9692, time 86.60ms, mfu 5.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4320: loss 0.9642, time 85.90ms, mfu 5.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4330: loss 1.0007, time 86.13ms, mfu 5.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4340: loss 0.9825, time 86.06ms, mfu 5.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4350: loss 0.9806, time 86.76ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4360: loss 0.9588, time 86.48ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4370: loss 0.9616, time 86.22ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4380: loss 0.9750, time 85.98ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4390: loss 0.9766, time 85.99ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4400: loss 0.9555, time 86.13ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4410: loss 0.9456, time 86.43ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4420: loss 0.9845, time 86.28ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4430: loss 0.9488, time 86.47ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4440: loss 0.9563, time 85.90ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4450: loss 0.9770, time 86.23ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4460: loss 0.9936, time 85.99ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4470: loss 0.9369, time 86.38ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4480: loss 0.9790, time 86.29ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4490: loss 0.9585, time 86.60ms, mfu 5.55%\n",
      "step 4500: train loss 0.8131, val loss 1.5496\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4500: loss 0.9667, time 15674.98ms, mfu 4.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4510: loss 0.9594, time 85.58ms, mfu 5.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4520: loss 0.9702, time 85.61ms, mfu 5.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4530: loss 0.9399, time 85.60ms, mfu 5.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4540: loss 0.9516, time 85.72ms, mfu 5.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4550: loss 0.9433, time 85.69ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4560: loss 0.9627, time 85.62ms, mfu 5.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4570: loss 0.9607, time 85.62ms, mfu 5.33%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4580: loss 0.9522, time 85.66ms, mfu 5.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4590: loss 0.9595, time 85.60ms, mfu 5.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4600: loss 0.9508, time 85.70ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4610: loss 0.9826, time 85.60ms, mfu 5.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4620: loss 0.9617, time 85.60ms, mfu 5.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4630: loss 0.9629, time 85.79ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4640: loss 0.9587, time 85.93ms, mfu 5.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4650: loss 0.9465, time 84.92ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4660: loss 0.9505, time 85.96ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4670: loss 0.9494, time 85.88ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4680: loss 0.9673, time 85.62ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4690: loss 0.9459, time 85.86ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4700: loss 0.9228, time 85.90ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4710: loss 0.9433, time 85.94ms, mfu 5.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4720: loss 0.9274, time 85.79ms, mfu 5.57%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4730: loss 0.9256, time 85.88ms, mfu 5.57%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4740: loss 0.9445, time 85.92ms, mfu 5.58%\n",
      "step 4750: train loss 0.7946, val loss 1.5556\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4750: loss 0.9567, time 15673.92ms, mfu 5.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4760: loss 0.9434, time 86.48ms, mfu 5.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4770: loss 0.9413, time 85.88ms, mfu 5.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4780: loss 0.9708, time 85.81ms, mfu 5.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4790: loss 0.9327, time 85.77ms, mfu 5.22%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4800: loss 0.9431, time 86.30ms, mfu 5.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4810: loss 0.9661, time 86.02ms, mfu 5.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4820: loss 0.9577, time 85.98ms, mfu 5.33%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4830: loss 0.9608, time 85.92ms, mfu 5.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4840: loss 0.9429, time 85.74ms, mfu 5.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4850: loss 0.9517, time 86.14ms, mfu 5.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4860: loss 0.9488, time 86.31ms, mfu 5.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4870: loss 0.9429, time 86.18ms, mfu 5.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4880: loss 0.9156, time 86.36ms, mfu 5.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4890: loss 0.9408, time 86.11ms, mfu 5.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4900: loss 0.9319, time 86.49ms, mfu 5.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4910: loss 0.9568, time 85.81ms, mfu 5.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4920: loss 0.9473, time 85.68ms, mfu 5.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4930: loss 0.9617, time 85.82ms, mfu 5.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4940: loss 0.9447, time 86.07ms, mfu 5.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4950: loss 0.9606, time 85.95ms, mfu 5.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4960: loss 0.9124, time 85.84ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4970: loss 0.9135, time 86.50ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4980: loss 0.9361, time 86.18ms, mfu 5.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4990: loss 0.9504, time 86.18ms, mfu 5.56%\n",
      "step 5000: train loss 0.7857, val loss 1.5606\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 5000: loss 0.9512, time 15655.16ms, mfu 5.01%\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "X, Y = get_batch('train', batch_size, block_size) # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    # 'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train', batch_size, block_size)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 53, 51, 43, 53]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx = meta_data['stoi']\n",
    "prompt = 'Romeo'\n",
    "prompt_idx = torch.from_numpy(np.array([char_to_idx[c] for c in prompt])).unsqueeze(0).to('cuda')\n",
    "prompt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo is too right\n",
      "To say that truly, as I said, yet it would unplot\n",
      "A tied where I loved you there.\n",
      "\n",
      "BRUTUS:\n",
      "For shame!\n",
      "All this is the penural great Apollo's corse,\n",
      "That is not the thought to scarlet me now\n",
      "That no hage hath caused me and lament\n",
      "To my blood of imprembrance. The gates hands revolt to\n",
      "The glass of mine on.\n",
      "\n",
      "MARCIUS:\n",
      "They have they do curse they find the court: they will;\n",
      "No more, love had they say, they follow the rise,\n",
      "CaAnd now not.\n",
      "\n",
      "MENENIUS:\n",
      "I am so breath of the luke: I'll be a very coll,\n",
      "Apply in one stripe to be a child, on still-sheep-charge:\n",
      "That out-peach does thee with thy wealth follow'd,\n",
      "Were thy wilt to seek against what thou hast\n",
      "Suspicion as thou a palm thine body.\n",
      "\n",
      "SICINIUS:\n",
      "A bloody is cheered at the meaner, and infrint\n",
      "Thy breck of Corioli and no more-ent air.\n",
      "The commons' sharp-corn unon thy castle hurt by\n",
      "the file stone: if give not the wisest warrant mine\n",
      "I swear from off, something the weariest true,\n",
      "And fine outward in sucributation thstain;\n",
      "Only, if\n"
     ]
    }
   ],
   "source": [
    "sample_gen = model.generate(prompt_idx, max_new_tokens=1000, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = ''.join([idx_to_char[idx] for idx in np.array(sample_gen.cpu())])\n",
    "print(sample_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
