{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from  tqdm import tqdm\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "import sys; sys.path.append('../..')\n",
    "from language_models import TransformerLM, AbstractTransformerLM, configure_optimizers\n",
    "from train_utils import train_model\n",
    "from utils.pl_tqdm_progbar import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available:  True\n",
      "device count:  1\n",
      "current device name:  NVIDIA A100 80GB PCIe\n",
      "Memory Usage:\n",
      "\tAllocated: 1.1 GB\n",
      "\tReserved:    3.1 GB\n"
     ]
    }
   ],
   "source": [
    "print('cuda available: ', torch.cuda.is_available())\n",
    "print('device count: ', torch.cuda.device_count())\n",
    "print('current device name: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print('Memory Usage:')\n",
    "print('\\tAllocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('\\tReserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 500 # keep frequent because we'll overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "\n",
    "# system\n",
    "device = 'cuda'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "compile = True\n",
    "\n",
    "# evaluation and output\n",
    "out_dir = '../out/tiny_stories'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "eval_interval = 250 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "log_interval = 10 # don't print too too often\n",
    "\n",
    "# we expect to overfit on this small dataset, so only save when val improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = 'abstract_transformer--tiny_stories'\n",
    "\n",
    "# optimization hyperparams\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "max_iters = 5000\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "weight_decay = 1e-1\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "warmup_iters = 100\n",
    "gradient_accumulation_steps = 1 # accumulate gradients over this many steps. simulates larger batch size\n",
    "\n",
    "# batch size and block size\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "# DDP (distributed data parallel) training\n",
    "ddp = False\n",
    "master_process = True\n",
    "\n",
    "# TODO: set up DDP for future experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use tiktoken instead; much faster\n",
    "dataset = dataset.map(lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=block_size+1), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c802e667cc34879a78c77b88f79c59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "dataset = dataset.map(lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=block_size+1), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLES\n",
      "[CLS] One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \" Mom, I found this needle. Can you share it with me and sew my shirt? \" Her mom smiled and said, \" Yes, Lily, we can share the needle and fix your shirt. \" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong. One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn. Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \" Hi, I am Fin. Do you want to play? \" asked the little fish. The crab looked at Fin and said, \" No, I don't want to play. I am cold and I don't feel fine. \" Fin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \" Please, sun, help my new friend feel fine and not freeze! \" The sun heard Fin's call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \" Thank you, little fish, for making me feel fine. I don't feel like I will freeze now. Let's play together! \" And so, Fin and the crab played and became good friends. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] Once upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees. One day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \" You are special because you have sweet cherries that everyone loves. \" The cherry tree started to feel a little better. As time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] Once upon a time, there was a little girl named Lily. Lily liked to pretend she was a popular princess. She lived in a big castle with her best friends, a cat and a dog. One day, while playing in the castle, Lily found a big cobweb. The cobweb was in the way of her fun game. She wanted to get rid of it, but she was scared of the spider that lived there. Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. The spider was sad, but it found a new home outside. Lily, the cat, and the dog were happy they could play without the cobweb in the way. And they all lived happily ever after. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"EXAMPLES\")\n",
    "\n",
    "for x in dataset['train']['input_ids'][:5]:\n",
    "    print(tokenizer.decode(x))\n",
    "    print('-'*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset['validation'], batch_size=batch_size, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to handle padding token? ignore in loss/perplexity/etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_on_step = True\n",
    "\n",
    "class LitLanguageModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text = batch['input_ids']\n",
    "        x, y = text[:, :-1], text[:, 1:]\n",
    "\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, on_step=log_on_step, on_epoch=True)\n",
    "        self.log('train_perplexity', perplexity, prog_bar=True, logger=True, on_step=log_on_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text = batch['input_ids']\n",
    "        x, y = text[:, :-1], text[:, 1:]\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        self.log(f\"val_loss\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f'val_perplexity', perplexity, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        text = batch['input_ids']\n",
    "        x, y = text[:, :-1], text[:, 1:]\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        self.log(f\"test_loss\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f'test_perplexity', perplexity, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "TransformerLM                                 --\n",
       "├─ModuleDict: 1-1                             --\n",
       "│    └─Embedding: 2-1                         22,268,928\n",
       "│    │    └─Linear: 3-1                       22,297,924\n",
       "│    └─Embedding: 2-2                         196,608\n",
       "│    └─Dropout: 2-3                           --\n",
       "│    └─ModuleList: 2-4                        --\n",
       "│    │    └─EncoderBlock: 3-2                 7,085,568\n",
       "│    │    └─EncoderBlock: 3-3                 7,085,568\n",
       "│    └─Linear: 2-5                            (recursive)\n",
       "======================================================================\n",
       "Total params: 58,934,596\n",
       "Trainable params: 58,934,596\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=768, n_layers=2, n_heads=12, dff=None,\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=block_size, bias=True)\n",
    "model = transformer_lm = TransformerLM(**model_args).to(device)\n",
    "# NOTE: embedding layer/output layer account for much of the params... (but they use weight sharing)\n",
    "torchinfo.summary(model, device='cuda') # input_data=[torch.randint(0, 10, size=(1,block_size))]*2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitLanguageModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33121"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m group_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m run_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTransformerLM (new implementation)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m run \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39mwandb_project, group\u001b[39m=\u001b[39mgroup_name, name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m      6\u001b[0m     config\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m: group_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args})\n\u001b[1;32m      8\u001b[0m wandb_logger \u001b[39m=\u001b[39m WandbLogger(experiment\u001b[39m=\u001b[39mrun, log_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# name=run_name, project=wandb_project,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# wandb_logger.watch(model, log_graph=False)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "group_name = None\n",
    "run_name = 'TransformerLM (new implementation)'\n",
    "run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "    config={'group': group_name, **model_args})\n",
    "\n",
    "wandb_logger = WandbLogger(experiment=run, log_model=False) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "wandb_logger = None\n",
    "callbacks = [\n",
    "    TQDMProgressBar()\n",
    "    # LitProgressBar()\n",
    "    # L.pytorch.callbacks.TQDMProgressBar(refresh_rate=50)\n",
    "    # L.pytorch.callbacks.RichProgressBar()\n",
    "]\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    max_epochs=n_epochs, enable_checkpointing=False, enable_model_summary=True,\n",
    "    enable_progress_bar=True, callbacks=callbacks, logger=False,\n",
    "    accumulate_grad_batches=1, benchmark=True, gradient_clip_val=None,\n",
    "    log_every_n_steps=10, max_steps=5000, val_check_interval=eval_interval)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    **trainer_kwargs\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Once upon a time, there was a cheerful fairy called Jack who lived in a forest. Every day she fluttered around the woods. One day Jack overheard out of her quicker pie box. \" Where did you get ready? \" replied Jack. \" Ok, \" replied Jack. \" Follow me! \" there Once Jack followed the bush, Jack spotted a tree of moved a tune. \" Come come closer and be ready! \" said Jack. Jack and the bird flew together and John went out together. But when they finally got\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time,'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=100, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] There once was a girl named Lucy. One day, Lucy's mommy and daddy told her that some were very creative nests waved and said \" That's a wonderful opinion! \" Lucy was so excited to learn about the next tutor. He nodded his head and said \" Today, \" Daddy! \" [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'There once was a girl named '\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=100, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] On a rainy day, boy morning and Danny wanted to play a game. He saw some birds flying high towards him. \" Let us go discover the beautiful has risen and see the pretty colors! \", Danny, it was too far and too far away to take a break. \" Let's turn this ; we can only watch it \", said Bill. \" That sounds fun! \" \" Let us watch! \" They said [UNK] wait until finally, mum rang. She said, \" See was it? \" Rufus\n"
     ]
    }
   ],
   "source": [
    "prompt = 'On a rainy day,'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=100, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Emma is a curious person. She likes to explore and learn new things. One day, Emma decided to go on an adventure with her. As she is in the outdoors, Emma says to her mom, \" Emma, I am so excited, I can go! \" Her mom says, \" But we can have a conversation promise. We have to pack a skirt together, \" Emma and her mom watch whoonies there and watch the birds! The uniforms are useful between the trees coming out. Emma thinks this was a moment from\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Emma is a curious person.'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=100, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Transformer Model (Symbolic Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=384, n_layers=6, n_heads_enc=4, n_heads_abs=2, dff=None,\n",
    "    symbol_retrieval='sym_attn', symbol_retrieval_kwargs=dict(num_symbols=50, n_heads=4, model_dim=384), # FIXME make names consistent: d_model, model_dim\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True)\n",
    "model = abstracttransformer_lm = AbstractTransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "AbstractTransformerLM                         [1, 1, 28996]             --\n",
       "├─ModuleDict: 1-1                             --                        --\n",
       "│    └─Embedding: 2-1                         [1, 256, 384]             11,134,464\n",
       "│    └─Embedding: 2-2                         [256, 384]                98,304\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─AbstractEncoderBlock: 3-1         [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-3         --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-4         [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-6         --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-7         [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-9         --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-10        [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-12        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-13        [1, 256, 384]             2,404,224\n",
       "│    │    └─AbstractEncoderBlock: 3-14        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-15        --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-16        [1, 256, 384]             2,404,224\n",
       "│    └─Linear: 2-4                            [1, 1, 28996]             11,163,460\n",
       "===============================================================================================\n",
       "Total params: 48,093,892\n",
       "Trainable params: 48,093,892\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 55.45\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 39.55\n",
       "Params size (MB): 118.57\n",
       "Estimated Total Size (MB): 158.13\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,block_size)), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of params 35,792,068\n"
     ]
    }
   ],
   "source": [
    "# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\n",
    "# this is the correct number (similar to TransformerLM)\n",
    "\n",
    "num_params = model.get_num_params() #sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'# of params {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can we implement in a way that torchinfo can understand? i.e., without \"recursive\" and overcounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 42, with 35,824,128 parameters\n",
      "num non-decayed parameter tensors: 62, with 66,244 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# grad scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = dict(\n",
    "    model=model, get_batch=get_batch, batch_size=batch_size, max_iters=max_iters,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr, eval_model=eval_model,\n",
    "    compile=True, grad_clip=0, gradient_accumulation_steps=1,\n",
    "    eval_main_metric='val/loss', eval_interval=eval_interval, always_save_checkpoint=always_save_checkpoint, out_dir=out_dir,\n",
    "    log_interval=10, wandb_log=True, wandb_init_kwargs=dict(project=wandb_project, name='AbstractTransformerLM'), \n",
    "    ckpt_dict=dict(model_args=model_args), track_mfu=True,\n",
    "    master_process=True, ddp=False, device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mythd0p7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d5627a96a943fd85c17a86b38d14ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.077 MB of 0.077 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AbstractTransformerLM</strong> at: <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/mythd0p7' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/mythd0p7</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240118_214100-mythd0p7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mythd0p7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a082c49f3264503852b06adfe5e5e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112226033583283, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/gibbs/project/lafferty/ma2393/abstract_transformer/experiments/wandb/run-20240118_214204-zedqv5g5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/zedqv5g5' target=\"_blank\">AbstractTransformerLM</a></strong> to <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/zedqv5g5' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/zedqv5g5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "step 0: train loss 10.7940, val loss 10.8234\n",
      "iter 0: loss 10.8110, time 34751.53ms, mfu -100.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 10: loss 7.0523, time 67.64ms, mfu 17.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 20: loss 5.4574, time 66.70ms, mfu 17.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 30: loss 3.8451, time 67.47ms, mfu 17.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 40: loss 3.1769, time 67.91ms, mfu 17.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 50: loss 2.6737, time 66.91ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 60: loss 2.1552, time 67.41ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 70: loss 1.6518, time 67.89ms, mfu 17.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 80: loss 1.1627, time 66.89ms, mfu 17.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 90: loss 0.7457, time 66.89ms, mfu 17.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 100: loss 0.3881, time 68.36ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 110: loss 0.1610, time 67.62ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 120: loss 0.0763, time 67.20ms, mfu 17.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 130: loss 0.0499, time 67.42ms, mfu 17.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 140: loss 0.0385, time 67.94ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 150: loss 0.0316, time 67.89ms, mfu 17.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 160: loss 0.0295, time 67.20ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 170: loss 0.0265, time 67.26ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 180: loss 0.0264, time 67.45ms, mfu 17.53%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 190: loss 0.0247, time 68.35ms, mfu 17.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 200: loss 0.0237, time 67.95ms, mfu 17.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 210: loss 0.0222, time 67.30ms, mfu 17.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 220: loss 0.0221, time 67.62ms, mfu 17.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 230: loss 0.0225, time 67.70ms, mfu 17.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 240: loss 0.0219, time 68.07ms, mfu 17.50%\n",
      "step 250: train loss 0.0174, val loss 5.9976\n",
      "saving checkpoint to ../out/tiny_stories\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 250: loss 0.0224, time 10150.48ms, mfu 15.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 260: loss 0.0209, time 68.00ms, mfu 15.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 270: loss 0.0215, time 68.17ms, mfu 16.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 280: loss 0.0206, time 67.75ms, mfu 16.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 290: loss 0.0207, time 67.41ms, mfu 16.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 300: loss 0.0213, time 67.14ms, mfu 16.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 310: loss 0.0209, time 67.38ms, mfu 16.58%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 320: loss 0.0195, time 67.29ms, mfu 16.68%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 330: loss 0.0195, time 67.61ms, mfu 16.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 340: loss 0.0191, time 68.00ms, mfu 16.83%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 350: loss 0.0196, time 68.27ms, mfu 16.88%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 360: loss 0.0201, time 68.43ms, mfu 16.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 370: loss 0.0200, time 68.23ms, mfu 16.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 380: loss 0.0197, time 68.33ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 390: loss 0.0199, time 68.12ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 400: loss 0.0196, time 68.05ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 410: loss 0.0196, time 67.85ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 420: loss 0.0193, time 67.37ms, mfu 17.15%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 430: loss 0.0199, time 67.39ms, mfu 17.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 440: loss 0.0196, time 67.46ms, mfu 17.23%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 450: loss 0.0209, time 67.58ms, mfu 17.26%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 460: loss 0.0204, time 67.22ms, mfu 17.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 470: loss 0.0208, time 67.64ms, mfu 17.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 480: loss 0.0208, time 67.19ms, mfu 17.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 490: loss 0.0195, time 67.23ms, mfu 17.37%\n",
      "step 500: train loss 0.0171, val loss 6.7413\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 500: loss 0.0200, time 9597.80ms, mfu 15.64%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 510: loss 0.0202, time 68.59ms, mfu 15.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 520: loss 0.0197, time 68.55ms, mfu 15.95%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 530: loss 0.0203, time 68.66ms, mfu 16.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 540: loss 0.0213, time 68.53ms, mfu 16.20%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 550: loss 0.0211, time 68.56ms, mfu 16.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 560: loss 0.0208, time 68.75ms, mfu 16.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 570: loss 0.0207, time 68.61ms, mfu 16.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 580: loss 0.0196, time 68.66ms, mfu 16.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 590: loss 0.0197, time 68.72ms, mfu 16.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 600: loss 0.0205, time 68.48ms, mfu 16.69%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 610: loss 0.0201, time 68.65ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 620: loss 0.0196, time 68.48ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 630: loss 0.0209, time 67.97ms, mfu 16.86%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 640: loss 0.0195, time 68.06ms, mfu 16.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 650: loss 0.0196, time 68.49ms, mfu 16.95%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 660: loss 0.0197, time 68.13ms, mfu 16.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 670: loss 0.0192, time 68.72ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 680: loss 0.0184, time 68.15ms, mfu 17.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 690: loss 0.0197, time 68.19ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 700: loss 0.0192, time 68.37ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 710: loss 0.0203, time 67.99ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 720: loss 0.0189, time 68.10ms, mfu 17.16%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 730: loss 0.0194, time 67.91ms, mfu 17.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 740: loss 0.0198, time 67.88ms, mfu 17.21%\n",
      "step 750: train loss 0.0169, val loss 7.2955\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 750: loss 0.0201, time 9600.42ms, mfu 15.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 760: loss 0.0207, time 68.38ms, mfu 15.68%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 770: loss 0.0191, time 69.02ms, mfu 15.83%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 780: loss 0.0203, time 68.63ms, mfu 15.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 790: loss 0.0198, time 68.22ms, mfu 16.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 800: loss 0.0198, time 67.83ms, mfu 16.24%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 810: loss 0.0206, time 67.94ms, mfu 16.36%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 820: loss 0.0192, time 67.88ms, mfu 16.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 830: loss 0.0205, time 68.31ms, mfu 16.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 840: loss 0.0201, time 68.43ms, mfu 16.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 850: loss 0.0201, time 69.02ms, mfu 16.68%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 860: loss 0.0192, time 68.64ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 870: loss 0.0189, time 68.39ms, mfu 16.79%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 880: loss 0.0192, time 68.29ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 890: loss 0.0203, time 67.80ms, mfu 16.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 900: loss 0.0191, time 67.82ms, mfu 16.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 910: loss 0.0198, time 67.85ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 920: loss 0.0187, time 68.21ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 930: loss 0.0192, time 69.01ms, mfu 17.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 940: loss 0.0183, time 68.63ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 950: loss 0.0196, time 68.88ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 960: loss 0.0193, time 67.96ms, mfu 17.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 970: loss 0.0196, time 67.85ms, mfu 17.15%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 980: loss 0.0197, time 68.18ms, mfu 17.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 990: loss 0.0198, time 68.28ms, mfu 17.19%\n",
      "step 1000: train loss 0.0169, val loss 7.3134\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1000: loss 0.0181, time 9599.05ms, mfu 15.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1010: loss 0.0188, time 68.88ms, mfu 15.65%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1020: loss 0.0203, time 68.79ms, mfu 15.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1030: loss 0.0191, time 68.69ms, mfu 15.95%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1040: loss 0.0189, time 68.02ms, mfu 16.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1050: loss 0.0187, time 67.79ms, mfu 16.23%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1060: loss 0.0178, time 68.30ms, mfu 16.34%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1070: loss 0.0183, time 67.83ms, mfu 16.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1080: loss 0.0181, time 68.28ms, mfu 16.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1090: loss 0.0195, time 69.24ms, mfu 16.59%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1100: loss 0.0195, time 68.74ms, mfu 16.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1110: loss 0.0186, time 68.21ms, mfu 16.73%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1120: loss 0.0190, time 67.92ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1130: loss 0.0183, time 68.10ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1140: loss 0.0190, time 68.59ms, mfu 16.89%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1150: loss 0.0183, time 69.05ms, mfu 16.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1160: loss 0.0179, time 68.96ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1170: loss 0.0190, time 68.14ms, mfu 16.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1180: loss 0.0180, time 68.05ms, mfu 17.03%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1190: loss 0.0185, time 67.86ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1200: loss 0.0185, time 68.77ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1210: loss 0.0187, time 68.51ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1220: loss 0.0191, time 68.57ms, mfu 17.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1230: loss 0.0183, time 68.04ms, mfu 17.14%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1240: loss 0.0188, time 67.79ms, mfu 17.18%\n",
      "step 1250: train loss 0.0165, val loss 7.7846\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1250: loss 0.0187, time 9630.42ms, mfu 15.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1260: loss 0.0179, time 68.88ms, mfu 15.64%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1270: loss 0.0178, time 69.10ms, mfu 15.79%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1280: loss 0.0186, time 68.66ms, mfu 15.93%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1290: loss 0.0178, time 67.91ms, mfu 16.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1300: loss 0.0178, time 67.80ms, mfu 16.22%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1310: loss 0.0189, time 68.25ms, mfu 16.33%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1320: loss 0.0191, time 67.88ms, mfu 16.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1330: loss 0.0176, time 68.71ms, mfu 16.52%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1340: loss 0.0179, time 69.12ms, mfu 16.58%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1350: loss 0.0175, time 68.01ms, mfu 16.66%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1360: loss 0.0185, time 67.98ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1370: loss 0.0180, time 67.68ms, mfu 16.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1380: loss 0.0179, time 68.83ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1390: loss 0.0182, time 69.05ms, mfu 16.88%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1400: loss 0.0175, time 68.74ms, mfu 16.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1410: loss 0.0176, time 67.66ms, mfu 16.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1420: loss 0.0172, time 68.17ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1430: loss 0.0180, time 68.53ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1440: loss 0.0174, time 68.94ms, mfu 17.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1450: loss 0.0175, time 68.70ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1460: loss 0.0172, time 67.89ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1470: loss 0.0180, time 68.06ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1480: loss 0.0181, time 69.07ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1490: loss 0.0180, time 68.94ms, mfu 17.14%\n",
      "step 1500: train loss 0.0165, val loss 7.6726\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1500: loss 0.0184, time 10269.63ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1510: loss 0.0180, time 68.31ms, mfu 15.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1520: loss 0.0179, time 68.01ms, mfu 15.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1530: loss 0.0184, time 67.83ms, mfu 15.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1540: loss 0.0175, time 69.00ms, mfu 16.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1550: loss 0.0178, time 68.88ms, mfu 16.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1560: loss 0.0180, time 68.12ms, mfu 16.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1570: loss 0.0175, time 68.33ms, mfu 16.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1580: loss 0.0175, time 67.89ms, mfu 16.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1590: loss 0.0173, time 68.94ms, mfu 16.58%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1600: loss 0.0179, time 69.03ms, mfu 16.64%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1610: loss 0.0178, time 68.16ms, mfu 16.71%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1620: loss 0.0173, time 68.06ms, mfu 16.78%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1630: loss 0.0176, time 69.06ms, mfu 16.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1640: loss 0.0179, time 68.99ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1650: loss 0.0175, time 68.72ms, mfu 16.88%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1660: loss 0.0173, time 67.85ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1670: loss 0.0180, time 67.97ms, mfu 16.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1680: loss 0.0180, time 68.32ms, mfu 17.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1690: loss 0.0178, time 68.78ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1700: loss 0.0172, time 68.71ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1710: loss 0.0174, time 68.02ms, mfu 17.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1720: loss 0.0180, time 68.24ms, mfu 17.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1730: loss 0.0180, time 68.39ms, mfu 17.14%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1740: loss 0.0178, time 69.12ms, mfu 17.13%\n",
      "step 1750: train loss 0.0164, val loss 8.0320\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1750: loss 0.0186, time 9646.44ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1760: loss 0.0178, time 68.16ms, mfu 15.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1770: loss 0.0175, time 68.69ms, mfu 15.79%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1780: loss 0.0174, time 69.01ms, mfu 15.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1790: loss 0.0179, time 68.96ms, mfu 16.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1800: loss 0.0179, time 68.08ms, mfu 16.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1810: loss 0.0173, time 68.40ms, mfu 16.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1820: loss 0.0172, time 68.02ms, mfu 16.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1830: loss 0.0175, time 68.54ms, mfu 16.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1840: loss 0.0172, time 69.27ms, mfu 16.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1850: loss 0.0173, time 69.03ms, mfu 16.61%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1860: loss 0.0176, time 68.37ms, mfu 16.68%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1870: loss 0.0177, time 68.31ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1880: loss 0.0173, time 68.34ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1890: loss 0.0173, time 68.99ms, mfu 16.84%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1900: loss 0.0178, time 69.02ms, mfu 16.87%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1910: loss 0.0172, time 68.09ms, mfu 16.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1920: loss 0.0178, time 67.87ms, mfu 16.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1930: loss 0.0172, time 68.77ms, mfu 16.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1940: loss 0.0175, time 68.92ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1950: loss 0.0174, time 68.67ms, mfu 17.03%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1960: loss 0.0173, time 68.42ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1970: loss 0.0174, time 68.33ms, mfu 17.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1980: loss 0.0169, time 68.04ms, mfu 17.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 1990: loss 0.0178, time 68.46ms, mfu 17.13%\n",
      "step 2000: train loss 0.0164, val loss 7.9886\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2000: loss 0.0174, time 9668.84ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2010: loss 0.0177, time 68.24ms, mfu 15.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2020: loss 0.0171, time 68.96ms, mfu 15.78%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2030: loss 0.0171, time 68.60ms, mfu 15.93%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2040: loss 0.0170, time 67.78ms, mfu 16.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2050: loss 0.0173, time 68.15ms, mfu 16.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2060: loss 0.0168, time 68.93ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2070: loss 0.0175, time 69.15ms, mfu 16.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2080: loss 0.0175, time 68.41ms, mfu 16.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2090: loss 0.0175, time 67.82ms, mfu 16.57%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2100: loss 0.0174, time 68.64ms, mfu 16.64%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2110: loss 0.0172, time 69.00ms, mfu 16.69%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2120: loss 0.0176, time 68.91ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2130: loss 0.0166, time 68.20ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2140: loss 0.0178, time 67.96ms, mfu 16.86%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2150: loss 0.0176, time 69.02ms, mfu 16.89%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2160: loss 0.0171, time 69.09ms, mfu 16.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2170: loss 0.0170, time 68.06ms, mfu 16.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2180: loss 0.0165, time 68.01ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2190: loss 0.0179, time 68.24ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2200: loss 0.0173, time 68.92ms, mfu 17.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2210: loss 0.0172, time 68.67ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2220: loss 0.0170, time 67.87ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2230: loss 0.0172, time 68.10ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2240: loss 0.0172, time 69.01ms, mfu 17.14%\n",
      "step 2250: train loss 0.0164, val loss 7.9689\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2250: loss 0.0171, time 9649.91ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2260: loss 0.0172, time 68.00ms, mfu 15.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2270: loss 0.0168, time 68.78ms, mfu 15.79%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2280: loss 0.0171, time 68.61ms, mfu 15.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2290: loss 0.0169, time 67.93ms, mfu 16.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2300: loss 0.0170, time 68.12ms, mfu 16.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2310: loss 0.0174, time 68.92ms, mfu 16.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2320: loss 0.0173, time 69.24ms, mfu 16.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2330: loss 0.0171, time 68.47ms, mfu 16.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2340: loss 0.0171, time 68.21ms, mfu 16.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2350: loss 0.0172, time 68.45ms, mfu 16.64%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2360: loss 0.0169, time 69.01ms, mfu 16.69%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2370: loss 0.0169, time 68.86ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2380: loss 0.0172, time 68.34ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2390: loss 0.0173, time 68.57ms, mfu 16.84%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2400: loss 0.0174, time 68.96ms, mfu 16.87%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2410: loss 0.0175, time 68.84ms, mfu 16.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2420: loss 0.0176, time 68.32ms, mfu 16.95%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2430: loss 0.0167, time 67.92ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2440: loss 0.0167, time 68.40ms, mfu 17.03%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2450: loss 0.0167, time 68.98ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2460: loss 0.0168, time 69.14ms, mfu 17.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2470: loss 0.0168, time 68.03ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2480: loss 0.0172, time 68.32ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2490: loss 0.0166, time 68.04ms, mfu 17.13%\n",
      "step 2500: train loss 0.0163, val loss 8.1791\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2500: loss 0.0168, time 9677.42ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2510: loss 0.0167, time 68.47ms, mfu 15.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2520: loss 0.0168, time 68.29ms, mfu 15.79%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2530: loss 0.0168, time 68.48ms, mfu 15.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2540: loss 0.0164, time 68.63ms, mfu 16.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2550: loss 0.0173, time 69.06ms, mfu 16.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2560: loss 0.0167, time 68.11ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2570: loss 0.0168, time 68.40ms, mfu 16.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2580: loss 0.0170, time 68.10ms, mfu 16.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2590: loss 0.0172, time 69.01ms, mfu 16.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2600: loss 0.0169, time 69.20ms, mfu 16.61%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2610: loss 0.0166, time 68.45ms, mfu 16.68%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2620: loss 0.0169, time 68.13ms, mfu 16.75%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2630: loss 0.0167, time 68.00ms, mfu 16.82%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2640: loss 0.0172, time 69.02ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2650: loss 0.0171, time 68.65ms, mfu 16.89%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2660: loss 0.0165, time 68.52ms, mfu 16.93%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2670: loss 0.0169, time 68.21ms, mfu 16.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2680: loss 0.0165, time 68.61ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2690: loss 0.0169, time 69.03ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2700: loss 0.0175, time 68.96ms, mfu 17.03%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2710: loss 0.0166, time 67.82ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2720: loss 0.0165, time 68.25ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2730: loss 0.0170, time 68.57ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2740: loss 0.0167, time 68.98ms, mfu 17.12%\n",
      "step 2750: train loss 0.0163, val loss 8.0757\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2750: loss 0.0163, time 9646.07ms, mfu 15.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2760: loss 0.0167, time 68.25ms, mfu 15.61%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2770: loss 0.0166, time 69.17ms, mfu 15.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2780: loss 0.0168, time 68.74ms, mfu 15.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2790: loss 0.0169, time 68.15ms, mfu 16.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2800: loss 0.0170, time 68.33ms, mfu 16.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2810: loss 0.0168, time 68.68ms, mfu 16.28%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2820: loss 0.0169, time 68.98ms, mfu 16.37%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2830: loss 0.0167, time 68.97ms, mfu 16.45%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2840: loss 0.0166, time 68.31ms, mfu 16.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2850: loss 0.0168, time 68.20ms, mfu 16.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2860: loss 0.0170, time 68.83ms, mfu 16.68%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2870: loss 0.0172, time 68.78ms, mfu 16.73%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2880: loss 0.0168, time 67.88ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2890: loss 0.0168, time 67.94ms, mfu 16.86%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2900: loss 0.0162, time 68.69ms, mfu 16.90%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2910: loss 0.0165, time 69.03ms, mfu 16.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2920: loss 0.0169, time 68.62ms, mfu 16.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2930: loss 0.0167, time 67.88ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2940: loss 0.0166, time 68.17ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2950: loss 0.0170, time 68.93ms, mfu 17.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2960: loss 0.0167, time 69.16ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2970: loss 0.0169, time 67.79ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2980: loss 0.0168, time 68.12ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 2990: loss 0.0168, time 68.64ms, mfu 17.14%\n",
      "step 3000: train loss 0.0163, val loss 8.5541\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3000: loss 0.0166, time 9668.27ms, mfu 15.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3010: loss 0.0167, time 68.34ms, mfu 15.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3020: loss 0.0166, time 67.84ms, mfu 15.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3030: loss 0.0167, time 67.89ms, mfu 15.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3040: loss 0.0166, time 68.35ms, mfu 16.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3050: loss 0.0166, time 69.05ms, mfu 16.21%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3060: loss 0.0164, time 68.74ms, mfu 16.31%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3070: loss 0.0166, time 68.53ms, mfu 16.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3080: loss 0.0169, time 68.30ms, mfu 16.50%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3090: loss 0.0169, time 67.97ms, mfu 16.59%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3100: loss 0.0164, time 68.79ms, mfu 16.65%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3110: loss 0.0165, time 69.24ms, mfu 16.69%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3120: loss 0.0164, time 68.74ms, mfu 16.75%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3130: loss 0.0166, time 68.18ms, mfu 16.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3140: loss 0.0169, time 67.78ms, mfu 16.87%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3150: loss 0.0165, time 68.72ms, mfu 16.91%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3160: loss 0.0166, time 68.92ms, mfu 16.93%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3170: loss 0.0166, time 68.89ms, mfu 16.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3180: loss 0.0166, time 68.16ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3190: loss 0.0165, time 68.15ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3200: loss 0.0166, time 68.53ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3210: loss 0.0167, time 69.01ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3220: loss 0.0165, time 68.74ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3230: loss 0.0166, time 68.16ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3240: loss 0.0165, time 68.21ms, mfu 17.13%\n",
      "step 3250: train loss 0.0163, val loss 8.7084\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3250: loss 0.0166, time 9653.28ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3260: loss 0.0162, time 68.09ms, mfu 15.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3270: loss 0.0163, time 68.03ms, mfu 15.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3280: loss 0.0165, time 69.13ms, mfu 15.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3290: loss 0.0164, time 69.16ms, mfu 16.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3300: loss 0.0163, time 68.14ms, mfu 16.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3310: loss 0.0164, time 68.18ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3320: loss 0.0164, time 69.04ms, mfu 16.39%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3330: loss 0.0166, time 68.92ms, mfu 16.47%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3340: loss 0.0167, time 68.76ms, mfu 16.54%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3350: loss 0.0168, time 67.99ms, mfu 16.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3360: loss 0.0165, time 68.13ms, mfu 16.70%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3370: loss 0.0167, time 68.83ms, mfu 16.75%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3380: loss 0.0165, time 68.78ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3390: loss 0.0165, time 68.49ms, mfu 16.84%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3400: loss 0.0166, time 68.03ms, mfu 16.90%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3410: loss 0.0163, time 68.58ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3420: loss 0.0168, time 69.15ms, mfu 16.95%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3430: loss 0.0165, time 68.74ms, mfu 16.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3440: loss 0.0163, time 67.81ms, mfu 17.03%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3450: loss 0.0167, time 68.04ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3460: loss 0.0166, time 68.63ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3470: loss 0.0166, time 69.00ms, mfu 17.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3480: loss 0.0166, time 68.73ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3490: loss 0.0163, time 67.89ms, mfu 17.13%\n",
      "step 3500: train loss 0.0163, val loss 8.5481\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3500: loss 0.0165, time 9664.74ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3510: loss 0.0165, time 68.97ms, mfu 15.61%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3520: loss 0.0167, time 69.00ms, mfu 15.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3530: loss 0.0164, time 68.18ms, mfu 15.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3540: loss 0.0164, time 68.06ms, mfu 16.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3550: loss 0.0164, time 69.13ms, mfu 16.17%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3560: loss 0.0162, time 69.08ms, mfu 16.27%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3570: loss 0.0165, time 68.09ms, mfu 16.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3580: loss 0.0163, time 68.21ms, mfu 16.48%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3590: loss 0.0164, time 68.49ms, mfu 16.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3600: loss 0.0165, time 69.19ms, mfu 16.61%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3610: loss 0.0167, time 68.92ms, mfu 16.67%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3620: loss 0.0164, time 68.21ms, mfu 16.74%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3630: loss 0.0167, time 68.02ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3640: loss 0.0169, time 68.47ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3650: loss 0.0164, time 68.98ms, mfu 16.88%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3660: loss 0.0165, time 68.51ms, mfu 16.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3670: loss 0.0166, time 67.92ms, mfu 16.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3680: loss 0.0165, time 68.36ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3690: loss 0.0167, time 69.07ms, mfu 17.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3700: loss 0.0165, time 68.83ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3710: loss 0.0163, time 68.05ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3720: loss 0.0163, time 68.08ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3730: loss 0.0164, time 68.06ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3740: loss 0.0165, time 68.63ms, mfu 17.14%\n",
      "step 3750: train loss 0.0163, val loss 8.5447\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3750: loss 0.0166, time 9641.56ms, mfu 15.44%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3760: loss 0.0164, time 67.94ms, mfu 15.64%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3770: loss 0.0166, time 68.06ms, mfu 15.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3780: loss 0.0164, time 68.66ms, mfu 15.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3790: loss 0.0166, time 68.98ms, mfu 16.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3800: loss 0.0165, time 68.80ms, mfu 16.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3810: loss 0.0164, time 68.39ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3820: loss 0.0164, time 68.56ms, mfu 16.40%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3830: loss 0.0163, time 68.12ms, mfu 16.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3840: loss 0.0163, time 68.62ms, mfu 16.57%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3850: loss 0.0163, time 69.09ms, mfu 16.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3860: loss 0.0166, time 68.49ms, mfu 16.69%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3870: loss 0.0165, time 68.21ms, mfu 16.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3880: loss 0.0165, time 67.84ms, mfu 16.83%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3890: loss 0.0165, time 68.83ms, mfu 16.86%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3900: loss 0.0162, time 68.89ms, mfu 16.89%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3910: loss 0.0166, time 68.41ms, mfu 16.93%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3920: loss 0.0164, time 68.28ms, mfu 16.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3930: loss 0.0166, time 68.19ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3940: loss 0.0163, time 69.00ms, mfu 17.03%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3950: loss 0.0163, time 69.01ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3960: loss 0.0165, time 67.94ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3970: loss 0.0164, time 68.11ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3980: loss 0.0164, time 68.62ms, mfu 17.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 3990: loss 0.0164, time 69.02ms, mfu 17.12%\n",
      "step 4000: train loss 0.0163, val loss 9.4888\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4000: loss 0.0163, time 9624.31ms, mfu 15.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4010: loss 0.0165, time 67.86ms, mfu 15.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4020: loss 0.0165, time 68.05ms, mfu 15.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4030: loss 0.0165, time 68.82ms, mfu 15.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4040: loss 0.0164, time 69.04ms, mfu 16.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4050: loss 0.0166, time 68.40ms, mfu 16.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4060: loss 0.0163, time 68.34ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4070: loss 0.0165, time 68.07ms, mfu 16.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4080: loss 0.0166, time 68.50ms, mfu 16.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4090: loss 0.0164, time 69.06ms, mfu 16.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4100: loss 0.0162, time 68.81ms, mfu 16.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4110: loss 0.0165, time 68.26ms, mfu 16.69%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4120: loss 0.0161, time 68.31ms, mfu 16.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4130: loss 0.0165, time 68.82ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4140: loss 0.0162, time 68.85ms, mfu 16.84%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4150: loss 0.0164, time 68.34ms, mfu 16.89%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4160: loss 0.0165, time 68.10ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4170: loss 0.0163, time 68.19ms, mfu 16.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4180: loss 0.0163, time 69.03ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4190: loss 0.0164, time 68.60ms, mfu 17.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4200: loss 0.0163, time 68.14ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4210: loss 0.0163, time 67.97ms, mfu 17.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4220: loss 0.0163, time 68.73ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4230: loss 0.0162, time 68.78ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4240: loss 0.0165, time 68.76ms, mfu 17.12%\n",
      "step 4250: train loss 0.0163, val loss 9.7575\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4250: loss 0.0165, time 9628.21ms, mfu 15.42%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4260: loss 0.0165, time 67.85ms, mfu 15.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4270: loss 0.0164, time 68.74ms, mfu 15.78%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4280: loss 0.0163, time 69.08ms, mfu 15.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4290: loss 0.0164, time 68.89ms, mfu 16.05%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4300: loss 0.0164, time 68.14ms, mfu 16.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4310: loss 0.0164, time 67.92ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4320: loss 0.0163, time 68.09ms, mfu 16.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4330: loss 0.0165, time 67.99ms, mfu 16.51%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4340: loss 0.0162, time 68.48ms, mfu 16.59%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4350: loss 0.0164, time 68.91ms, mfu 16.65%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4360: loss 0.0165, time 68.77ms, mfu 16.70%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4370: loss 0.0165, time 68.89ms, mfu 16.75%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4380: loss 0.0164, time 68.15ms, mfu 16.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4390: loss 0.0164, time 68.04ms, mfu 16.87%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4400: loss 0.0164, time 68.31ms, mfu 16.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4410: loss 0.0166, time 68.77ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4420: loss 0.0163, time 68.93ms, mfu 16.97%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4430: loss 0.0165, time 68.51ms, mfu 17.00%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4440: loss 0.0165, time 67.82ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4450: loss 0.0162, time 67.93ms, mfu 17.08%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4460: loss 0.0165, time 68.09ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4470: loss 0.0163, time 68.57ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4480: loss 0.0163, time 69.00ms, mfu 17.13%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4490: loss 0.0162, time 69.00ms, mfu 17.13%\n",
      "step 4500: train loss 0.0163, val loss 9.5601\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4500: loss 0.0165, time 9641.14ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4510: loss 0.0160, time 68.19ms, mfu 15.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4520: loss 0.0165, time 68.10ms, mfu 15.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4530: loss 0.0163, time 68.24ms, mfu 15.95%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4540: loss 0.0163, time 69.08ms, mfu 16.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4550: loss 0.0163, time 68.88ms, mfu 16.18%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4560: loss 0.0163, time 68.26ms, mfu 16.30%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4570: loss 0.0162, time 67.82ms, mfu 16.41%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4580: loss 0.0165, time 68.71ms, mfu 16.49%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4590: loss 0.0166, time 68.96ms, mfu 16.56%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4600: loss 0.0163, time 68.90ms, mfu 16.62%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4610: loss 0.0163, time 68.19ms, mfu 16.70%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4620: loss 0.0162, time 68.04ms, mfu 16.77%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4630: loss 0.0164, time 68.76ms, mfu 16.81%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4640: loss 0.0164, time 68.81ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4650: loss 0.0164, time 68.88ms, mfu 16.88%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4660: loss 0.0163, time 67.89ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4670: loss 0.0164, time 67.79ms, mfu 16.99%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4680: loss 0.0162, time 68.67ms, mfu 17.01%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4690: loss 0.0163, time 69.11ms, mfu 17.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4700: loss 0.0165, time 68.79ms, mfu 17.04%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4710: loss 0.0166, time 68.24ms, mfu 17.07%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4720: loss 0.0163, time 68.12ms, mfu 17.10%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4730: loss 0.0164, time 68.33ms, mfu 17.12%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4740: loss 0.0163, time 68.81ms, mfu 17.13%\n",
      "step 4750: train loss 0.0163, val loss 10.1181\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4750: loss 0.0163, time 9660.90ms, mfu 15.43%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4760: loss 0.0163, time 68.81ms, mfu 15.61%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4770: loss 0.0161, time 68.93ms, mfu 15.76%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4780: loss 0.0164, time 68.44ms, mfu 15.92%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4790: loss 0.0163, time 68.39ms, mfu 16.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4800: loss 0.0164, time 67.94ms, mfu 16.19%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4810: loss 0.0164, time 68.75ms, mfu 16.29%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4820: loss 0.0165, time 69.13ms, mfu 16.38%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4830: loss 0.0163, time 68.71ms, mfu 16.46%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4840: loss 0.0163, time 68.31ms, mfu 16.55%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4850: loss 0.0165, time 68.14ms, mfu 16.63%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4860: loss 0.0165, time 68.28ms, mfu 16.70%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4870: loss 0.0164, time 69.01ms, mfu 16.75%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4880: loss 0.0163, time 68.44ms, mfu 16.80%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4890: loss 0.0163, time 68.50ms, mfu 16.85%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4900: loss 0.0163, time 67.99ms, mfu 16.90%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4910: loss 0.0164, time 68.64ms, mfu 16.94%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4920: loss 0.0161, time 68.95ms, mfu 16.96%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4930: loss 0.0163, time 69.09ms, mfu 16.98%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4940: loss 0.0164, time 68.07ms, mfu 17.02%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4950: loss 0.0164, time 67.75ms, mfu 17.06%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4960: loss 0.0163, time 68.44ms, mfu 17.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4970: loss 0.0165, time 68.96ms, mfu 17.09%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4980: loss 0.0163, time 68.72ms, mfu 17.11%\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 4990: loss 0.0162, time 68.59ms, mfu 17.12%\n",
      "step 5000: train loss 0.0163, val loss 9.8164\n",
      "WARNING: estimate_mfu implementation not checked for AbstractTransformerLM\n",
      "iter 5000: loss 0.0164, time 9655.81ms, mfu 15.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e8916a07c44369bafc5e5b0ae76ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.018 MB uploaded\\r'), FloatProgress(value=0.327321272885789, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>lr</td><td>▁████▇▇▆▆▅▅▄▄▃▃▃▂▂▂▂▂</td></tr><tr><td>mfu</td><td>▁████████████████████</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▁▂▃▃▄▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>5000</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>mfu</td><td>17.12092</td></tr><tr><td>train/loss</td><td>0.01627</td></tr><tr><td>val/loss</td><td>9.8164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AbstractTransformerLM</strong> at: <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/zedqv5g5' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/zedqv5g5</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240118_214204-zedqv5g5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "train_model(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Once upon a time, [SEP] a big forest, there was a tiny mushroom. It was all alone. The sun was very harsh, and the mushroom did not like it. It wanted to find a friend to play with and to help it hide from the sun. One day, a little bunny came hopping by. The mushroom called out, \" Hello, bunny! Will you be my friend? \" The bunny looked at the mushroom and smiled. \" Sure, I will be your friend. Let's play together! \" The bunny and the mushroom played all day, and they were very happy. As they played, the bunny realized that the mushroom needed help to hide from the harsh sun. So, the bunny dug a hole in the ground and put the mushroom inside. Now, the mushroom was safe and cool. The mushroom and the bunny were the best of friends, and they played in the forest every day. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time,'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0).to(device)\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=250, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Transformer Model (Relational Symbolic Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=384, n_layers=6, n_heads_enc=4, n_heads_abs=2, dff=None,\n",
    "    symbol_retrieval='rel_sym_attn', symbol_retrieval_kwargs=dict(\n",
    "        model_dim=384, rel_n_heads=4, symbolic_attn_n_heads=4,\n",
    "        num_symbols=20, nbhd_delta=5, causal_nbhd=True, include_self=False,\n",
    "        normalize_rels=True), # FIXME make names consistent: d_model, model_dim\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True)\n",
    "model = abstracttransformer_lm = AbstractTransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "AbstractTransformerLM                                   [1, 1, 65]                --\n",
       "├─ModuleDict: 1-1                                       --                        --\n",
       "│    └─Embedding: 2-1                                   [1, 256, 384]             24,960\n",
       "│    └─Embedding: 2-2                                   [256, 384]                98,304\n",
       "│    └─ModuleList: 2-3                                  --                        --\n",
       "│    │    └─AbstractEncoderBlock: 3-1                   [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-3                   --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-4                   [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-6                   --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-7                   [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-9                   --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-10                  [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-12                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-13                  [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-15                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-16                  [1, 256, 384]             2,684,928\n",
       "│    └─Linear: 2-4                                      [1, 1, 65]                25,025\n",
       "=========================================================================================================\n",
       "Total params: 27,810,881\n",
       "Trainable params: 27,810,881\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 35.02\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 53.48\n",
       "Params size (MB): 30.79\n",
       "Estimated Total Size (MB): 84.27\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,256)), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of params 13,824,833\n"
     ]
    }
   ],
   "source": [
    "# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\n",
    "# this is the correct number (similar to TransformerLM)\n",
    "\n",
    "num_params = model.get_num_params() #sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'# of params {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can we implement in a way that torchinfo can understand? i.e., without \"recursive\" and overcounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 45, with 13,884,672 parameters\n",
      "num non-decayed parameter tensors: 65, with 38,465 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# grad scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = dict(\n",
    "    model=model, get_batch=get_batch, batch_size=batch_size, max_iters=max_iters,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr, eval_model=eval_model,\n",
    "    compile=True, grad_clip=0, gradient_accumulation_steps=1,\n",
    "    eval_main_metric='val/loss', eval_interval=eval_interval, always_save_checkpoint=always_save_checkpoint, out_dir=out_dir,\n",
    "    log_interval=10, wandb_log=True, wandb_init_kwargs=dict(project=wandb_project, name='AbstractTransformerLM (Relational Symbolic Attn)'), \n",
    "    ckpt_dict=dict(model_args=model_args), track_mfu=True,\n",
    "    master_process=True, ddp=False, device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/gibbs/project/lafferty/ma2393/abstract_transformer/experiments/wandb/run-20240118_184804-2v1kqg5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char/runs/2v1kqg5t' target=\"_blank\">AbstractTransformerLM</a></strong> to <a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--shakespeare_char</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char/runs/2v1kqg5t' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--shakespeare_char/runs/2v1kqg5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "step 0: train loss 4.5255, val loss 4.5268\n",
      "iter 0: loss 4.5382, time 62455.47ms, mfu -100.00%\n"
     ]
    }
   ],
   "source": [
    "train_model(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo,--\n",
      "\n",
      "KING HENRY VI:\n",
      "He was for him to be found that Richard now.\n",
      "\n",
      "DERBY:\n",
      "My lord, I proud away not: ount take no letters\n",
      "Twixt thy gracious lord begin. Who's the matter,\n",
      "With some gone aloof?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Say brought your not, sir?\n",
      "\n",
      "TRANCIO:\n",
      "What scorn you do very good note:\n",
      "My lord, I am content my duteous bildhs away.\n",
      "\n",
      "BAGOT:\n",
      "I have subjects beats but next thy father,\n",
      "Apparender thee: Take honour in our senate.\n",
      "\n",
      "PAULINA:\n",
      "This way be thy nose physician, if he be\n",
      "To be a bad-: Tradam? his any conduction,\n",
      "His rason was not a parturer,\n",
      "Or hang denied at the king! Sir Jethop's off,\n",
      "Should be halp there, says Tybalt, Queen Nedalus,\n",
      "Shore the king.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Oh shall forge you ashame to Richmond!\n",
      "\n",
      "KING HENRY VI:\n",
      "For welcome, welcomemes thee from the key-roof\n",
      "Lord MarsS ay, and fly will not this hoofest\n",
      "Rest, and let he go forth his wither's groans?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "True executions of men:\n",
      "Ah, with all horses of conceit so evide.\n",
      "\n",
      "KING HENRY VI:\n",
      "Defusions, worship thee, and what subje\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time,'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0).to(device)\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=100, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
