{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from  tqdm import tqdm\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "import sys; sys.path.append('../..')\n",
    "from language_models import TransformerLM, AbstractTransformerLM, configure_optimizers\n",
    "from train_utils import train_model\n",
    "from utils.pl_tqdm_progbar import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available:  True\n",
      "device count:  1\n",
      "current device name:  NVIDIA A100 80GB PCIe\n",
      "Memory Usage:\n",
      "\tAllocated: 0.0 GB\n",
      "\tReserved:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print('cuda available: ', torch.cuda.is_available())\n",
    "print('device count: ', torch.cuda.device_count())\n",
    "print('current device name: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print('Memory Usage:')\n",
    "print('\\tAllocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('\\tReserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 500 # keep frequent because we'll overfit\n",
    "max_steps = 5000\n",
    "n_epochs = 1\n",
    "log_every_n_steps = 10\n",
    "log_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "\n",
    "# system\n",
    "device = 'cuda'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "compile = True\n",
    "\n",
    "# # evaluation and output\n",
    "# out_dir = '../out/tiny_stories'\n",
    "# if not os.path.exists(out_dir):\n",
    "#     os.makedirs(out_dir)\n",
    "# eval_interval = 250 # keep frequent because we'll overfit\n",
    "# eval_iters = 200\n",
    "# log_interval = 10 # don't print too too often\n",
    "\n",
    "# # we expect to overfit on this small dataset, so only save when val improves\n",
    "# always_save_checkpoint = False\n",
    "\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = 'abstract_transformer--tiny_stories'\n",
    "\n",
    "# optimization hyperparams\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "max_iters = 5000\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "weight_decay = 1e-1\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "# warmup_iters = 100\n",
    "gradient_accumulation_steps = 1 # accumulate gradients over this many steps. simulates larger batch size\n",
    "\n",
    "# batch size and block size\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "# DDP (distributed data parallel) training\n",
    "# ddp = False\n",
    "# master_process = True\n",
    "\n",
    "# TODO: set up DDP for future experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use tiktoken instead?; much faster\n",
    "dataset = dataset.map(lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=block_size+1), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLES\n",
      "[CLS] One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt. Lily went to her mom and said, \" Mom, I found this needle. Can you share it with me and sew my shirt? \" Her mom smiled and said, \" Yes, Lily, we can share the needle and fix your shirt. \" Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong. One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn. Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \" Hi, I am Fin. Do you want to play? \" asked the little fish. The crab looked at Fin and said, \" No, I don't want to play. I am cold and I don't feel fine. \" Fin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \" Please, sun, help my new friend feel fine and not freeze! \" The sun heard Fin's call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \" Thank you, little fish, for making me feel fine. I don't feel like I will freeze now. Let's play together! \" And so, Fin and the crab played and became good friends. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] Once upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees. One day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \" You are special because you have sweet cherries that everyone loves. \" The cherry tree started to feel a little better. As time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[CLS] Once upon a time, there was a little girl named Lily. Lily liked to pretend she was a popular princess. She lived in a big castle with her best friends, a cat and a dog. One day, while playing in the castle, Lily found a big cobweb. The cobweb was in the way of her fun game. She wanted to get rid of it, but she was scared of the spider that lived there. Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. The spider was sad, but it found a new home outside. Lily, the cat, and the dog were happy they could play without the cobweb in the way. And they all lived happily ever after. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"EXAMPLES\")\n",
    "\n",
    "for x in dataset['train']['input_ids'][:5]:\n",
    "    print(tokenizer.decode(x))\n",
    "    print('-'*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset['validation'], batch_size=batch_size, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to handle padding token? ignore in loss/perplexity/etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_on_step = True\n",
    "\n",
    "class LitLanguageModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text = batch['input_ids']\n",
    "        x, y = text[:, :-1], text[:, 1:]\n",
    "\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        self.log('train/loss', loss, prog_bar=True, logger=True, on_step=log_on_step, on_epoch=True)\n",
    "        self.log('train/perplexity', perplexity, prog_bar=True, logger=True, on_step=log_on_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text = batch['input_ids']\n",
    "        x, y = text[:, :-1], text[:, 1:]\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        self.log(f\"val/loss\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f'val/perplexity', perplexity, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        text = batch['input_ids']\n",
    "        x, y = text[:, :-1], text[:, 1:]\n",
    "        # with ctx:\n",
    "        logits, loss = self.model(x, y)\n",
    "\n",
    "        perplexity = torchmetrics.functional.text.perplexity(logits, y, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        self.log(f\"test/loss\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f'test/perplexity', perplexity, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "TransformerLM                                 --\n",
       "├─ModuleDict: 1-1                             --\n",
       "│    └─Embedding: 2-1                         22,268,928\n",
       "│    │    └─Linear: 3-1                       22,297,924\n",
       "│    └─Dropout: 2-2                           --\n",
       "│    └─ModuleList: 2-3                        --\n",
       "│    │    └─EncoderBlock: 3-2                 7,085,568\n",
       "│    │    └─EncoderBlock: 3-3                 7,085,568\n",
       "│    └─Linear: 2-4                            (recursive)\n",
       "======================================================================\n",
       "Total params: 58,737,988\n",
       "Trainable params: 58,737,988\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=768, n_layers=2, n_heads=12, dff=None, pos_enc_type='RoPE',\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=block_size, bias=True)\n",
    "model = transformer_lm = TransformerLM(**model_args).to(device)\n",
    "# NOTE: embedding layer/output layer account for much of the params... (but they use weight sharing)\n",
    "torchinfo.summary(model, device='cuda') # input_data=[torch.randint(0, 10, size=(1,block_size))]*2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitLanguageModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33121"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawni00\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/radev/scratch/lafferty/ma2393/abstract_transformer/experiments/tiny stories/wandb/run-20240412_210128-apryycs5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/apryycs5' target=\"_blank\">TransformerLM (TEST)</a></strong> to <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/apryycs5' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/apryycs5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma2393/.conda/envs/abstract_transformer/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | TransformerLM | 58.9 M\n",
      "----------------------------------------\n",
      "58.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.9 M    Total params\n",
      "235.738   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 15, with 58,890,240 parameters\n",
      "num non-decayed parameter tensors: 15, with 44,356 parameters\n",
      "using fused AdamW: True\n",
      "Epoch 0:  15%|█▌        | 5000/33121 [29:08<2:43:52,  2.86it/s, v_num=ycs5, train/loss_step=1.150, train/perplexity_step=5.210, val/loss=1.480, val/perplexity=7.620, train/loss_epoch=1.810, train/perplexity_epoch=35.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▌        | 5000/33121 [29:08<2:43:52,  2.86it/s, v_num=ycs5, train/loss_step=1.150, train/perplexity_step=5.210, val/loss=1.480, val/perplexity=7.620, train/loss_epoch=1.810, train/perplexity_epoch=35.70]\n"
     ]
    }
   ],
   "source": [
    "group_name = None\n",
    "run_name = 'TransformerLM (TEST)'\n",
    "run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "    config={'group': group_name, **model_args})\n",
    "\n",
    "wandb_logger = WandbLogger(experiment=run, log_model=False) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "# wandb_logger = None\n",
    "callbacks = [\n",
    "    TQDMProgressBar(refresh_rate=50)\n",
    "    # LitProgressBar()\n",
    "    # L.pytorch.callbacks.TQDMProgressBar(refresh_rate=50)\n",
    "    # L.pytorch.callbacks.RichProgressBar()\n",
    "]\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    max_epochs=n_epochs, enable_checkpointing=False, enable_model_summary=True,\n",
    "    enable_progress_bar=True, callbacks=callbacks, logger=wandb_logger,\n",
    "    accumulate_grad_batches=gradient_accumulation_steps, benchmark=True, gradient_clip_val=grad_clip,\n",
    "    log_every_n_steps=log_every_n_steps, max_steps=max_steps, val_check_interval=eval_interval)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    **trainer_kwargs\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Once upon a time,',\n",
    "    'There once was a girl named ',\n",
    "    'On a rainy day,'\n",
    "    'Emma is a curious person.'\n",
    "]\n",
    "\n",
    "def generate_from_prompt(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None, tokenizer=tokenizer):\n",
    "    prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "    prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "    sample_gen = model.generate(prompt_idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)[0]\n",
    "    sample_gen = tokenizer.decode(sample_gen)\n",
    "    return sample_gen\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(\"GENERATED TEXT:\")\n",
    "    sample_gen = generate_from_prompt(model, prompt)\n",
    "    print(sample_gen)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Transformer Model (Symbolic Attention; RoPE; Disentangled RCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=384, n_layers=6, n_heads_sa=4, n_heads_rca=2, dff=None, rca_disentangled=True,\n",
    "    symbol_retrieval='sym_attn', symbol_retrieval_kwargs=dict(num_symbols=50, n_heads=4, model_dim=384), # FIXME make names consistent: d_model, model_dim\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True, pos_enc_type='RoPE')\n",
    "model = abstracttransformer_lm = AbstractTransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
       "========================================================================================================================\n",
       "AbstractTransformerLM                                                  [1, 1, 28996]             --\n",
       "├─ModuleDict: 1-1                                                      --                        --\n",
       "│    └─Embedding: 2-1                                                  [1, 256, 384]             22,297,924\n",
       "│    └─ModuleList: 2-2                                                 --                        --\n",
       "│    │    └─AbstractEncoderBlock: 3-1                                  [1, 256, 384]             1,991,936\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-3                                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-4                                  [1, 256, 384]             1,991,936\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-6                                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-7                                  [1, 256, 384]             1,991,936\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-9                                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-10                                 [1, 256, 384]             1,991,936\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-12                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-13                                 [1, 256, 384]             1,991,936\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-15                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-16                                 [1, 256, 384]             1,991,936\n",
       "│    └─Linear: 2-3                                                     [1, 1, 28996]             11,163,460\n",
       "========================================================================================================================\n",
       "Total params: 54,623,880\n",
       "Trainable params: 54,623,880\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 22.88\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 60.00\n",
       "Params size (MB): 88.58\n",
       "Estimated Total Size (MB): 148.58\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,block_size)), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleDict' object has no attribute 'positional_embedder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# this is the correct number (similar to TransformerLM)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m num_params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_num_params() \u001b[39m#sum(p.numel() for p in model.parameters() if p.requires_grad)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m# of params \u001b[39m\u001b[39m{\u001b[39;00mnum_params\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/radev/scratch/lafferty/ma2393/abstract_transformer/experiments/tiny stories/../../language_models.py:304\u001b[0m, in \u001b[0;36mAbstractTransformerLM.get_num_params\u001b[0;34m(self, non_embedding)\u001b[0m\n\u001b[1;32m    302\u001b[0m n_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m non_embedding:\n\u001b[0;32m--> 304\u001b[0m     n_params \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mpositional_embedder\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    305\u001b[0m \u001b[39mreturn\u001b[39;00m n_params\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModuleDict' object has no attribute 'positional_embedder'"
     ]
    }
   ],
   "source": [
    "# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\n",
    "# this is the correct number (similar to TransformerLM)\n",
    "\n",
    "num_params = model.get_num_params() #sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'# of params {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can we implement in a way that torchinfo can understand? i.e., without \"recursive\" and overcounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitLanguageModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33121"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawni00\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/radev/scratch/lafferty/ma2393/abstract_transformer/experiments/tiny stories/wandb/run-20240412_213851-20x70258</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/20x70258' target=\"_blank\">AbstractTransformerLM (TEST)</a></strong> to <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/20x70258' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/20x70258</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma2393/.conda/envs/abstract_transformer/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | AbstractTransformerLM | 33.3 M\n",
      "------------------------------------------------\n",
      "33.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "33.3 M    Total params\n",
      "133.273   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 77, with 33,268,224 parameters\n",
      "num non-decayed parameter tensors: 38, with 50,116 parameters\n",
      "using fused AdamW: True\n",
      "Epoch 0:   3%|▎         | 1150/33121 [06:10<2:51:47,  3.10it/s, v_num=0258, train/loss_step=1.420, train/perplexity_step=8.040, val/loss=1.770, val/perplexity=11.20]"
     ]
    }
   ],
   "source": [
    "group_name = None\n",
    "run_name = 'AbstractTransformerLM (TEST)'\n",
    "run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "    config={'group': group_name, **model_args})\n",
    "\n",
    "wandb_logger = WandbLogger(experiment=run, log_model=False) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "# wandb_logger = None\n",
    "callbacks = [\n",
    "    TQDMProgressBar(refresh_rate=50)\n",
    "    # LitProgressBar()\n",
    "    # L.pytorch.callbacks.TQDMProgressBar(refresh_rate=50)\n",
    "    # L.pytorch.callbacks.RichProgressBar()\n",
    "]\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    max_epochs=n_epochs, enable_checkpointing=False, enable_model_summary=True,\n",
    "    enable_progress_bar=True, callbacks=callbacks, logger=wandb_logger,\n",
    "    accumulate_grad_batches=gradient_accumulation_steps, benchmark=True, gradient_clip_val=grad_clip,\n",
    "    log_every_n_steps=log_every_n_steps, max_steps=max_steps, val_check_interval=eval_interval)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    **trainer_kwargs\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Once upon a time,',\n",
    "    'There once was a girl named ',\n",
    "    'On a rainy day,'\n",
    "    'Emma is a curious person.'\n",
    "]\n",
    "\n",
    "def generate_from_prompt(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None, tokenizer=tokenizer):\n",
    "    prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "    prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "    sample_gen = model.generate(prompt_idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)[0]\n",
    "    sample_gen = tokenizer.decode(sample_gen)\n",
    "    return sample_gen\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(\"GENERATED TEXT:\")\n",
    "    sample_gen = generate_from_prompt(model, prompt)\n",
    "    print(sample_gen)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Transformer Model (Position-Relative Symbols; RoPE; Disentangled RCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=384, n_layers=6, n_heads_sa=4, n_heads_rca=2, dff=None, rca_disentangled=True,\n",
    "    symbol_retrieval='pos_relative', symbol_retrieval_kwargs=dict(symbol_dim=384, max_rel_pos=block_size), rca_kwargs=dict(use_relative_positional_symbols=True),\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True, pos_enc_type='RoPE')\n",
    "model = abstracttransformer_lm = AbstractTransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
       "========================================================================================================================\n",
       "AbstractTransformerLM                                                  [1, 1, 28996]             --\n",
       "├─ModuleDict: 1-1                                                      --                        --\n",
       "│    └─Embedding: 2-1                                                  [1, 256, 384]             22,297,924\n",
       "│    └─ModuleList: 2-2                                                 --                        --\n",
       "│    │    └─AbstractEncoderBlock: 3-1                                  [1, 256, 384]             2,002,688\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-3                                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-4                                  [1, 256, 384]             2,002,688\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-6                                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-7                                  [1, 256, 384]             2,002,688\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-9                                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-10                                 [1, 256, 384]             2,002,688\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-12                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-13                                 [1, 256, 384]             2,002,688\n",
       "│    │    └─AbstractEncoderBlock: 3-14                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-15                                 --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-16                                 [1, 256, 384]             2,002,688\n",
       "│    └─Linear: 2-3                                                     [1, 1, 28996]             11,163,460\n",
       "========================================================================================================================\n",
       "Total params: 54,699,144\n",
       "Trainable params: 54,699,144\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 97.20\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1664.32\n",
       "Params size (MB): 88.78\n",
       "Estimated Total Size (MB): 1753.10\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,block_size)), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of params 33,329,092\n"
     ]
    }
   ],
   "source": [
    "# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\n",
    "# this is the correct number (similar to TransformerLM)\n",
    "\n",
    "num_params = model.get_num_params() #sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'# of params {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can we implement in a way that torchinfo can understand? i.e., without \"recursive\" and overcounting\n",
    "# maybe symbol retrieval should be done outside AbstractBlock!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitLanguageModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33121"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawni00\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/radev/scratch/lafferty/ma2393/abstract_transformer/experiments/tiny stories/wandb/run-20240413_142146-ustwd4cn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/ustwd4cn' target=\"_blank\">AbstractTransformerLM (TEST)</a></strong> to <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/ustwd4cn' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/ustwd4cn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma2393/.conda/envs/abstract_transformer/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | AbstractTransformerLM | 33.3 M\n",
      "------------------------------------------------\n",
      "33.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "33.3 M    Total params\n",
      "133.316   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 75, with 33,279,360 parameters\n",
      "num non-decayed parameter tensors: 37, with 49,732 parameters\n",
      "using fused AdamW: True\n",
      "Epoch 0:  15%|█▌        | 5000/33121 [22:07<2:04:24,  3.77it/s, v_num=d4cn, train/loss_step=0.458, train/perplexity_step=1.930, val/loss=0.599, val/perplexity=2.230, train/loss_epoch=1.130, train/perplexity_epoch=86.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▌        | 5000/33121 [22:07<2:04:24,  3.77it/s, v_num=d4cn, train/loss_step=0.458, train/perplexity_step=1.930, val/loss=0.599, val/perplexity=2.230, train/loss_epoch=1.130, train/perplexity_epoch=86.00]\n"
     ]
    }
   ],
   "source": [
    "group_name = None\n",
    "run_name = 'AbstractTransformerLM (TEST)'\n",
    "run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "    config={'group': group_name, **model_args})\n",
    "\n",
    "wandb_logger = WandbLogger(experiment=run, log_model=log_model) # name=run_name, project=wandb_project,\n",
    "# wandb_logger.watch(model, log_graph=False)\n",
    "# wandb_logger = None\n",
    "callbacks = [\n",
    "    TQDMProgressBar(refresh_rate=50)\n",
    "    # LitProgressBar()\n",
    "    # L.pytorch.callbacks.TQDMProgressBar(refresh_rate=50)\n",
    "    # L.pytorch.callbacks.RichProgressBar()\n",
    "]\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    max_epochs=n_epochs, enable_checkpointing=False, enable_model_summary=True,\n",
    "    enable_progress_bar=True, callbacks=callbacks, logger=wandb_logger,\n",
    "    accumulate_grad_batches=gradient_accumulation_steps, benchmark=True, gradient_clip_val=grad_clip,\n",
    "    log_every_n_steps=log_every_n_steps, max_steps=max_steps, val_check_interval=eval_interval)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    **trainer_kwargs\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "GENERATING SAMPLES\n",
      "PROMPT: Once upon a time,\n",
      "GENERATED TEXT:\n",
      "[CLS] Once upon a time, there was a little boy who had a powerful and strong robot. The powerful shark was a good, powerful ship, the powerful ships. They imagined they were playing and laughing. After a while, the powerful ship was tired, but he was so tired. He gently guessed to the powerful shark, and the powerful shark was able to get away from the powerful ship. He was just as fast as he could, but then he heard the wave coming from the powerful water. He hopped all back to his\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PROMPT: There once was a girl named \n",
      "GENERATED TEXT:\n",
      "[CLS] There once was a girl named Lily in the park. Her mummy warn her she always used the Making would be arranged away and she would stay safe and sound. Lily learned that sometimes we have a promise engine sometimes a good time and it makes us it! They left the park to be important and tidy. Lily learned her lesson that day : it's important to listen and listen to those in kindness. They also learn to enjoy their time so she goes the highest game's always a way to be organized or organized\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PROMPT: On a rainy day,\n",
      "GENERATED TEXT:\n",
      "[CLS] On a rainy day, he looked out at his house, who was getting a bit short before she saw the garden. The garden looked very large and it was just a little girl. She thanked the fairy from then said'Thanks to my new friend. I'm so lucky to hear Saunders at a few flowers and special things like snow. [UNK] every day, first you're the most great thing to take walks in the same spot and always invite me lawn. And sure enough, the girl felt very lucky that she\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PROMPT: Emma is a curious person.\n",
      "GENERATED TEXT:\n",
      "[CLS] Emma is a curious person. He walks on the person. He is always very curious and smells as wants to touch things that goes off to him. He walks away the park, where she talks to admire the broken things. But when he gets there, he hears he hears a voice. It was the birdsaring in front of the park. It is their mom. She hears the same lady come into the distance of the park. She sees the sign and calls Daddy, who is on his way and some dogs. She sees\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PROMPT: \n",
      "GENERATED TEXT:\n",
      "[CLS] Marge taught her mum. They enjoyed the whole day and gathering food - they caught and ate it. Remember, they stored together to remind each other and t sting - try to make the food right! Mara's mum said it was the best food to her daughter's family. Her mum smiled and said, \" That's see, we can do it! I will never forget you too late. You could do whatever they want, you will be okay? \" Mara thought\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PROMPT: \n",
      "GENERATED TEXT:\n",
      "[CLS] Once there was summer. victorious adventurous day, it and the result filled with sticky cream. It also temples to have much happier, don't take too much that lived on. You are proud, loop wear proud of my curiosity. I'm glad you well Finn like that. \"rilla smiled ; he was smiling at himself. But this was 3 treat belonged that waiting for him. He is safe and sound at bad behaviour and countersishing that sometimes, bad things can happen. That day\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "PROMPT: \n",
      "GENERATED TEXT:\n",
      "[CLS] Once there was nowhere : saw Lucy was sad. She wanted to wash her dress. knee wanted her to clean Lucy so she had a warm blanket. Lucy hugged mum and said goodbye. They worked together to clean. Lucy was so happy that a few days later. She created a difficult tasks with it. Lucy felt sorry for her mum. The end. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Once upon a time,',\n",
    "    'There once was a girl named ',\n",
    "    'On a rainy day,',\n",
    "    'Emma is a curious person.',\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "]\n",
    "\n",
    "def generate_from_prompt(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None, tokenizer=tokenizer):\n",
    "    prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0)#.to(device)\n",
    "    prompt_idx = prompt_idx[:, :-1] # remove final token because it is [SEP]\n",
    "    sample_gen = model.generate(prompt_idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)[0]\n",
    "    sample_gen = tokenizer.decode(sample_gen)\n",
    "    return sample_gen\n",
    "\n",
    "print()\n",
    "print('='*100)\n",
    "print(\"GENERATING SAMPLES\")\n",
    "samples = []\n",
    "for prompt in prompts:\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(\"GENERATED TEXT:\")\n",
    "    sample_gen = generate_from_prompt(model, prompt)\n",
    "    print(sample_gen)\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    samples.append(sample_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_table = [[p, g] for p, g in zip(prompts, samples)]\n",
    "samples_table = wandb.Table(columns=[\"Prompt\", \"Generated Sample\"], data = samples_table)\n",
    "run.log({\"Generated Samples\": samples_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663f98f269634afdb1f629ee33a74680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.103 MB of 0.152 MB uploaded\\r'), FloatProgress(value=0.6817309567701223, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss_epoch</td><td>▁</td></tr><tr><td>train/loss_step</td><td>▇▆██▅▆▆▄▃▃▂▃▂▂▂▃▂▃▂▂▂▂▃▃▁▃▃▃▃▂▂▃▂▁▂▁▁▂▁▂</td></tr><tr><td>train/perplexity_epoch</td><td>▁</td></tr><tr><td>train/perplexity_step</td><td>█▄▃▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val/loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val/perplexity</td><td>█▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train/loss_epoch</td><td>1.13302</td></tr><tr><td>train/loss_step</td><td>0.4578</td></tr><tr><td>train/perplexity_epoch</td><td>85.96976</td></tr><tr><td>train/perplexity_step</td><td>1.93338</td></tr><tr><td>trainer/global_step</td><td>4999</td></tr><tr><td>val/loss</td><td>0.59906</td></tr><tr><td>val/perplexity</td><td>2.23264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AbstractTransformerLM (TEST)</strong> at: <a href='https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/ustwd4cn' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--tiny_stories/runs/ustwd4cn</a><br/>Synced 6 W&B file(s), 2 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240413_142146-ustwd4cn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 66, with 32,776,704 parameters\n",
      "num non-decayed parameter tensors: 38, with 50,116 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# grad scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     model\u001b[39m=\u001b[39mmodel, get_batch\u001b[39m=\u001b[39mget_batch, batch_size\u001b[39m=\u001b[39mbatch_size, max_iters\u001b[39m=\u001b[39mmax_iters,\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer, scaler\u001b[39m=\u001b[39mscaler, get_lr\u001b[39m=\u001b[39mget_lr, eval_model\u001b[39m=\u001b[39meval_model,\n\u001b[1;32m      4\u001b[0m     \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, grad_clip\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, gradient_accumulation_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     eval_main_metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval/loss\u001b[39m\u001b[39m'\u001b[39m, eval_interval\u001b[39m=\u001b[39meval_interval, always_save_checkpoint\u001b[39m=\u001b[39malways_save_checkpoint, out_dir\u001b[39m=\u001b[39mout_dir,\n\u001b[1;32m      6\u001b[0m     log_interval\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, wandb_log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, wandb_init_kwargs\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(project\u001b[39m=\u001b[39mwandb_project, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAbstractTransformerLM\u001b[39m\u001b[39m'\u001b[39m), \n\u001b[1;32m      7\u001b[0m     ckpt_dict\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(model_args\u001b[39m=\u001b[39mmodel_args), track_mfu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     master_process\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ddp\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, device_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_batch' is not defined"
     ]
    }
   ],
   "source": [
    "train_kwargs = dict(\n",
    "    model=model, get_batch=get_batch, batch_size=batch_size, max_iters=max_iters,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr, eval_model=eval_model,\n",
    "    compile=True, grad_clip=0, gradient_accumulation_steps=1,\n",
    "    eval_main_metric='val/loss', eval_interval=eval_interval, always_save_checkpoint=always_save_checkpoint, out_dir=out_dir,\n",
    "    log_interval=10, wandb_log=True, wandb_init_kwargs=dict(project=wandb_project, name='AbstractTransformerLM'), \n",
    "    ckpt_dict=dict(model_args=model_args), track_mfu=True,\n",
    "    master_process=True, ddp=False, device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_kwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Once upon a time, [SEP] a big forest, there was a tiny mushroom. It was all alone. The sun was very harsh, and the mushroom did not like it. It wanted to find a friend to play with and to help it hide from the sun. One day, a little bunny came hopping by. The mushroom called out, \" Hello, bunny! Will you be my friend? \" The bunny looked at the mushroom and smiled. \" Sure, I will be your friend. Let's play together! \" The bunny and the mushroom played all day, and they were very happy. As they played, the bunny realized that the mushroom needed help to hide from the harsh sun. So, the bunny dug a hole in the ground and put the mushroom inside. Now, the mushroom was safe and cool. The mushroom and the bunny were the best of friends, and they played in the forest every day. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time,'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0).to(device)\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=250, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Transformer Model (Relational Symbolic Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AbstractTransformerLM.__init__() got an unexpected keyword argument 'n_heads_enc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m      2\u001b[0m     vocab_size\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab_size, d_model\u001b[39m=\u001b[39m\u001b[39m384\u001b[39m, n_layers\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, n_heads_enc\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, n_heads_abs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, dff\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     symbol_retrieval\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrel_sym_attn\u001b[39m\u001b[39m'\u001b[39m, symbol_retrieval_kwargs\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m         normalize_rels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), \u001b[39m# FIXME make names consistent: d_model, model_dim\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     dropout_rate\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, norm_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_block_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[39m=\u001b[39m abstracttransformer_lm \u001b[39m=\u001b[39m AbstractTransformerLM(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_args)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: AbstractTransformerLM.__init__() got an unexpected keyword argument 'n_heads_enc'"
     ]
    }
   ],
   "source": [
    "model_args = dict(\n",
    "    vocab_size=tokenizer.vocab_size, d_model=384, n_layers=6, n_heads_enc=4, n_heads_abs=2, dff=None,\n",
    "    symbol_retrieval='rel_sym_attn', symbol_retrieval_kwargs=dict(\n",
    "        model_dim=384, rel_n_heads=4, symbolic_attn_n_heads=4,\n",
    "        num_symbols=20, nbhd_delta=5, causal_nbhd=True, include_self=False,\n",
    "        normalize_rels=True), # FIXME make names consistent: d_model, model_dim\n",
    "    dropout_rate=0.2, activation='relu', norm_first=True, max_block_size=256, bias=True)\n",
    "model = abstracttransformer_lm = AbstractTransformerLM(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "AbstractTransformerLM                                   [1, 1, 65]                --\n",
       "├─ModuleDict: 1-1                                       --                        --\n",
       "│    └─Embedding: 2-1                                   [1, 256, 384]             24,960\n",
       "│    └─Embedding: 2-2                                   [256, 384]                98,304\n",
       "│    └─ModuleList: 2-3                                  --                        --\n",
       "│    │    └─AbstractEncoderBlock: 3-1                   [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-3                   --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-4                   [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-6                   --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-7                   [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-9                   --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-10                  [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-12                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-13                  [1, 256, 384]             2,684,928\n",
       "│    │    └─AbstractEncoderBlock: 3-14                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-15                  --                        (recursive)\n",
       "│    │    └─AbstractEncoderBlock: 3-16                  [1, 256, 384]             2,684,928\n",
       "│    └─Linear: 2-4                                      [1, 1, 65]                25,025\n",
       "=========================================================================================================\n",
       "Total params: 27,810,881\n",
       "Trainable params: 27,810,881\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 35.02\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 53.48\n",
       "Params size (MB): 30.79\n",
       "Estimated Total Size (MB): 84.27\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_data=torch.randint(0, 10, size=(1,256)), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of params 13,824,833\n"
     ]
    }
   ],
   "source": [
    "# torchinfo overcounts # of params... something to do with symbolic attention shared across layers\n",
    "# this is the correct number (similar to TransformerLM)\n",
    "\n",
    "num_params = model.get_num_params() #sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'# of params {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can we implement in a way that torchinfo can understand? i.e., without \"recursive\" and overcounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 45, with 13,884,672 parameters\n",
      "num non-decayed parameter tensors: 65, with 38,465 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# grad scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = dict(\n",
    "    model=model, get_batch=get_batch, batch_size=batch_size, max_iters=max_iters,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr, eval_model=eval_model,\n",
    "    compile=True, grad_clip=0, gradient_accumulation_steps=1,\n",
    "    eval_main_metric='val/loss', eval_interval=eval_interval, always_save_checkpoint=always_save_checkpoint, out_dir=out_dir,\n",
    "    log_interval=10, wandb_log=True, wandb_init_kwargs=dict(project=wandb_project, name='AbstractTransformerLM (Relational Symbolic Attn)'), \n",
    "    ckpt_dict=dict(model_args=model_args), track_mfu=True,\n",
    "    master_process=True, ddp=False, device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/gibbs/project/lafferty/ma2393/abstract_transformer/experiments/wandb/run-20240118_184804-2v1kqg5t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char/runs/2v1kqg5t' target=\"_blank\">AbstractTransformerLM</a></strong> to <a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--shakespeare_char</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awni00/abstract_transformer--shakespeare_char/runs/2v1kqg5t' target=\"_blank\">https://wandb.ai/awni00/abstract_transformer--shakespeare_char/runs/2v1kqg5t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "step 0: train loss 4.5255, val loss 4.5268\n",
      "iter 0: loss 4.5382, time 62455.47ms, mfu -100.00%\n"
     ]
    }
   ],
   "source": [
    "train_model(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo,--\n",
      "\n",
      "KING HENRY VI:\n",
      "He was for him to be found that Richard now.\n",
      "\n",
      "DERBY:\n",
      "My lord, I proud away not: ount take no letters\n",
      "Twixt thy gracious lord begin. Who's the matter,\n",
      "With some gone aloof?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Say brought your not, sir?\n",
      "\n",
      "TRANCIO:\n",
      "What scorn you do very good note:\n",
      "My lord, I am content my duteous bildhs away.\n",
      "\n",
      "BAGOT:\n",
      "I have subjects beats but next thy father,\n",
      "Apparender thee: Take honour in our senate.\n",
      "\n",
      "PAULINA:\n",
      "This way be thy nose physician, if he be\n",
      "To be a bad-: Tradam? his any conduction,\n",
      "His rason was not a parturer,\n",
      "Or hang denied at the king! Sir Jethop's off,\n",
      "Should be halp there, says Tybalt, Queen Nedalus,\n",
      "Shore the king.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Oh shall forge you ashame to Richmond!\n",
      "\n",
      "KING HENRY VI:\n",
      "For welcome, welcomemes thee from the key-roof\n",
      "Lord MarsS ay, and fly will not this hoofest\n",
      "Rest, and let he go forth his wither's groans?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "True executions of men:\n",
      "Ah, with all horses of conceit so evide.\n",
      "\n",
      "KING HENRY VI:\n",
      "Defusions, worship thee, and what subje\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time,'\n",
    "\n",
    "prompt_idx = torch.from_numpy(np.array(tokenizer.encode(prompt))).unsqueeze(0).to(device)\n",
    "sample_gen = model.generate(prompt_idx, max_new_tokens=100, temperature=1.0, top_k=None)[0]\n",
    "sample_gen = tokenizer.decode(sample_gen)\n",
    "print(sample_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
