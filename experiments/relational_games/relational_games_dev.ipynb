{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torchinfo\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from relational_games_data_utils import RelationalGamesDataset\n",
    "\n",
    "import sys; sys.path.append('../..')\n",
    "from vision_models import ViT, VAT, configure_optimizers\n",
    "from utils.pl_tqdm_progbar import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/relational_games'\n",
    "task = '1task_match_patt'\n",
    "batch_size = 512\n",
    "\n",
    "train_split = 'pentos'\n",
    "\n",
    "train_ds = RelationalGamesDataset(data_path, task, train_split)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "val_ds_dict = dict()\n",
    "val_dls = []\n",
    "val_splits = ('hexos', 'stripes')\n",
    "for val_split in val_splits:\n",
    "    ds = RelationalGamesDataset(data_path, task, val_split)\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_ds_dict[val_split] = ds\n",
    "    val_dls.append(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 36, 36])\n",
      "torch.float32\n",
      "torch.int64\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(x.dtype)\n",
    "    print(y.dtype)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available:  True\n",
      "device count:  1\n",
      "current device name:  NVIDIA H100 80GB HBM3\n",
      "Memory Usage:\n",
      "\tAllocated: 0.0 GB\n",
      "\tReserved:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print('cuda available: ', torch.cuda.is_available())\n",
    "print('device count: ', torch.cuda.device_count())\n",
    "print('current device name: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print('Memory Usage:')\n",
    "print('\\tAllocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('\\tReserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "# optimization hyperparams\n",
    "learning_rate = 5e-4 # with baby networks can afford to go a bit higher #NOTE: this was useful for match_pattern\n",
    "# max_iters = 5000\n",
    "grad_clip = 0.0 # 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# decay_lr = True # whether to decay the learning rate\n",
    "# lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "weight_decay = None # 1e-1 # NOTE: maybe need this?\n",
    "# min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "# warmup_iters = 100\n",
    "gradient_accumulation_steps = 1 # 32 # 1 # accumulate gradients over this many steps. simulates larger batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pytorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_dataloader:\n",
    "#     # print(x)\n",
    "#     print(x[0].shape)\n",
    "#     print(x[1].shape)\n",
    "#     # print(len(x))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_on_step = True\n",
    "\n",
    "class LitVisionModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = torch.nn.functional.cross_entropy\n",
    "        self.accuracy = lambda pred, y: torchmetrics.functional.accuracy(pred, y, task=\"multiclass\", num_classes=n_classes, top_k=1, average='micro')\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "\n",
    "        self.log('train/loss', loss, prog_bar=True, logger=True, on_step=log_on_step, on_epoch=True)\n",
    "        self.log('train/acc', acc, prog_bar=True, logger=True, on_step=log_on_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(f\"val/loss_{val_splits[dataloader_idx]}\", loss, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "        self.log(f\"val/acc_{val_splits[dataloader_idx]}\", acc, prog_bar=True, logger=True, add_dataloader_idx=False)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = configure_optimizers(self.model, weight_decay, learning_rate, (beta1, beta2), device_type=device)\n",
    "        # optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "        return optimizer\n",
    "\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c, w, h = images.shape[1:]\n",
    "c, w, h = (3, 36, 36)\n",
    "image_shape = (c, w, h)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS WORKED FOR MATCH-PATTERN\n",
    "# # model args\n",
    "# symbol_type = 'positional_symbols'\n",
    "# d_model, n_layers, dff = 128, 1, 256\n",
    "# sa, rca = 4, 4\n",
    "# patch_size = (12, 12)\n",
    "# n_patches = (w // patch_size[0]) * (h // patch_size[1])\n",
    "# activation = 'gelu'\n",
    "# dropout_rate = 0.1\n",
    "# rca_type = 'relational_attention'\n",
    "# norm_first = False\n",
    "# bias = False\n",
    "# pool = 'mean'\n",
    "\n",
    "# run_name = f'sa={sa}; rca={rca}; d={d_model}; L={n_layers}; rca_type={rca_type}; symbol_type={symbol_type}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "symbol_type = 'positional_symbols'\n",
    "d_model, n_layers, dff = 128, 2, 256\n",
    "# d_model, n_layers, dff = 32, 2, 64\n",
    "# sa, rca = 4, 4\n",
    "# sa, rca = 2, 2\n",
    "# sa, rca = 8, 0\n",
    "# sa, rca = 4, 0\n",
    "sa, rca = 4, 0\n",
    "patch_size = (12, 12)\n",
    "n_patches = (w // patch_size[0]) * (h // patch_size[1])\n",
    "activation = 'swiglu'\n",
    "dropout_rate = 0.1\n",
    "# rca_type = 'relational_attention'\n",
    "rca_type = 'rca'\n",
    "norm_first = True\n",
    "bias = False\n",
    "pool = 'mean'\n",
    "norm_type = 'layernorm'\n",
    "\n",
    "run_name = f'sa={sa}; rca={rca}; d={d_model}; L={n_layers}; rca_type={rca_type}; symbol_type={symbol_type}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %\n",
      "============================================================================================================================================\n",
      "ViT                                      [1, 3, 36, 36]            [1, 2]                    1,408                       0.36%\n",
      "├─Sequential: 1-1                        [1, 3, 36, 36]            [1, 9, 128]               --                             --\n",
      "│    └─Rearrange: 2-1                    [1, 3, 36, 36]            [1, 9, 432]               --                             --\n",
      "│    └─LayerNorm: 2-2                    [1, 9, 432]               [1, 9, 432]               864                         0.22%\n",
      "│    └─Linear: 2-3                       [1, 9, 432]               [1, 9, 128]               55,424                     14.32%\n",
      "│    └─LayerNorm: 2-4                    [1, 9, 128]               [1, 9, 128]               256                         0.07%\n",
      "├─Dropout: 1-2                           [1, 10, 128]              [1, 10, 128]              --                             --\n",
      "├─ModuleList: 1-3                        --                        --                        --                             --\n",
      "│    └─EncoderBlock: 2-5                 [1, 10, 128]              [1, 10, 128]              --                             --\n",
      "│    │    └─LayerNorm: 3-1               [1, 10, 128]              [1, 10, 128]              256                         0.07%\n",
      "│    │    └─Attention: 3-2               --                        [1, 10, 128]              65,536                     16.94%\n",
      "│    │    └─Dropout: 3-3                 [1, 10, 128]              [1, 10, 128]              --                             --\n",
      "│    │    └─LayerNorm: 3-4               [1, 10, 128]              [1, 10, 128]              256                         0.07%\n",
      "│    │    └─FeedForwardBlock: 3-5        [1, 10, 128]              [1, 10, 128]              98,304                     25.41%\n",
      "│    │    └─Dropout: 3-6                 [1, 10, 128]              [1, 10, 128]              --                        (recursive)\n",
      "│    └─EncoderBlock: 2-6                 [1, 10, 128]              [1, 10, 128]              --                             --\n",
      "│    │    └─LayerNorm: 3-7               [1, 10, 128]              [1, 10, 128]              256                         0.07%\n",
      "│    │    └─Attention: 3-8               --                        [1, 10, 128]              65,536                     16.94%\n",
      "│    │    └─Dropout: 3-9                 [1, 10, 128]              [1, 10, 128]              --                             --\n",
      "│    │    └─LayerNorm: 3-10              [1, 10, 128]              [1, 10, 128]              256                         0.07%\n",
      "│    │    └─FeedForwardBlock: 3-11       [1, 10, 128]              [1, 10, 128]              98,304                     25.41%\n",
      "│    │    └─Dropout: 3-12                [1, 10, 128]              [1, 10, 128]              --                        (recursive)\n",
      "├─Linear: 1-4                            [1, 128]                  [1, 2]                    258                         0.07%\n",
      "============================================================================================================================================\n",
      "Total params: 386,914\n",
      "Trainable params: 386,914\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.39\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.27\n",
      "Params size (MB): 1.54\n",
      "Estimated Total Size (MB): 1.83\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# define kwargs for symbol-retrieval module based on type\n",
    "rca_kwargs = dict()\n",
    "if symbol_type == 'symbolic_attention':\n",
    "    symbol_retrieval_kwargs = dict(d_model=d_model, n_symbols=50, n_heads=4) # NOTE: n_heads, n_symbols fixed for now\n",
    "elif symbol_type == 'positional_symbols':\n",
    "    symbol_retrieval_kwargs = dict(symbol_dim=d_model, max_length=n_patches+1)\n",
    "elif symbol_type == 'position_relative':\n",
    "    symbol_retrieval_kwargs = dict(symbol_dim=d_model, max_rel_pos=n_patches+1)\n",
    "    rca_kwargs['use_relative_positional_symbols'] = True # if using position-relative symbols, need to tell RCA module\n",
    "elif rca != 0:\n",
    "    raise ValueError(f'`symbol_type` {symbol_type} not valid')\n",
    "\n",
    "# if rca=0, use TransformerLM\n",
    "if rca == 0:\n",
    "    model_args = dict(\n",
    "        image_shape=image_shape, patch_size=patch_size, num_classes=n_classes, pool=pool,\n",
    "        d_model=d_model, n_layers=n_layers, n_heads=sa, dff=dff, dropout_rate=dropout_rate, norm_type=norm_type,\n",
    "        activation=activation, norm_first=norm_first, bias=bias)\n",
    "\n",
    "    model = transformer_lm = ViT(**model_args).to(device)\n",
    "# otherwise, use AbstractTransformerLM\n",
    "else:\n",
    "    model_args = dict(\n",
    "        image_shape=image_shape, patch_size=patch_size, num_classes=n_classes, pool=pool,\n",
    "        d_model=d_model, n_layers=n_layers, n_heads_sa=sa, n_heads_rca=rca, dff=dff, dropout_rate=dropout_rate, norm_type=norm_type,\n",
    "        activation=activation, norm_first=norm_first, bias=bias, rca_type=rca_type,\n",
    "        symbol_retrieval=symbol_type, symbol_retrieval_kwargs=symbol_retrieval_kwargs, rca_kwargs=rca_kwargs)\n",
    "\n",
    "    model = abstracttransformer_lm = VAT(**model_args).to(device)\n",
    "\n",
    "print(torchinfo.summary(\n",
    "    model, input_size=(1, *image_shape),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"params_percent\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unoptimized_model = model\n",
    "model = torch.compile(model)\n",
    "lit_model = LitVisionModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_wandb = False\n",
    "n_epochs = 25\n",
    "max_steps = -1\n",
    "log_every_n_steps = 20\n",
    "eval_interval = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ma2393/.conda/envs/abstract_transformer/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-05-04 21:12:08.009134: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 21:12:08.036637: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-04 21:12:08.036666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-04 21:12:08.037691: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-04 21:12:08.043441: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 21:12:09.997661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ma2393/.conda/envs/abstract_transformer/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /gpfs/radev/project/lafferty/ma2393/abstract_transformer/experiments/relational_games/lightning_logs/version_5898/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | OptimizedModule | 386 K \n",
      "------------------------------------------\n",
      "386 K     Trainable params\n",
      "0         Non-trainable params\n",
      "386 K     Total params\n",
      "1.548     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 489/489 [00:03<00:00, 127.29it/s, v_num=5898, train/loss_step=0.693, train/acc_step=0.521, val/loss_hexos=0.693, val/acc_hexos=0.497, val/loss_stripes=0.693, val/acc_stripes=0.496, train/loss_epoch=0.693, train/acc_epoch=0.506]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 489/489 [00:03<00:00, 126.67it/s, v_num=5898, train/loss_step=0.693, train/acc_step=0.521, val/loss_hexos=0.693, val/acc_hexos=0.497, val/loss_stripes=0.693, val/acc_stripes=0.496, train/loss_epoch=0.693, train/acc_epoch=0.506]\n"
     ]
    }
   ],
   "source": [
    "if log_to_wandb:\n",
    "    run = wandb.init(project=wandb_project, group=group_name, name=run_name,\n",
    "        config={'group': group_name, 'num_params': num_params, **model_args})\n",
    "\n",
    "    wandb_logger = WandbLogger(experiment=run, log_model=log_model),\n",
    "else:\n",
    "    wandb_logger = None\n",
    "\n",
    "callbacks = [\n",
    "    # TQDMProgressBar(refresh_rate=50)\n",
    "    TQDMProgressBar(),\n",
    "    # L.pytorch.callbacks.ModelCheckpoint(dirpath=f'out/{run_name}', every_n_train_steps=10) #every_n_epochs=1)\n",
    "]\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    max_epochs=n_epochs, enable_model_summary=True, benchmark=True, enable_checkpointing=True,\n",
    "    enable_progress_bar=True, callbacks=callbacks, logger=wandb_logger,\n",
    "    accumulate_grad_batches=gradient_accumulation_steps, gradient_clip_val=grad_clip,\n",
    "    # log_every_n_steps=log_every_n_steps, max_steps=max_steps, val_check_interval=eval_interval) # FIXME\n",
    "    log_every_n_steps=log_every_n_steps, max_steps=max_steps)#, val_check_interval=eval_interval)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    **trainer_kwargs\n",
    "    )\n",
    "\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dls)\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # L=1, sa=0, rca=4, layernorm, d_model, n_layers, dff = 64, 1, 128, mean pooling\n",
    "# Epoch 24: 100%|██████████| 977/977 [00:06<00:00, 154.98it/s, v_num=5898, train/loss_step=0.432, train/acc_step=0.799, val/loss_hexos=0.553, val/acc_hexos=0.807, val/loss_stripes=1.190, val/acc_stripes=0.569, train/loss_epoch=0.437, train/acc_epoch=0.805]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=4, rca=0, layernorm, d_model, n_layers, dff = 64, 1, 128, mean pooling\n",
    "# Epoch 24: 100%|██████████| 977/977 [00:05<00:00, 165.00it/s, v_num=5898, train/loss_step=0.509, train/acc_step=0.771, val/loss_hexos=0.648, val/acc_hexos=0.713, val/loss_stripes=0.958, val/acc_stripes=0.582, train/loss_epoch=0.565, train/acc_epoch=0.714]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=2, rca=2, layernorm, d_model, n_layers, dff = 64, 1, 128, mean pooling\n",
    "# Epoch 24: 100%|██████████| 977/977 [00:06<00:00, 145.32it/s, v_num=5898, train/loss_step=0.695, train/acc_step=0.465, val/loss_hexos=0.693, val/acc_hexos=0.502, val/loss_stripes=0.693, val/acc_stripes=0.497, train/loss_epoch=0.693, train/acc_epoch=0.508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=2, rca=2, layernorm, d_model, n_layers, dff = 64, 1, 128, mean pooling; sym_attn\n",
    "# Epoch 24: 100%|██████████| 977/977 [00:06<00:00, 141.41it/s, v_num=5898, train/loss_step=0.548, train/acc_step=0.729, val/loss_hexos=0.579, val/acc_hexos=0.728, val/loss_stripes=0.757, val/acc_stripes=0.575, train/loss_epoch=0.568, train/acc_epoch=0.707]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=2, rca=2, layernorm, d_model, n_layers, dff = 64, 2, 128, mean pooling; sym_attn; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 101.71it/s, v_num=5898, train/loss_step=0.255, train/acc_step=0.910, val/loss_hexos=0.484, val/acc_hexos=0.866, val/loss_stripes=1.560, val/acc_stripes=0.643, train/loss_epoch=0.192, train/acc_epoch=0.931]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=2, rca=2, layernorm, d_model, n_layers, dff = 64, 2, 128, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 103.85it/s, v_num=5898, train/loss_step=0.188, train/acc_step=0.931, val/loss_hexos=0.325, val/acc_hexos=0.903, val/loss_stripes=1.050, val/acc_stripes=0.762, train/loss_epoch=0.191, train/acc_epoch=0.931]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=0, rca=4, layernorm, d_model, n_layers, dff = 64, 2, 128, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 110.84it/s, v_num=5898, train/loss_step=0.317, train/acc_step=0.896, val/loss_hexos=0.511, val/acc_hexos=0.851, val/loss_stripes=1.280, val/acc_stripes=0.691, train/loss_epoch=0.238, train/acc_epoch=0.914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=2, rca=2, layernorm, d_model, n_layers, activation=gelu; dff = 64, 2, 128, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 106.02it/s, v_num=5898, train/loss_step=0.459, train/acc_step=0.785, val/loss_hexos=0.538, val/acc_hexos=0.766, val/loss_stripes=0.800, val/acc_stripes=0.688, train/loss_epoch=0.469, train/acc_epoch=0.775]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=2, rca=2, layernorm, d_model, n_layers, activation=swiglu; dff = 64, 2, 64*4, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 106.59it/s, v_num=5898, train/loss_step=0.168, train/acc_step=0.917, val/loss_hexos=0.665, val/acc_hexos=0.841, val/loss_stripes=2.550, val/acc_stripes=0.546, train/loss_epoch=0.201, train/acc_epoch=0.928]\n",
    "# hexos better but stripes worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=0, rca=4, layernorm, d_model, n_layers, activation=swiglu; dff = 64, 2, 64*4, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 115.16it/s, v_num=5898, train/loss_step=0.185, train/acc_step=0.917, val/loss_hexos=0.539, val/acc_hexos=0.861, val/loss_stripes=1.650, val/acc_stripes=0.642, train/loss_epoch=0.186, train/acc_epoch=0.933]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=0, rca=4, layernorm, d_model, n_layers, activation=swiglu; dff = 32, 2, 32*4, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 115.21it/s, v_num=5898, train/loss_step=0.377, train/acc_step=0.826, val/loss_hexos=0.527, val/acc_hexos=0.820, val/loss_stripes=0.858, val/acc_stripes=0.712, train/loss_epoch=0.410, train/acc_epoch=0.820]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=0, rca=4, layernorm, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 114.35it/s, v_num=5898, train/loss_step=0.296, train/acc_step=0.903, val/loss_hexos=0.543, val/acc_hexos=0.830, val/loss_stripes=1.220, val/acc_stripes=0.627, train/loss_epoch=0.272, train/acc_epoch=0.898]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=2, sa=2, rca=2, layernorm, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:05<00:00, 93.00it/s, v_num=5898, train/loss_step=0.700, train/acc_step=0.458, val/loss_hexos=0.691, val/acc_hexos=0.527, val/loss_stripes=0.693, val/acc_stripes=0.510, train/loss_epoch=0.690, train/acc_epoch=0.527]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=2, rca=2, layernorm, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:05<00:00, 90.42it/s, v_num=5898, train/loss_step=0.471, train/acc_step=0.778, val/loss_hexos=0.538, val/acc_hexos=0.757, val/loss_stripes=0.744, val/acc_stripes=0.686, train/loss_epoch=0.474, train/acc_epoch=0.772]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=4, rca=0, layernorm, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:03<00:00, 125.21it/s, v_num=5898, train/loss_step=0.493, train/acc_step=0.778, val/loss_hexos=0.513, val/acc_hexos=0.789, val/loss_stripes=0.661, val/acc_stripes=0.701, train/loss_epoch=0.551, train/acc_epoch=0.736]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=0, rca=4, layernorm, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:05<00:00, 97.05it/s, v_num=5898, train/loss_step=0.284, train/acc_step=0.917, val/loss_hexos=0.597, val/acc_hexos=0.826, val/loss_stripes=1.460, val/acc_stripes=0.632, train/loss_epoch=0.292, train/acc_epoch=0.890]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=0, rca=4, norm_type=none, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 115.33it/s, v_num=5898, train/loss_step=0.354, train/acc_step=0.833, val/loss_hexos=0.346, val/acc_hexos=0.857, val/loss_stripes=0.748, val/acc_stripes=0.680, train/loss_epoch=0.327, train/acc_epoch=0.872]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=0, rca=4, norm_type=none, d_model, n_layers, activation=swiglu; dff = 32, 2, 64, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 112.48it/s, v_num=5898, train/loss_step=0.549, train/acc_step=0.750, val/loss_hexos=0.531, val/acc_hexos=0.761, val/loss_stripes=0.594, val/acc_stripes=0.738, train/loss_epoch=0.482, train/acc_epoch=0.769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=1, sa=0, rca=4, norm_type=none, d_model, n_layers, activation=swiglu; dff = 128, 2, 256, mean pooling; pos_sym_retriever; batch_size = 512 (from 256)\n",
    "# Epoch 24: 100%|██████████| 489/489 [00:04<00:00, 119.37it/s, v_num=5898, train/loss_step=0.0976, train/acc_step=0.965, val/loss_hexos=0.213, val/acc_hexos=0.941, val/loss_stripes=0.751, val/acc_stripes=0.803, train/loss_epoch=0.171, train/acc_epoch=0.941]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstract_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
