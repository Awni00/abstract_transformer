{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_q(text):\n",
    "    l = len(text)\n",
    "    return [char_to_idx[c] for c in text] + [eos_token] + [empty_token] * (max_q_len - l - 1)\n",
    "\n",
    "def tokenize_a(text):\n",
    "    l = len(text)\n",
    "    token_a = [char_to_idx[c] for c in text]\n",
    "    token_a = [start_token] + token_a + [eos_token] + [empty_token] * (max_a_len - l - 1)\n",
    "    token_target = token_a[:-1]\n",
    "    token_label = token_a[1:]\n",
    "    return token_target, token_label\n",
    "\n",
    "def invert_tokenization(idx):\n",
    "    return [idx_to_char[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa(filename):\n",
    "    with open(f'{data_path}/{tr_l}/{task}.txt') as f:\n",
    "        text = f.read().splitlines()\n",
    "        quess = text[::2]\n",
    "        anss = text[1::2]\n",
    "\n",
    "    return quess, anss\n",
    "\n",
    "def tokenize_qa(quess, anss):\n",
    "    tokenized_source = [tokenize_q(text) for text in tqdm(quess)]\n",
    "\n",
    "    tokenized_target, tokenized_label = [], []\n",
    "    for text in tqdm(anss):\n",
    "        tt, tl = tokenize_a(text)\n",
    "        tokenized_target.append(tt)\n",
    "        tokenized_label.append(tl)\n",
    "\n",
    "    tokenized_source = torch.tensor(tokenized_source)\n",
    "    tokenized_target = torch.tensor(tokenized_target)\n",
    "    tokenized_label = torch.tensor(tokenized_label)\n",
    "\n",
    "    return tokenized_source, tokenized_target, tokenized_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds(fname):\n",
    "    quess, anss = load_qa(fname)\n",
    "\n",
    "    tokenized_source, tokenized_target, tokenized_label = tokenize_qa(quess, anss)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(tokenized_source, tokenized_target, tokenized_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/math/mathematics_dataset-v1.0'\n",
    "out_path = 'tokenized_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_vectorizer/vocabulary.txt') as f:\n",
    "    vocab = f.read().splitlines()\n",
    "\n",
    "idx_to_char = {i: c for i, c in enumerate(vocab)}\n",
    "char_to_idx = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "empty_token = char_to_idx['']\n",
    "eos_token = char_to_idx[';']\n",
    "start_token = char_to_idx['@']\n",
    "\n",
    "max_q_len, max_a_len = 161, 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks =  ['algebra__linear_1d', 'polynomials__add', 'polynomials__expand', 'calculus__differentiate', 'algebra__sequence_next_term']\n",
    "\n",
    "# task = 'polynomials__add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_levels = ['train-easy', 'train-medium', 'train-hard']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_task(task):\n",
    "    tss, tts, tls = [], [], []\n",
    "\n",
    "    for tr_l in tqdm(train_levels):\n",
    "        fname = f'{data_path}/{tr_l}/{task}.txt'\n",
    "\n",
    "        quess, anss = load_qa(fname)\n",
    "\n",
    "        tokenized_source, tokenized_target, tokenized_label = tokenize_qa(quess, anss)\n",
    "        tss.append(tokenized_source)\n",
    "        tts.append(tokenized_target)\n",
    "        tls.append(tokenized_label)\n",
    "\n",
    "        del quess, anss\n",
    "\n",
    "    ts = torch.concat(tss)\n",
    "    tt = torch.concat(tts)\n",
    "    tl = torch.concat(tls)\n",
    "\n",
    "    del tss, tts, tls\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(ts, tt, tl)\n",
    "\n",
    "    torch.save(train_ds, f'{out_path}/{task}_train.pt')\n",
    "\n",
    "    interpolate_ds = create_ds(f'{data_path}/interpolate/{task}.txt')\n",
    "    torch.save(interpolate_ds, f'{out_path}/{task}_interpolate.pt')\n",
    "    interpolate_ds = create_ds(f'{data_path}/extrapolate/{task}.txt')\n",
    "    torch.save(interpolate_ds, f'{out_path}/{task}_extrapolate.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algebra__linear_1d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666666/666666 [00:02<00:00, 239459.47it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 191717.96it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 181588.21it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 188481.38it/s]\n",
      "100%|██████████| 666666/666666 [00:02<00:00, 247201.97it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 195482.21it/s]\n",
      "100%|██████████| 3/3 [01:00<00:00, 20.03s/it]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 176004.45it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 187816.22it/s]\n",
      "100%|██████████| 666666/666666 [00:02<00:00, 235482.89it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 193230.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polynomials__add\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666666/666666 [00:05<00:00, 131617.48it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 175334.62it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 145356.81it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 178173.26it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 150836.72it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 179684.41it/s]\n",
      "100%|██████████| 3/3 [01:05<00:00, 21.98s/it]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 142044.04it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 182500.13it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 135371.59it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 179334.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polynomials__expand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666666/666666 [00:04<00:00, 165741.30it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 216347.83it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 153999.34it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 180422.63it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 145408.23it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 174631.02it/s]\n",
      "100%|██████████| 3/3 [01:04<00:00, 21.39s/it]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 166855.44it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 216377.50it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 156434.31it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 187101.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculus__differentiate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666666/666666 [00:04<00:00, 148411.50it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 170079.46it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 197250.91it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 177817.90it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 152056.37it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 172489.45it/s]\n",
      "100%|██████████| 3/3 [01:04<00:00, 21.53s/it]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 173690.80it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 214842.55it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 163104.72it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 183468.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algebra__sequence_next_term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666666/666666 [00:04<00:00, 155121.86it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 183576.02it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 214646.74it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 191979.51it/s]\n",
      "100%|██████████| 666666/666666 [00:04<00:00, 161735.94it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 184931.65it/s]\n",
      "100%|██████████| 3/3 [01:03<00:00, 21.10s/it]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 184356.03it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 185265.19it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 193069.39it/s]\n",
      "100%|██████████| 666666/666666 [00:03<00:00, 187188.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    print(task)\n",
    "    tokenize_task(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('abstract_transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8af8745886d4de51e837abafc38af8fb9452f5565518612da5aaf75440d8b7fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
