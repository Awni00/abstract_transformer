{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import torchinfo\n",
    "from contextlib import nullcontext\n",
    "from  tqdm import tqdm, trange\n",
    "\n",
    "import os\n",
    "import sys; sys.path += ['../', '../..']\n",
    "from train_utils import train_model\n",
    "from seq2seq_models import Seq2SeqAbstractTransformer, Seq2SeqTransformer\n",
    "from og_seq2seq_models import Seq2SeqAbstractorArchb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "\n",
    "# system\n",
    "# device = 'cpu'\n",
    "device = 'cuda'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "dtype = 'float32'\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "compile = True\n",
    "\n",
    "# evaluation and output\n",
    "out_dir = '../out/object_sorting'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "eval_interval = 250 # keep frequent because we'll overfit\n",
    "eval_iters = 200\n",
    "log_interval = 10 # don't print too too often\n",
    "\n",
    "# we expect to overfit on this small dataset, so only save when val improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = 'abstract_transformer--object_sorting'\n",
    "\n",
    "# optimization hyperparams\n",
    "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
    "max_iters = 5000\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "lr_decay_iters = 5000 # make equal to max_iters usually\n",
    "weight_decay = 1e-1\n",
    "min_lr = 1e-4 # learning_rate / 10 usually\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
    "warmup_iters = 100\n",
    "gradient_accumulation_steps = 1 # accumulate gradients over this many steps. simulates larger batch size\n",
    "\n",
    "# batch size and block size\n",
    "# batch_size = 64\n",
    "# block_size = 256\n",
    "\n",
    "# DDP (distributed data parallel) training\n",
    "ddp = False\n",
    "master_process = True\n",
    "\n",
    "# TODO: set up DDP for future experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'object_sorting_datasets/task1_object_sort_dataset.npy'\n",
    "data = np.load(data_path, allow_pickle=True).item()\n",
    "\n",
    "objects, seqs, sorted_seqs, object_seqs, target, labels, start_token = tuple(\n",
    "    data[key] for key in ['objects', 'seqs', 'sorted_seqs', 'object_seqs', 'target', 'labels', 'start_token'])\n",
    "\n",
    "# convert to torch tensors\n",
    "object_seqs = torch.tensor(object_seqs, dtype=ptdtype, device=device)\n",
    "target = torch.tensor(target, dtype=torch.long, device=device)\n",
    "labels = torch.tensor(labels, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(*arrays, val_size=0.1, test_size=0.2):\n",
    "    n = len(arrays[0])\n",
    "    indices = np.random.permutation(n)\n",
    "    val_start = int(n * (1 - val_size - test_size))\n",
    "    test_start = int(n * (1 - test_size))\n",
    "    train_indices = indices[:val_start]\n",
    "    val_indices = indices[val_start:test_start]\n",
    "    test_indices = indices[test_start:]\n",
    "    return tuple(tuple(array[idx] for idx in (train_indices, val_indices, test_indices)) for array in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(object_seqs_train, object_seqs_val, object_seqs_test), (target_train, target_val, target_test), (labels_train, labels_val, labels_test) = train_val_test_split(\n",
    "    object_seqs, target, labels, val_size=0.1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shapes: torch.Size([70000, 10, 8]), torch.Size([70000, 10]), torch.Size([70000, 10])\n",
      "validation shapes: torch.Size([10000, 10, 8]), torch.Size([10000, 10]), torch.Size([10000, 10])\n",
      "test shapes: torch.Size([20000, 10, 8]), torch.Size([20000, 10]), torch.Size([20000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f'training shapes: {object_seqs_train.shape}, {target_train.shape}, {labels_train.shape}')\n",
    "print(f'validation shapes: {object_seqs_val.shape}, {target_val.shape}, {labels_val.shape}')\n",
    "print(f'test shapes: {object_seqs_test.shape}, {target_test.shape}, {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1500\n",
    "sample_idx = np.random.choice(object_seqs_train.shape[0], train_size)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(object_seqs_train[sample_idx], target_train[sample_idx], labels_train[sample_idx])\n",
    "val_ds = torch.utils.data.TensorDataset(object_seqs_val, target_val, labels_val)\n",
    "test_ds = torch.utils.data.TensorDataset(object_seqs_test, target_test, labels_test)\n",
    "\n",
    "batch_size = 128 # 512\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dl(train_size, batch_size=batch_size):\n",
    "    sample_idx = np.random.choice(object_seqs_train.shape[0], train_size)\n",
    "    train_ds = torch.utils.data.TensorDataset(object_seqs_train[sample_idx], target_train[sample_idx], labels_train[sample_idx])\n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "# TODO: add to module\n",
    "# TODO: add features; e.g., logging etc\n",
    "\n",
    "class LitSeq2SeqModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, z = batch\n",
    "        with ctx:\n",
    "            logits, loss = self.model(x, y, z)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, z = batch\n",
    "        with ctx:\n",
    "            logits, loss = self.model(x, y, z)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        tf_acc = torch.mean((torch.argmax(logits, dim=-1) == z).float())\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, z = batch\n",
    "\n",
    "        n, seqs_length = y.shape\n",
    "        output = torch.zeros(size=(n, (seqs_length+1)), dtype=torch.int, device=device)\n",
    "        output[:,0] = start_token\n",
    "\n",
    "        for i in range(seqs_length):\n",
    "            with ctx:\n",
    "                predictions, _ = self.model(x, output[:, :-1], z)\n",
    "            predictions = predictions[:, i, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1)\n",
    "            output[:,i+1] = predicted_id\n",
    "\n",
    "        elementwise_acc = torch.mean((output[:,1:] == z).float()).item()\n",
    "        # acc_per_position = [torch.mean((output[:, i+1] == labels_test[:, i]).float()).item() for i in range(seqs_length)]\n",
    "        seq_acc = torch.mean((torch.all(output[:,1:]==z, axis=1)).float()).item()\n",
    "\n",
    "        with ctx:\n",
    "            tf_pred, loss = self.model(x, y, z)\n",
    "            tf_pred = torch.argmax(tf_pred, axis=-1)\n",
    "        teacher_forcing_acc = torch.mean((z==tf_pred).float()).item()\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"teacher_forcing_acc\", teacher_forcing_acc)\n",
    "        self.log(\"elementwise_acc\", elementwise_acc)\n",
    "        self.log(\"seq_acc\", seq_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_lr(it):\n",
    "#     # 1) linear warmup for warmup_iters steps\n",
    "#     if it < warmup_iters:\n",
    "#         return learning_rate * it / warmup_iters\n",
    "#     # 2) if it > lr_decay_iters, return min learning rate\n",
    "#     if it > lr_decay_iters:\n",
    "#         return min_lr\n",
    "#     # 3) in between, use cosine decay down to min learning rate\n",
    "#     decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "#     assert 0 <= decay_ratio <= 1\n",
    "#     coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "#     return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def get_lr(it):\n",
    "    return 0.001\n",
    "@torch.no_grad()\n",
    "def eval_model(model, ctx=None):\n",
    "\n",
    "    ctx = nullcontext() if ctx is None else ctx\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        dl = train_dl if split == 'train' else val_dl\n",
    "        max_batches = min(eval_iters, len(dl)) if eval_iters is not None else len(dl)\n",
    "        losses = torch.zeros(max_batches)\n",
    "        tfaccs = torch.zeros(max_batches)\n",
    "        for k, batch in enumerate(dl):\n",
    "            source, target, label = batch\n",
    "            if eval_iters is not None and k >= max_batches:\n",
    "                break\n",
    "            with ctx:\n",
    "                logits, loss = model(source, target, label)\n",
    "            losses[k] = loss.item()\n",
    "            tfaccs[k] = torch.mean((torch.argmax(logits, dim=-1) == label).float())\n",
    "\n",
    "        out[f'{split}/loss'] = losses.mean() # FIXME loss is averaged over batch. batch sizes may be unnequal?\n",
    "        out[f'{split}/tfacc'] = tfaccs.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_seq2seq_model(model, source_test, target_test, labels_test, start_token, print_=False, ctx=ctx):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    n, seqs_length = target_test.shape\n",
    "    output = torch.zeros(size=(n, (seqs_length+1)), dtype=torch.int, device=device)\n",
    "    output[:,0] = start_token\n",
    "\n",
    "    for i in range(seqs_length):\n",
    "        with ctx:\n",
    "            predictions, _ = model(source_test, output[:, :-1], labels_test)\n",
    "        predictions = predictions[:, i, :]\n",
    "        predicted_id = torch.argmax(predictions, axis=-1)\n",
    "        output[:,i+1] = predicted_id\n",
    "\n",
    "    elementwise_acc = torch.mean((output[:,1:] == labels_test).float()).item()\n",
    "    acc_per_position = [torch.mean((output[:, i+1] == labels_test[:, i]).float()).item() for i in range(seqs_length)]\n",
    "    seq_acc = torch.mean((torch.all(output[:,1:]==labels_test, axis=1)).float()).item()\n",
    "\n",
    "    with ctx:\n",
    "        tf_pred = model(source_test, target_test, labels_test)[0]\n",
    "        tf_pred = torch.argmax(tf_pred, axis=-1)\n",
    "    teacher_forcing_acc = torch.mean((labels_test==tf_pred).float()).item()\n",
    "\n",
    "    if print_:\n",
    "        print('element-wise accuracy: %.2f%%' % (100*elementwise_acc))\n",
    "        print('full sequence accuracy: %.2f%%' % (100*seq_acc))\n",
    "        print('teacher-forcing accuracy:  %.2f%%' % (100*teacher_forcing_acc))\n",
    "\n",
    "\n",
    "    return_dict = {\n",
    "        'elementwise_accuracy': elementwise_acc, 'full_sequence_accuracy': seq_acc,\n",
    "        'teacher_forcing_accuracy': teacher_forcing_acc, 'acc_by_position': acc_per_position\n",
    "        }\n",
    "\n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement attn with relative positional embedding or RoPE\n",
    "# TODO: add these to module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Transforemr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                           Param #              Param %              Trainable\n",
       "=======================================================================================================================================\n",
       "Seq2SeqTransformer (Seq2SeqTransformer)                                     --                        --              True\n",
       "├─ModuleDict (layers): 1-1                                                  --                        --              True\n",
       "│    └─Linear (source_embedder): 2-1                                        576                    0.34%              True\n",
       "│    └─Embedding (target_embedder): 2-2                                     704                    0.42%              True\n",
       "│    └─SinusoidalPositionalEncoding (source_pos_embedder): 2-3              --                        --              --\n",
       "│    │    └─Dropout (dropout): 3-1                                          --                        --              --\n",
       "│    └─SinusoidalPositionalEncoding (target_pos_embedder): 2-4              --                        --              --\n",
       "│    │    └─Dropout (dropout): 3-2                                          --                        --              --\n",
       "│    └─ModuleList (encoder_blocks): 2-5                                     --                        --              True\n",
       "│    │    └─EncoderBlock (0): 3-3                                           33,472                19.76%              True\n",
       "│    │    └─EncoderBlock (1): 3-4                                           33,472                19.76%              True\n",
       "│    └─ModuleList (decoder_blocks): 2-6                                     --                        --              True\n",
       "│    │    └─DecoderBlock (0): 3-5                                           50,240                29.67%              True\n",
       "│    │    └─DecoderBlock (1): 3-6                                           50,240                29.67%              True\n",
       "│    └─Linear (final_out): 2-7                                              650                    0.38%              True\n",
       "=======================================================================================================================================\n",
       "Total params: 169,354\n",
       "Trainable params: 169,354\n",
       "Non-trainable params: 0\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = dict(\n",
    "    input_spec=dict(type='vector', dim=8), output_spec=dict(type='token', vocab_size=10+1),\n",
    "    d_model=64, out_dim=10, n_layers_enc=2, n_layers_dec=2,\n",
    "    encoder_kwargs=dict(n_heads=2, dff=128, activation='relu', norm_first=True, dropout_rate=0.1, causal=False),\n",
    "    decoder_kwargs=dict(n_heads=2, dff=128, activation='relu', norm_first=True, dropout_rate=0.1, causal=True),\n",
    "    in_block_size=10, out_block_size=10)\n",
    "seq2seqtransformer = Seq2SeqTransformer(**model_args)\n",
    "torchinfo.summary(seq2seqtransformer, row_settings=[\"depth\", \"var_names\"], col_names=[\"num_params\", \"params_percent\", \"trainable\"], depth=3, col_width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, train_dl\u001b[38;5;241m=\u001b[39mtrain_dl, eval_model\u001b[38;5;241m=\u001b[39meval_model, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, scaler\u001b[38;5;241m=\u001b[39mscaler, get_lr\u001b[38;5;241m=\u001b[39mget_lr,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, grad_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      5\u001b[0m     eval_main_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval/loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     always_save_checkpoint\u001b[38;5;241m=\u001b[39malways_save_checkpoint, ckpt_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(model_args\u001b[38;5;241m=\u001b[39mmodel_args), out_dir\u001b[38;5;241m=\u001b[39mout_dir,\n\u001b[1;32m      7\u001b[0m     wandb_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, wandb_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(project\u001b[38;5;241m=\u001b[39mwandb_project, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m), track_mfu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     ddp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "train_kwargs = dict(\n",
    "    model=model, train_dl=train_dl, eval_model=eval_model, n_epochs=200,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr,\n",
    "    compile=True, grad_clip=0,\n",
    "    eval_main_metric='val/loss',\n",
    "    always_save_checkpoint=always_save_checkpoint, ckpt_dict=dict(model_args=model_args), out_dir=out_dir,\n",
    "    wandb_log=False, wandb_init_kwargs=dict(project=wandb_project, name='Transformer'), track_mfu=True,\n",
    "    ddp=False, device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model... done compiling.\n",
      "starting training loop...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/abstract_transformer/experiments/object_sorting/train_utils.py:55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, eval_model, n_epochs, optimizer, scaler, get_lr, compile, grad_clip, eval_main_metric, always_save_checkpoint, ckpt_dict, out_dir, wandb_log, wandb_init_kwargs, track_mfu, ddp, device_type)\u001b[0m\n\u001b[1;32m     52\u001b[0m     param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 55\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# clip the gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/abstract_transformer/experiments/object_sorting/../../seq2seq_models.py:174\u001b[0m, in \u001b[0;36mSeq2SeqAbstractTransformer.forward\u001b[0;34m(self, x, y, targets)\u001b[0m\n\u001b[1;32m    171\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mtarget_pos_embedder(y)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mencoder_blocks:\n\u001b[0;32m--> 174\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43menc_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdecoder_blocks:\n\u001b[1;32m    177\u001b[0m     y \u001b[38;5;241m=\u001b[39m dec_block(y, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/abstract_transformer/experiments/object_sorting/../../abstract_blocks.py:89\u001b[0m, in \u001b[0;36mAbstractEncoderBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_block \u001b[38;5;241m=\u001b[39m FeedForwardBlock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdff, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[1;32m     91\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_abstract_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:490\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:641\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks, frame_state)\u001b[0m\n\u001b[1;32m    639\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:133\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:389\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    378\u001b[0m signpost_event(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     },\n\u001b[1;32m    387\u001b[0m )\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:569\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    572\u001b[0m         Unsupported,\n\u001b[1;32m    573\u001b[0m         TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m         ValidationException,\n\u001b[1;32m    579\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    188\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    191\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:491\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m         out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m         orig_code_map[out_code] \u001b[38;5;241m=\u001b[39m code\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1028\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1025\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1026\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1028\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:458\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context):\n\u001b[0;32m--> 458\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (exc\u001b[38;5;241m.\u001b[39mRestartAnalysis, exc\u001b[38;5;241m.\u001b[39mSkipFrame):\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2069\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2069\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:719\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 719\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m     ):\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:683\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    680\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    682\u001b[0m     )\n\u001b[0;32m--> 683\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2157\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2152\u001b[0m _step_logger()(\n\u001b[1;32m   2153\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2155\u001b[0m )\n\u001b[1;32m   2156\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:833\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    830\u001b[0m     append_prefix_insts()\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 833\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m     graph_output_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:957\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    952\u001b[0m graph_tabular_log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lazy_format_graph_tabular(name, gm))\n\u001b[1;32m    953\u001b[0m graph_sizes_log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, LazyString(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_graph_sizes_log_str(name))\n\u001b[1;32m    955\u001b[0m )\n\u001b[0;32m--> 957\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    960\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    188\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    191\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1009\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1008\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1009\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/__init__.py:1568\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   1566\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1150\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1143\u001b[0m tracing_context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1144\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mTracingContext\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mTracingContext(fake_mode)\n\u001b[1;32m   1145\u001b[0m )\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m     tracing_context\n\u001b[1;32m   1149\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m-> 1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:55\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 55\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3891\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[1;32m   3875\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[1;32m   3876\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[1;32m   3877\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3887\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3888\u001b[0m )\n\u001b[1;32m   3890\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m-> 3891\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   3898\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   3900\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39mruntime_args):\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    188\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    191\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3429\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   3426\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[38;5;241m=\u001b[39mcompiler_fn)\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 3429\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n\u001b[1;32m   3432\u001b[0m     mutated_user_inp_locs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3433\u001b[0m         idx \u001b[38;5;241m-\u001b[39m aot_config\u001b[38;5;241m.\u001b[39mnum_params_buffers\n\u001b[1;32m   3434\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m fw_metadata\u001b[38;5;241m.\u001b[39mmutated_inp_indices\n\u001b[1;32m   3435\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m aot_config\u001b[38;5;241m.\u001b[39mnum_params_buffers\n\u001b[1;32m   3436\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2212\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[0;32m-> 2212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;66;03m# export path: ban duplicate inputs for now, add later if requested.\u001b[39;00m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2392\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;66;03m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[1;32m   2391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synthetic_base_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2394\u001b[0m \u001b[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2917\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m   2914\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mTracingContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m fw_metadata\n\u001b[1;32m   2916\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[0;32m-> 2917\u001b[0m         compiled_fw_func \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_flat_args\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[38;5;66;03m# NB: It's important to compile backwards ahead of time, as this may\u001b[39;00m\n\u001b[1;32m   2922\u001b[0m \u001b[38;5;66;03m# add extra guards which we need to apply to the Dynamo cache at\u001b[39;00m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;66;03m# forwards\u001b[39;00m\n\u001b[1;32m   2924\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m track_graph_compiling(aot_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    188\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    191\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1092\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;66;03m# We makes the following assumption\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;66;03m# For inference\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m#   len(orig_model_outputs) == len(model_outputs)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;66;03m# To make things safe, we'll use original_output_start_index field\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;66;03m# set by AOTAutograd to decide where the original module outputs start.\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1084\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[1;32m   1090\u001b[0m     }\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py:80\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/debug.py:228\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:54\u001b[0m, in \u001b[0;36mtime_and_log.<locals>.wrap.<locals>.newFunction\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(old_func)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewFunction\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:341\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt)\u001b[0m\n\u001b[1;32m    328\u001b[0m graph_args \u001b[38;5;241m=\u001b[39m [gm, example_inputs]\n\u001b[1;32m    329\u001b[0m graph_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m: cudagraphs,\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_fixed\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_fixed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout_opt\u001b[39m\u001b[38;5;124m\"\u001b[39m: layout_opt,\n\u001b[1;32m    339\u001b[0m }\n\u001b[0;32m--> 341\u001b[0m compiled_graph: CompiledFxGraph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    343\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_mode:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:565\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m             context\u001b[38;5;241m.\u001b[39moutput_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 565\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mdisable_cudagraphs:\n\u001b[1;32m    568\u001b[0m     BoxedBool\u001b[38;5;241m.\u001b[39mdisable(cudagraphs)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:970\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCache\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m, code, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    188\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    191\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:938\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;129m@dynamo_timed\u001b[39m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m--> 938\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m     linemap \u001b[38;5;241m=\u001b[39m [(line_no, node\u001b[38;5;241m.\u001b[39mstack_trace) \u001b[38;5;28;01mfor\u001b[39;00m line_no, node \u001b[38;5;129;01min\u001b[39;00m linemap]\n\u001b[1;32m    940\u001b[0m     key, path \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mwrite(code)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:913\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scheduler\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_wrapper_code()\n\u001b[0;32m--> 913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[43mScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# mypy can't figure this out\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mcodegen()\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    188\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    191\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:971\u001b[0m, in \u001b[0;36mScheduler.__init__\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m    969\u001b[0m }\n\u001b[0;32m--> 971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_scheduler_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# some new constants could have been created above\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names\u001b[38;5;241m.\u001b[39mupdate(V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:971\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m    969\u001b[0m }\n\u001b[0;32m--> 971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_scheduler_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# some new constants could have been created above\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names\u001b[38;5;241m.\u001b[39mupdate(V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:1038\u001b[0m, in \u001b[0;36mScheduler.create_scheduler_node\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer)):\n\u001b[1;32m   1037\u001b[0m     group_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_backend(node\u001b[38;5;241m.\u001b[39mget_device())\u001b[38;5;241m.\u001b[39mgroup_fn\n\u001b[0;32m-> 1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSchedulerNode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, ir\u001b[38;5;241m.\u001b[39mExternKernel):\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ExternKernelSchedulerNode(\u001b[38;5;28mself\u001b[39m, node)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:515\u001b[0m, in \u001b[0;36mSchedulerNode.__init__\u001b[0;34m(self, scheduler, node, group_fn)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, scheduler: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m, node: ir\u001b[38;5;241m.\u001b[39mComputedBuffer, group_fn):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(scheduler, node)\n\u001b[1;32m    512\u001b[0m     (\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes,\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body,\n\u001b[0;32m--> 515\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify_and_reorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m (node\u001b[38;5;241m.\u001b[39mget_device(), group_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes))\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_template():\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/ir.py:2574\u001b[0m, in \u001b[0;36mComputedBuffer.simplify_and_reorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2570\u001b[0m args, var_ranges \u001b[38;5;241m=\u001b[39m dependencies\u001b[38;5;241m.\u001b[39mindex_vars_squeeze(\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_size(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_reduction_size(), prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2572\u001b[0m )\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mobject(ConstantBuffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverride_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_device()):\n\u001b[0;32m-> 2574\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[43mLoopBody\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_store_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reduction_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2579\u001b[0m index_formulas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39mbody\u001b[38;5;241m.\u001b[39mindexing_exprs\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m   2580\u001b[0m reads_bufs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2581\u001b[0m     V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mname_to_buffer[reads_name]\n\u001b[1;32m   2582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reads_name \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mname_to_buffer\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m reads_name \u001b[38;5;129;01min\u001b[39;00m body\u001b[38;5;241m.\u001b[39mreads_name2expr\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   2585\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/ir.py:5285\u001b[0m, in \u001b[0;36mLoopBody.__init__\u001b[0;34m(self, fn, args, var_ranges)\u001b[0m\n\u001b[1;32m   5283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubblocks \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   5284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindirect_vars \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 5285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_block \u001b[38;5;241m=\u001b[39m \u001b[43mLoopBodyBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/ir.py:5480\u001b[0m, in \u001b[0;36mLoopBodyBlock.__init__\u001b[0;34m(self, body, fn, args)\u001b[0m\n\u001b[1;32m   5475\u001b[0m     handler \u001b[38;5;241m=\u001b[39m IndexPropagation(handler)\n\u001b[1;32m   5477\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_ops_handler(handler):\n\u001b[1;32m   5478\u001b[0m     \u001b[38;5;66;03m# This indirection is just a cute way to get IndexPropagation to\u001b[39;00m\n\u001b[1;32m   5479\u001b[0m     \u001b[38;5;66;03m# unwrap the return value.\u001b[39;00m\n\u001b[0;32m-> 5480\u001b[0m     \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m tracer\u001b[38;5;241m.\u001b[39mgraph\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/virtualized.py:232\u001b[0m, in \u001b[0;36mOpsWrapper.__getattr__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m new_args \u001b[38;5;241m=\u001b[39m [OpsWrapper\u001b[38;5;241m.\u001b[39m_unwrap(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m    231\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {k: OpsWrapper\u001b[38;5;241m.\u001b[39m_unwrap(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OpsWrapper\u001b[38;5;241m.\u001b[39m_wrap(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/index_propagation.py:215\u001b[0m, in \u001b[0;36mIndexPropagation.__getattr__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Any, IndexPropVar]:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(SymPyOps, name):\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     var_arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    218\u001b[0m         a\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(args, kwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, IndexPropVar)\n\u001b[1;32m    221\u001b[0m     ]\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(v\u001b[38;5;241m.\u001b[39mis_symbolic \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m var_arguments):\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/index_propagation.py:188\u001b[0m, in \u001b[0;36mIndexPropagation.fallback\u001b[0;34m(self, name, args, kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m new_args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munwrap(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m    187\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munwrap(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/_inductor/ir.py:5464\u001b[0m, in \u001b[0;36mLoopBodyBlock.__init__.<locals>.CaptureIndexing.output\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m   5462\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   5463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput\u001b[39m(result):\n\u001b[0;32m-> 5464\u001b[0m     \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/fx/proxy.py:189\u001b[0m, in \u001b[0;36mTracerBase.create_proxy\u001b[0;34m(self, kind, target, args, kwargs, name, type_expr, proxy_factory_fn)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs_, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m--> 189\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proxy_factory_fn:\n\u001b[1;32m    192\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy(node)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/fx/proxy.py:128\u001b[0m, in \u001b[0;36mTracerBase.create_node\u001b[0;34m(self, kind, target, args, kwargs, name, type_expr)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_mutable_operations:\n\u001b[1;32m    126\u001b[0m     check_for_mutable_operation(target, args, kwargs)\n\u001b[0;32m--> 128\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# TODO node_name_to_scope will be depreciated in favor of\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# node.meta['nn_module_stack']\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_name_to_scope[node\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope\u001b[38;5;241m.\u001b[39mmodule_path,\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope\u001b[38;5;241m.\u001b[39mmodule_type,\n\u001b[1;32m    134\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/fx/graph.py:844\u001b[0m, in \u001b[0;36mGraph.create_node\u001b[0;34m(self, op, target, args, kwargs, name, type_expr)\u001b[0m\n\u001b[1;32m    842\u001b[0m candidate \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_to_str(target)\n\u001b[1;32m    843\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_namespace\u001b[38;5;241m.\u001b[39mcreate_name(candidate, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 844\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43mNode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_namespace\u001b[38;5;241m.\u001b[39massociate_name_with_obj(name, n)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert(n)\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/fx/node.py:196\u001b[0m, in \u001b[0;36mNode.__init__\u001b[0;34m(self, graph, name, op, target, args, kwargs, return_type)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# being invoked, e.g add, layer1, or torch.add\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# All `Node`-valued inputs. Key is the Node, value is don't-care.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# The public API for this is `all_input_nodes`, this private attribute\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# should not be accessed directly.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_nodes : Dict[Node, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__update_args_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# All of the nodes that use the value produced by this Node\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Note one user may correspond to several uses, e.g. the node fo ``x + x``\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# would appear once here, but represents two uses.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Is a dict to act as an \"ordered set\". Keys are significant, value dont-care\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers : Dict[Node, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/torch/fx/node.py:408\u001b[0m, in \u001b[0;36mNode.__update_args_kwargs\u001b[0;34m(self, new_args, new_kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m map_arg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;28;01mlambda\u001b[39;00m n: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_nodes\u001b[38;5;241m.\u001b[39msetdefault(n))\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_use \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_nodes\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 408\u001b[0m     new_use\u001b[38;5;241m.\u001b[39musers\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 99.84%\n",
      "full sequence accuracy: 99.22%\n",
      "teacher-forcing accuracy:  99.92%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'elementwise_accuracy': 0.9984375238418579,\n",
       " 'full_sequence_accuracy': 0.9921875,\n",
       " 'teacher_forcing_accuracy': 0.999218761920929,\n",
       " 'acc_by_position': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9921875,\n",
       "  0.9921875,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq2seq_model(model, source_test, target_test, labels_test, start_token, print_=True, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../..']\n",
    "from seq2seq_models import Seq2SeqAbstractTransformer, configure_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                           Param #              Param %              Trainable\n",
       "=======================================================================================================================================\n",
       "Seq2SeqAbstractTransformer (Seq2SeqAbstractTransformer)                     --                    -0.32%              True\n",
       "├─PositionalSymbolRetriever (symbol_retriever): 1-1                         --                        --              True\n",
       "│    └─Embedding (symbol_library): 2-1                                      640                    0.32%              True\n",
       "├─ModuleDict (layers): 1-2                                                  --                    -0.32%              True\n",
       "│    └─Linear (source_embedder): 2-2                                        576                    0.29%              True\n",
       "│    └─Embedding (target_embedder): 2-3                                     704                    0.36%              True\n",
       "│    └─SinusoidalPositionalEncoding (source_pos_embedder): 2-4              --                        --              --\n",
       "│    │    └─Dropout (dropout): 3-1                                          --                        --              --\n",
       "│    └─SinusoidalPositionalEncoding (target_pos_embedder): 2-5              --                        --              --\n",
       "│    │    └─Dropout (dropout): 3-2                                          --                        --              --\n",
       "│    └─ModuleList (encoder_blocks): 2-6                                     --                    -0.32%              True\n",
       "│    │    └─AbstractEncoderBlock (0): 3-3                                   46,592                23.59%              True\n",
       "│    │    └─AbstractEncoderBlock (1): 3-4                                   46,592                23.59%              True\n",
       "│    └─ModuleList (decoder_blocks): 2-7                                     --                    -0.32%              True\n",
       "│    │    └─AbstractDecoderBlock (0): 3-5                                   50,880                25.76%              True\n",
       "│    │    └─AbstractDecoderBlock (1): 3-6                                   50,880                25.76%              True\n",
       "│    └─Linear (final_out): 2-8                                              650                    0.33%              True\n",
       "=======================================================================================================================================\n",
       "Total params: 197,514\n",
       "Trainable params: 197,514\n",
       "Non-trainable params: 0\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = dict(\n",
    "    input_spec=dict(type='vector', dim=8), output_spec=dict(type='token', vocab_size=10+1),\n",
    "    symbol_retrieval='positional_symbols', symbol_retrieval_kwargs=dict(symbol_dim=64, max_symbols=10),\n",
    "    d_model=64, out_dim=10, n_layers_enc=2, n_layers_dec=2,\n",
    "    encoder_kwargs=dict(n_heads_enc=2, n_heads_abs=2, dff=128, activation='relu', norm_first=False, dropout_rate=0.1, causal=False, rel_mask_diag=False),\n",
    "    decoder_kwargs=dict(n_heads_enc=2, n_heads_abs=0, n_heads_cross=2, dff=128, activation='relu', norm_first=False, dropout_rate=0.1, causal=True, rel_mask_diag=False),\n",
    "    in_block_size=10, out_block_size=10)\n",
    "seq2seqabstransformer = Seq2SeqAbstractTransformer(**model_args)#.to(device)\n",
    "torchinfo.summary(seq2seqabstransformer, row_settings=[\"depth\", \"var_names\"], col_names=[\"num_params\", \"params_percent\", \"trainable\"], depth=3, col_width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seqabstransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitSeq2SeqModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import RichProgressBar, TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/awni/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | Seq2SeqAbstractTransformer | 194 K \n",
      "-----------------------------------------------------\n",
      "194 K     Trainable params\n",
      "0         Non-trainable params\n",
      "194 K     Total params\n",
      "0.780     Total estimated model params size (MB)\n",
      "/home/awni/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c439322f4aca43e8a351c6e489ffd598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=500, enable_checkpointing=False, logger=False, enable_model_summary=True, precision='64-true',\n",
    "    # callbacks=[RichProgressBar()]\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dl)#, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6906f05f034b488989c112fb7e49459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      elementwise_acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9724309016496707     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          seq_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.878155048076923     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    teacher_forcing_acc    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9870042174290388     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.044189517713503185    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     elementwise_acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9724309016496707    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         seq_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.878155048076923    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   teacher_forcing_acc   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9870042174290388    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.044189517713503185   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.044189517713503185,\n",
       "  'teacher_forcing_acc': 0.9870042174290388,\n",
       "  'elementwise_acc': 0.9724309016496707,\n",
       "  'seq_acc': 0.878155048076923}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lit_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-7)\n",
    "# optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)\n",
    "\n",
    "train_kwargs = dict(\n",
    "    model=model, train_dl=train_dl, eval_model=eval_model, n_epochs=500,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr,\n",
    "    compile=True, grad_clip=0,\n",
    "    eval_main_metric='val/loss',\n",
    "    always_save_checkpoint=always_save_checkpoint, ckpt_dict=dict(model_args=model_args), out_dir=out_dir,\n",
    "    wandb_log=False, wandb_init_kwargs=dict(project=wandb_project, name='AbstractTransformer'), track_mfu=True,\n",
    "    ddp=False, device_type='cuda')\n",
    "\n",
    "train_model(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test, target_test, labels_test = next(iter(test_dl))\n",
    "evaluate_seq2seq_model(model, source_test, target_test, labels_test, start_token, print_=True, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Abstractor Architecture (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path += ['..', '../..']\n",
    "from og_seq2seq_models import Seq2SeqAbstractorArchb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                                Param #              Param %              Trainable\n",
       "============================================================================================================================================\n",
       "Seq2SeqAbstractorArchb (Seq2SeqAbstractorArchb)                                  --                        --              True\n",
       "├─PositionalSymbolRetriever (symbol_retriever): 1-1                              --                        --              True\n",
       "│    └─Embedding (symbol_library): 2-1                                           640                    0.26%              True\n",
       "├─ModuleDict (layers): 1-2                                                       --                        --              True\n",
       "│    └─Linear (source_embedder): 2-2                                             576                    0.24%              True\n",
       "│    └─Embedding (target_embedder): 2-3                                          704                    0.29%              True\n",
       "│    └─SinusoidalPositionalEncoding (source_pos_embedder): 2-4                   --                        --              --\n",
       "│    │    └─Dropout (dropout): 3-1                                               --                        --              --\n",
       "│    └─SinusoidalPositionalEncoding (target_pos_embedder): 2-5                   --                        --              --\n",
       "│    │    └─Dropout (dropout): 3-2                                               --                        --              --\n",
       "│    └─ModuleList (encoder_blocks): 2-6                                          --                        --              True\n",
       "│    │    └─EncoderBlock (0): 3-3                                                33,472                13.81%              True\n",
       "│    │    └─EncoderBlock (1): 3-4                                                33,472                13.81%              True\n",
       "│    └─AbstractorModule (abstractor): 2-7                                        --                        --              True\n",
       "│    │    └─SymbolicAttention (symbol_retriever): 3-5                            5,440                  2.24%              True\n",
       "│    │    └─ModuleList (abstractor_layers): 3-6                                  66,944                27.62%              True\n",
       "│    └─ModuleList (decoder_blocks): 2-8                                          --                        --              True\n",
       "│    │    └─DecoderBlock (0): 3-7                                                50,240                20.73%              True\n",
       "│    │    └─DecoderBlock (1): 3-8                                                50,240                20.73%              True\n",
       "│    └─Linear (final_out): 2-9                                                   650                    0.27%              True\n",
       "============================================================================================================================================\n",
       "Total params: 242,378\n",
       "Trainable params: 242,378\n",
       "Non-trainable params: 0\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = dict(\n",
    "    input_spec=dict(type='vector', dim=8), output_spec=dict(type='token', vocab_size=10+1),\n",
    "    symbol_retrieval='positional_symbols', symbol_retrieval_kwargs=dict(symbol_dim=64, max_symbols=10),\n",
    "    d_model=64, out_dim=10, n_layers_enc=2, n_layers_dec=2,\n",
    "    encoder_kwargs=dict(n_heads=2, dff=128, activation='relu', norm_first=False, dropout_rate=0.1, causal=False),\n",
    "    abstractor_kwargs=dict(\n",
    "        n_layers=2, d_model=64, n_heads=2, dff=128, activation='relu', norm_first=False, dropout_rate=0.1, symbol_retriever_type='symbolic_attention', symbol_add_pos_embedding=False, symbol_retriever_kwargs=dict(model_dim=64, n_heads=2, num_symbols=10), max_len=10),\n",
    "    decoder_kwargs=dict(n_heads=2, dff=128, activation='relu', norm_first=True, dropout_rate=0.1, causal=True),\n",
    "    in_block_size=10, out_block_size=10)\n",
    "model = Seq2SeqAbstractorArchb(**model_args)#.to(device)\n",
    "torchinfo.summary(model, row_settings=[\"depth\", \"var_names\"], col_names=[\"num_params\", \"params_percent\", \"trainable\"], depth=3, col_width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitSeq2SeqModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/awni/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | Seq2SeqAbstractorArchb | 242 K \n",
      "-------------------------------------------------\n",
      "242 K     Trainable params\n",
      "0         Non-trainable params\n",
      "242 K     Total params\n",
      "0.970     Total estimated model params size (MB)\n",
      "/home/awni/miniconda3/envs/abstract_transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa4fe9eced945e59c09c06de97601f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=500, enable_checkpointing=False, logger=False, enable_model_summary=True, precision='64-true',\n",
    "    # callbacks=[RichProgressBar()]\n",
    "    )\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dl)#, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtest(lit_model, test_dl)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.test(lit_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-7)\n",
    "# optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = dict(\n",
    "    model=model, train_dl=train_dl, eval_model=eval_model, n_epochs=500,\n",
    "    optimizer=optimizer, scaler=scaler, get_lr=get_lr,\n",
    "    compile=True, grad_clip=0,\n",
    "    eval_main_metric='val/loss',\n",
    "    always_save_checkpoint=always_save_checkpoint, ckpt_dict=dict(model_args=model_args), out_dir=out_dir,\n",
    "    wandb_log=False, wandb_init_kwargs=dict(project=wandb_project, name='Abstractor (arch. b)'), track_mfu=True,\n",
    "    ddp=False, device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 6 train/loss: 2.2730, train/tfacc: 0.1290, val/loss: 2.2817, val/tfacc: 0.1252, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.2860, time 50486.99ms, mfu -100.00%\n",
      "epoch: 1, step: 12 train/loss: 2.2274, train/tfacc: 0.1634, val/loss: 2.2376, val/tfacc: 0.1484, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 2.2389, time 1289.85ms, mfu -100.00%\n",
      "epoch: 2, step: 18 train/loss: 2.1491, train/tfacc: 0.2076, val/loss: 2.1670, val/tfacc: 0.1924, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 2.1712, time 1438.48ms, mfu -100.00%\n",
      "epoch: 3, step: 24 train/loss: 2.0488, train/tfacc: 0.2443, val/loss: 2.0739, val/tfacc: 0.2266, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 2.0807, time 1551.41ms, mfu -100.00%\n",
      "epoch: 4, step: 30 train/loss: 1.9562, train/tfacc: 0.2705, val/loss: 1.9832, val/tfacc: 0.2520, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 1.9946, time 1417.13ms, mfu -100.00%\n",
      "epoch: 5, step: 36 train/loss: 1.8766, train/tfacc: 0.2861, val/loss: 1.9056, val/tfacc: 0.2671, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 1.9307, time 1260.03ms, mfu -100.00%\n",
      "epoch: 6, step: 42 train/loss: 1.8145, train/tfacc: 0.3031, val/loss: 1.8486, val/tfacc: 0.2763, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 1.8660, time 1381.27ms, mfu -100.00%\n",
      "epoch: 7, step: 48 train/loss: 1.7711, train/tfacc: 0.3177, val/loss: 1.8142, val/tfacc: 0.2847, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 1.8098, time 1253.48ms, mfu -100.00%\n",
      "epoch: 8, step: 54 train/loss: 1.7223, train/tfacc: 0.3319, val/loss: 1.7725, val/tfacc: 0.2964, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 1.7617, time 1297.96ms, mfu -100.00%\n",
      "epoch: 9, step: 60 train/loss: 1.6724, train/tfacc: 0.3516, val/loss: 1.7316, val/tfacc: 0.3055, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 1.7367, time 1182.19ms, mfu -100.00%\n",
      "epoch: 10, step: 66 train/loss: 1.6158, train/tfacc: 0.3702, val/loss: 1.6878, val/tfacc: 0.3179, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 10: loss 1.7067, time 1273.47ms, mfu -100.00%\n",
      "epoch: 11, step: 72 train/loss: 1.5770, train/tfacc: 0.3855, val/loss: 1.6591, val/tfacc: 0.3300, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 11: loss 1.6592, time 1189.19ms, mfu -100.00%\n",
      "epoch: 12, step: 78 train/loss: 1.5371, train/tfacc: 0.4052, val/loss: 1.6251, val/tfacc: 0.3418, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 12: loss 1.5963, time 1233.71ms, mfu -100.00%\n",
      "epoch: 13, step: 84 train/loss: 1.5027, train/tfacc: 0.4216, val/loss: 1.5977, val/tfacc: 0.3562, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 13: loss 1.5823, time 1343.16ms, mfu -100.00%\n",
      "epoch: 14, step: 90 train/loss: 1.4538, train/tfacc: 0.4402, val/loss: 1.5588, val/tfacc: 0.3755, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 14: loss 1.5742, time 1347.29ms, mfu -100.00%\n",
      "epoch: 15, step: 96 train/loss: 1.4091, train/tfacc: 0.4586, val/loss: 1.5252, val/tfacc: 0.3888, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 15: loss 1.5131, time 1223.85ms, mfu -100.00%\n",
      "epoch: 16, step: 102 train/loss: 1.3532, train/tfacc: 0.4830, val/loss: 1.4810, val/tfacc: 0.4033, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 16: loss 1.4755, time 1154.93ms, mfu -100.00%\n",
      "epoch: 17, step: 108 train/loss: 1.3212, train/tfacc: 0.5043, val/loss: 1.4593, val/tfacc: 0.4147, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 17: loss 1.4479, time 1275.72ms, mfu -100.00%\n",
      "epoch: 18, step: 114 train/loss: 1.2661, train/tfacc: 0.5256, val/loss: 1.4178, val/tfacc: 0.4277, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 18: loss 1.3916, time 1240.09ms, mfu -100.00%\n",
      "epoch: 19, step: 120 train/loss: 1.2162, train/tfacc: 0.5456, val/loss: 1.3729, val/tfacc: 0.4471, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 19: loss 1.3446, time 1187.88ms, mfu -100.00%\n",
      "epoch: 20, step: 126 train/loss: 1.1733, train/tfacc: 0.5540, val/loss: 1.3386, val/tfacc: 0.4623, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 20: loss 1.3132, time 1178.68ms, mfu -100.00%\n",
      "epoch: 21, step: 132 train/loss: 1.1143, train/tfacc: 0.5706, val/loss: 1.2889, val/tfacc: 0.4844, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 21: loss 1.2817, time 1162.74ms, mfu -100.00%\n",
      "epoch: 22, step: 138 train/loss: 1.0350, train/tfacc: 0.6058, val/loss: 1.2147, val/tfacc: 0.5142, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 22: loss 1.2386, time 1158.25ms, mfu -100.00%\n",
      "epoch: 23, step: 144 train/loss: 0.9574, train/tfacc: 0.6437, val/loss: 1.1381, val/tfacc: 0.5430, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 23: loss 1.1626, time 1287.39ms, mfu -100.00%\n",
      "epoch: 24, step: 150 train/loss: 0.9134, train/tfacc: 0.6585, val/loss: 1.0806, val/tfacc: 0.5668, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 24: loss 1.1320, time 1273.11ms, mfu -100.00%\n",
      "epoch: 25, step: 156 train/loss: 0.8325, train/tfacc: 0.6969, val/loss: 0.9997, val/tfacc: 0.6022, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 25: loss 1.0787, time 1137.11ms, mfu -100.00%\n",
      "epoch: 26, step: 162 train/loss: 0.8110, train/tfacc: 0.7100, val/loss: 0.9671, val/tfacc: 0.6201, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 26: loss 1.0879, time 1210.01ms, mfu -100.00%\n",
      "epoch: 27, step: 168 train/loss: 0.7528, train/tfacc: 0.7267, val/loss: 0.9161, val/tfacc: 0.6374, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 27: loss 0.9896, time 1225.19ms, mfu -100.00%\n",
      "epoch: 28, step: 174 train/loss: 0.6933, train/tfacc: 0.7499, val/loss: 0.8472, val/tfacc: 0.6638, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 28: loss 0.9943, time 1142.80ms, mfu -100.00%\n",
      "epoch: 29, step: 180 train/loss: 0.6465, train/tfacc: 0.7782, val/loss: 0.7982, val/tfacc: 0.6877, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 29: loss 0.9557, time 1242.89ms, mfu -100.00%\n",
      "epoch: 30, step: 186 train/loss: 0.6186, train/tfacc: 0.7811, val/loss: 0.7709, val/tfacc: 0.6948, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 30: loss 0.9264, time 1293.68ms, mfu -100.00%\n",
      "epoch: 31, step: 192 train/loss: 0.5938, train/tfacc: 0.7912, val/loss: 0.7395, val/tfacc: 0.7038, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 31: loss 0.8903, time 1446.61ms, mfu -100.00%\n",
      "epoch: 32, step: 198 train/loss: 0.5635, train/tfacc: 0.8065, val/loss: 0.7166, val/tfacc: 0.7153, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 32: loss 0.8670, time 1174.41ms, mfu -100.00%\n",
      "epoch: 33, step: 204 train/loss: 0.5271, train/tfacc: 0.8251, val/loss: 0.6697, val/tfacc: 0.7382, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 33: loss 0.8126, time 1298.30ms, mfu -100.00%\n",
      "epoch: 34, step: 210 train/loss: 0.5159, train/tfacc: 0.8206, val/loss: 0.6573, val/tfacc: 0.7381, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 34: loss 0.7869, time 1315.09ms, mfu -100.00%\n",
      "epoch: 35, step: 216 train/loss: 0.5111, train/tfacc: 0.8177, val/loss: 0.6545, val/tfacc: 0.7341, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 35: loss 0.7818, time 1176.03ms, mfu -100.00%\n",
      "epoch: 36, step: 222 train/loss: 0.4440, train/tfacc: 0.8593, val/loss: 0.5883, val/tfacc: 0.7722, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 36: loss 0.7264, time 1338.79ms, mfu -100.00%\n",
      "epoch: 37, step: 228 train/loss: 0.4509, train/tfacc: 0.8527, val/loss: 0.5891, val/tfacc: 0.7713, 🤖\n",
      "epoch 37: loss 0.7468, time 1262.59ms, mfu -100.00%\n",
      "epoch: 38, step: 234 train/loss: 0.4470, train/tfacc: 0.8489, val/loss: 0.5933, val/tfacc: 0.7625, 🤖\n",
      "epoch 38: loss 0.7551, time 1222.08ms, mfu -100.00%\n",
      "epoch: 39, step: 240 train/loss: 0.4403, train/tfacc: 0.8522, val/loss: 0.5833, val/tfacc: 0.7673, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 39: loss 0.7278, time 1319.37ms, mfu -100.00%\n",
      "epoch: 40, step: 246 train/loss: 0.4170, train/tfacc: 0.8585, val/loss: 0.5585, val/tfacc: 0.7765, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 40: loss 0.6990, time 1274.11ms, mfu -100.00%\n",
      "epoch: 41, step: 252 train/loss: 0.3761, train/tfacc: 0.8844, val/loss: 0.5271, val/tfacc: 0.7911, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 41: loss 0.6789, time 1364.12ms, mfu -100.00%\n",
      "epoch: 42, step: 258 train/loss: 0.3776, train/tfacc: 0.8780, val/loss: 0.5229, val/tfacc: 0.7926, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 42: loss 0.6443, time 1227.26ms, mfu -100.00%\n",
      "epoch: 43, step: 264 train/loss: 0.3727, train/tfacc: 0.8722, val/loss: 0.5202, val/tfacc: 0.7922, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 43: loss 0.6736, time 1288.43ms, mfu -100.00%\n",
      "epoch: 44, step: 270 train/loss: 0.3615, train/tfacc: 0.8871, val/loss: 0.5078, val/tfacc: 0.7976, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 44: loss 0.6334, time 1313.46ms, mfu -100.00%\n",
      "epoch: 45, step: 276 train/loss: 0.3597, train/tfacc: 0.8833, val/loss: 0.5018, val/tfacc: 0.7999, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 45: loss 0.6080, time 1280.88ms, mfu -100.00%\n",
      "epoch: 46, step: 282 train/loss: 0.3546, train/tfacc: 0.8845, val/loss: 0.4961, val/tfacc: 0.8007, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 46: loss 0.6352, time 1318.97ms, mfu -100.00%\n",
      "epoch: 47, step: 288 train/loss: 0.3443, train/tfacc: 0.8825, val/loss: 0.4896, val/tfacc: 0.8014, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 47: loss 0.6057, time 1294.66ms, mfu -100.00%\n",
      "epoch: 48, step: 294 train/loss: 0.3600, train/tfacc: 0.8782, val/loss: 0.5059, val/tfacc: 0.7957, 🤖\n",
      "epoch 48: loss 0.6113, time 1206.44ms, mfu -100.00%\n",
      "epoch: 49, step: 300 train/loss: 0.3221, train/tfacc: 0.8995, val/loss: 0.4730, val/tfacc: 0.8105, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 49: loss 0.5987, time 1218.39ms, mfu -100.00%\n",
      "epoch: 50, step: 306 train/loss: 0.3206, train/tfacc: 0.8909, val/loss: 0.4608, val/tfacc: 0.8156, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 50: loss 0.5817, time 1193.63ms, mfu -100.00%\n",
      "epoch: 51, step: 312 train/loss: 0.2866, train/tfacc: 0.9240, val/loss: 0.4322, val/tfacc: 0.8318, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 51: loss 0.6266, time 1238.77ms, mfu -100.00%\n",
      "epoch: 52, step: 318 train/loss: 0.2977, train/tfacc: 0.9122, val/loss: 0.4403, val/tfacc: 0.8249, 🤖\n",
      "epoch 52: loss 0.5477, time 1138.19ms, mfu -100.00%\n",
      "epoch: 53, step: 324 train/loss: 0.2902, train/tfacc: 0.9111, val/loss: 0.4350, val/tfacc: 0.8257, 🤖\n",
      "epoch 53: loss 0.5646, time 1264.19ms, mfu -100.00%\n",
      "epoch: 54, step: 330 train/loss: 0.2889, train/tfacc: 0.9125, val/loss: 0.4344, val/tfacc: 0.8273, 🤖\n",
      "epoch 54: loss 0.5631, time 1121.13ms, mfu -100.00%\n",
      "epoch: 55, step: 336 train/loss: 0.2637, train/tfacc: 0.9304, val/loss: 0.4085, val/tfacc: 0.8407, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 55: loss 0.5430, time 1235.82ms, mfu -100.00%\n",
      "epoch: 56, step: 342 train/loss: 0.2497, train/tfacc: 0.9325, val/loss: 0.3971, val/tfacc: 0.8443, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 56: loss 0.5473, time 1272.07ms, mfu -100.00%\n",
      "epoch: 57, step: 348 train/loss: 0.2520, train/tfacc: 0.9368, val/loss: 0.3942, val/tfacc: 0.8471, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 57: loss 0.4985, time 1297.56ms, mfu -100.00%\n",
      "epoch: 58, step: 354 train/loss: 0.2616, train/tfacc: 0.9165, val/loss: 0.4098, val/tfacc: 0.8338, 🤖\n",
      "epoch 58: loss 0.5307, time 1207.76ms, mfu -100.00%\n",
      "epoch: 59, step: 360 train/loss: 0.2432, train/tfacc: 0.9374, val/loss: 0.3868, val/tfacc: 0.8472, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 59: loss 0.5037, time 1205.03ms, mfu -100.00%\n",
      "epoch: 60, step: 366 train/loss: 0.2440, train/tfacc: 0.9261, val/loss: 0.3912, val/tfacc: 0.8437, 🤖\n",
      "epoch 60: loss 0.4788, time 1166.65ms, mfu -100.00%\n",
      "epoch: 61, step: 372 train/loss: 0.2182, train/tfacc: 0.9476, val/loss: 0.3696, val/tfacc: 0.8548, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 61: loss 0.4836, time 1167.64ms, mfu -100.00%\n",
      "epoch: 62, step: 378 train/loss: 0.2099, train/tfacc: 0.9521, val/loss: 0.3527, val/tfacc: 0.8649, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 62: loss 0.4815, time 1258.86ms, mfu -100.00%\n",
      "epoch: 63, step: 384 train/loss: 0.2070, train/tfacc: 0.9535, val/loss: 0.3545, val/tfacc: 0.8624, 🤖\n",
      "epoch 63: loss 0.4844, time 1089.87ms, mfu -100.00%\n",
      "epoch: 64, step: 390 train/loss: 0.2011, train/tfacc: 0.9549, val/loss: 0.3433, val/tfacc: 0.8704, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 64: loss 0.4604, time 1338.22ms, mfu -100.00%\n",
      "epoch: 65, step: 396 train/loss: 0.1948, train/tfacc: 0.9565, val/loss: 0.3413, val/tfacc: 0.8690, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 65: loss 0.4795, time 1322.31ms, mfu -100.00%\n",
      "epoch: 66, step: 402 train/loss: 0.1946, train/tfacc: 0.9558, val/loss: 0.3428, val/tfacc: 0.8664, 🤖\n",
      "epoch 66: loss 0.4737, time 1173.13ms, mfu -100.00%\n",
      "epoch: 67, step: 408 train/loss: 0.1939, train/tfacc: 0.9546, val/loss: 0.3495, val/tfacc: 0.8638, 🤖\n",
      "epoch 67: loss 0.4835, time 1210.13ms, mfu -100.00%\n",
      "epoch: 68, step: 414 train/loss: 0.2062, train/tfacc: 0.9495, val/loss: 0.3650, val/tfacc: 0.8548, 🤖\n",
      "epoch 68: loss 0.4962, time 1132.88ms, mfu -100.00%\n",
      "epoch: 69, step: 420 train/loss: 0.2161, train/tfacc: 0.9348, val/loss: 0.3858, val/tfacc: 0.8410, 🤖\n",
      "epoch 69: loss 0.4676, time 1170.27ms, mfu -100.00%\n",
      "epoch: 70, step: 426 train/loss: 0.1919, train/tfacc: 0.9511, val/loss: 0.3505, val/tfacc: 0.8594, 🤖\n",
      "epoch 70: loss 0.4434, time 1101.61ms, mfu -100.00%\n",
      "epoch: 71, step: 432 train/loss: 0.1796, train/tfacc: 0.9568, val/loss: 0.3355, val/tfacc: 0.8665, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 71: loss 0.4790, time 1313.43ms, mfu -100.00%\n",
      "epoch: 72, step: 438 train/loss: 0.2029, train/tfacc: 0.9463, val/loss: 0.3583, val/tfacc: 0.8556, 🤖\n",
      "epoch 72: loss 0.4804, time 1104.35ms, mfu -100.00%\n",
      "epoch: 73, step: 444 train/loss: 0.1954, train/tfacc: 0.9456, val/loss: 0.3506, val/tfacc: 0.8573, 🤖\n",
      "epoch 73: loss 0.4664, time 1091.79ms, mfu -100.00%\n",
      "epoch: 74, step: 450 train/loss: 0.2020, train/tfacc: 0.9402, val/loss: 0.3628, val/tfacc: 0.8517, 🤖\n",
      "epoch 74: loss 0.4335, time 1149.36ms, mfu -100.00%\n",
      "epoch: 75, step: 456 train/loss: 0.1821, train/tfacc: 0.9559, val/loss: 0.3396, val/tfacc: 0.8636, 🤖\n",
      "epoch 75: loss 0.4756, time 1199.42ms, mfu -100.00%\n",
      "epoch: 76, step: 462 train/loss: 0.1911, train/tfacc: 0.9450, val/loss: 0.3608, val/tfacc: 0.8521, 🤖\n",
      "epoch 76: loss 0.4460, time 1199.58ms, mfu -100.00%\n",
      "epoch: 77, step: 468 train/loss: 0.1734, train/tfacc: 0.9583, val/loss: 0.3344, val/tfacc: 0.8661, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 77: loss 0.4348, time 1294.84ms, mfu -100.00%\n",
      "epoch: 78, step: 474 train/loss: 0.1738, train/tfacc: 0.9585, val/loss: 0.3320, val/tfacc: 0.8688, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 78: loss 0.4039, time 1284.96ms, mfu -100.00%\n",
      "epoch: 79, step: 480 train/loss: 0.1774, train/tfacc: 0.9567, val/loss: 0.3375, val/tfacc: 0.8628, 🤖\n",
      "epoch 79: loss 0.4077, time 1251.97ms, mfu -100.00%\n",
      "epoch: 80, step: 486 train/loss: 0.1906, train/tfacc: 0.9414, val/loss: 0.3597, val/tfacc: 0.8513, 🤖\n",
      "epoch 80: loss 0.3984, time 1245.89ms, mfu -100.00%\n",
      "epoch: 81, step: 492 train/loss: 0.1682, train/tfacc: 0.9607, val/loss: 0.3291, val/tfacc: 0.8668, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 81: loss 0.4001, time 1277.62ms, mfu -100.00%\n",
      "epoch: 82, step: 498 train/loss: 0.1718, train/tfacc: 0.9541, val/loss: 0.3381, val/tfacc: 0.8628, 🤖\n",
      "epoch 82: loss 0.4237, time 1148.50ms, mfu -100.00%\n",
      "epoch: 83, step: 504 train/loss: 0.1627, train/tfacc: 0.9572, val/loss: 0.3241, val/tfacc: 0.8700, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 83: loss 0.3969, time 1187.89ms, mfu -100.00%\n",
      "epoch: 84, step: 510 train/loss: 0.1651, train/tfacc: 0.9592, val/loss: 0.3263, val/tfacc: 0.8684, 🤖\n",
      "epoch 84: loss 0.3883, time 1108.60ms, mfu -100.00%\n",
      "epoch: 85, step: 516 train/loss: 0.1548, train/tfacc: 0.9625, val/loss: 0.3170, val/tfacc: 0.8728, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 85: loss 0.3816, time 1249.88ms, mfu -100.00%\n",
      "epoch: 86, step: 522 train/loss: 0.1476, train/tfacc: 0.9678, val/loss: 0.3146, val/tfacc: 0.8733, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 86: loss 0.3742, time 1141.70ms, mfu -100.00%\n",
      "epoch: 87, step: 528 train/loss: 0.1388, train/tfacc: 0.9736, val/loss: 0.3045, val/tfacc: 0.8798, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 87: loss 0.3791, time 1174.05ms, mfu -100.00%\n",
      "epoch: 88, step: 534 train/loss: 0.1383, train/tfacc: 0.9709, val/loss: 0.3040, val/tfacc: 0.8787, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 88: loss 0.3785, time 1199.44ms, mfu -100.00%\n",
      "epoch: 89, step: 540 train/loss: 0.1416, train/tfacc: 0.9691, val/loss: 0.3062, val/tfacc: 0.8755, 🤖\n",
      "epoch 89: loss 0.4127, time 1150.19ms, mfu -100.00%\n",
      "epoch: 90, step: 546 train/loss: 0.1459, train/tfacc: 0.9645, val/loss: 0.3075, val/tfacc: 0.8752, 🤖\n",
      "epoch 90: loss 0.4022, time 1178.79ms, mfu -100.00%\n",
      "epoch: 91, step: 552 train/loss: 0.1578, train/tfacc: 0.9564, val/loss: 0.3228, val/tfacc: 0.8681, 🤖\n",
      "epoch 91: loss 0.3841, time 1161.57ms, mfu -100.00%\n",
      "epoch: 92, step: 558 train/loss: 0.1421, train/tfacc: 0.9681, val/loss: 0.3103, val/tfacc: 0.8756, 🤖\n",
      "epoch 92: loss 0.4018, time 1167.66ms, mfu -100.00%\n",
      "epoch: 93, step: 564 train/loss: 0.1428, train/tfacc: 0.9660, val/loss: 0.3159, val/tfacc: 0.8720, 🤖\n",
      "epoch 93: loss 0.3985, time 1202.47ms, mfu -100.00%\n",
      "epoch: 94, step: 570 train/loss: 0.1403, train/tfacc: 0.9672, val/loss: 0.3143, val/tfacc: 0.8718, 🤖\n",
      "epoch 94: loss 0.3926, time 1065.88ms, mfu -100.00%\n",
      "epoch: 95, step: 576 train/loss: 0.1377, train/tfacc: 0.9677, val/loss: 0.3104, val/tfacc: 0.8734, 🤖\n",
      "epoch 95: loss 0.3428, time 1163.94ms, mfu -100.00%\n",
      "epoch: 96, step: 582 train/loss: 0.1271, train/tfacc: 0.9736, val/loss: 0.2965, val/tfacc: 0.8811, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 96: loss 0.3522, time 1203.26ms, mfu -100.00%\n",
      "epoch: 97, step: 588 train/loss: 0.1288, train/tfacc: 0.9708, val/loss: 0.2954, val/tfacc: 0.8827, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 97: loss 0.3503, time 1132.81ms, mfu -100.00%\n",
      "epoch: 98, step: 594 train/loss: 0.1284, train/tfacc: 0.9716, val/loss: 0.2986, val/tfacc: 0.8808, 🤖\n",
      "epoch 98: loss 0.3884, time 1157.40ms, mfu -100.00%\n",
      "epoch: 99, step: 600 train/loss: 0.1291, train/tfacc: 0.9738, val/loss: 0.2950, val/tfacc: 0.8811, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 99: loss 0.3577, time 1219.26ms, mfu -100.00%\n",
      "epoch: 100, step: 606 train/loss: 0.1353, train/tfacc: 0.9691, val/loss: 0.3094, val/tfacc: 0.8732, 🤖\n",
      "epoch 100: loss 0.3311, time 1184.34ms, mfu -100.00%\n",
      "epoch: 101, step: 612 train/loss: 0.1211, train/tfacc: 0.9758, val/loss: 0.2923, val/tfacc: 0.8827, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 101: loss 0.3487, time 1192.74ms, mfu -100.00%\n",
      "epoch: 102, step: 618 train/loss: 0.1215, train/tfacc: 0.9740, val/loss: 0.2926, val/tfacc: 0.8822, 🤖\n",
      "epoch 102: loss 0.3528, time 1099.23ms, mfu -100.00%\n",
      "epoch: 103, step: 624 train/loss: 0.1300, train/tfacc: 0.9693, val/loss: 0.2976, val/tfacc: 0.8788, 🤖\n",
      "epoch 103: loss 0.3614, time 1054.95ms, mfu -100.00%\n",
      "epoch: 104, step: 630 train/loss: 0.1145, train/tfacc: 0.9793, val/loss: 0.2834, val/tfacc: 0.8862, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 104: loss 0.3316, time 1237.94ms, mfu -100.00%\n",
      "epoch: 105, step: 636 train/loss: 0.1136, train/tfacc: 0.9766, val/loss: 0.2851, val/tfacc: 0.8840, 🤖\n",
      "epoch 105: loss 0.3354, time 1134.58ms, mfu -100.00%\n",
      "epoch: 106, step: 642 train/loss: 0.1027, train/tfacc: 0.9831, val/loss: 0.2676, val/tfacc: 0.8929, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 106: loss 0.3568, time 1249.37ms, mfu -100.00%\n",
      "epoch: 107, step: 648 train/loss: 0.1154, train/tfacc: 0.9745, val/loss: 0.2873, val/tfacc: 0.8848, 🤖\n",
      "epoch 107: loss 0.3598, time 1268.67ms, mfu -100.00%\n",
      "epoch: 108, step: 654 train/loss: 0.1114, train/tfacc: 0.9784, val/loss: 0.2841, val/tfacc: 0.8851, 🤖\n",
      "epoch 108: loss 0.3190, time 1207.81ms, mfu -100.00%\n",
      "epoch: 109, step: 660 train/loss: 0.1072, train/tfacc: 0.9783, val/loss: 0.2785, val/tfacc: 0.8877, 🤖\n",
      "epoch 109: loss 0.3687, time 1227.12ms, mfu -100.00%\n",
      "epoch: 110, step: 666 train/loss: 0.1020, train/tfacc: 0.9859, val/loss: 0.2723, val/tfacc: 0.8897, 🤖\n",
      "epoch 110: loss 0.3704, time 1269.19ms, mfu -100.00%\n",
      "epoch: 111, step: 672 train/loss: 0.1118, train/tfacc: 0.9754, val/loss: 0.2854, val/tfacc: 0.8843, 🤖\n",
      "epoch 111: loss 0.3529, time 1197.09ms, mfu -100.00%\n",
      "epoch: 112, step: 678 train/loss: 0.1110, train/tfacc: 0.9768, val/loss: 0.2857, val/tfacc: 0.8836, 🤖\n",
      "epoch 112: loss 0.3060, time 1096.74ms, mfu -100.00%\n",
      "epoch: 113, step: 684 train/loss: 0.1041, train/tfacc: 0.9797, val/loss: 0.2811, val/tfacc: 0.8871, 🤖\n",
      "epoch 113: loss 0.3487, time 1260.73ms, mfu -100.00%\n",
      "epoch: 114, step: 690 train/loss: 0.1098, train/tfacc: 0.9737, val/loss: 0.2878, val/tfacc: 0.8839, 🤖\n",
      "epoch 114: loss 0.3447, time 1123.54ms, mfu -100.00%\n",
      "epoch: 115, step: 696 train/loss: 0.1059, train/tfacc: 0.9745, val/loss: 0.2858, val/tfacc: 0.8844, 🤖\n",
      "epoch 115: loss 0.3338, time 1275.83ms, mfu -100.00%\n",
      "epoch: 116, step: 702 train/loss: 0.1003, train/tfacc: 0.9810, val/loss: 0.2788, val/tfacc: 0.8866, 🤖\n",
      "epoch 116: loss 0.3304, time 1133.51ms, mfu -100.00%\n",
      "epoch: 117, step: 708 train/loss: 0.1038, train/tfacc: 0.9801, val/loss: 0.2778, val/tfacc: 0.8885, 🤖\n",
      "epoch 117: loss 0.3598, time 1241.48ms, mfu -100.00%\n",
      "epoch: 118, step: 714 train/loss: 0.1003, train/tfacc: 0.9799, val/loss: 0.2737, val/tfacc: 0.8896, 🤖\n",
      "epoch 118: loss 0.3259, time 1146.49ms, mfu -100.00%\n",
      "epoch: 119, step: 720 train/loss: 0.0950, train/tfacc: 0.9841, val/loss: 0.2685, val/tfacc: 0.8920, 🤖\n",
      "epoch 119: loss 0.3659, time 1127.99ms, mfu -100.00%\n",
      "epoch: 120, step: 726 train/loss: 0.0993, train/tfacc: 0.9797, val/loss: 0.2780, val/tfacc: 0.8870, 🤖\n",
      "epoch 120: loss 0.3241, time 1268.60ms, mfu -100.00%\n",
      "epoch: 121, step: 732 train/loss: 0.0964, train/tfacc: 0.9813, val/loss: 0.2726, val/tfacc: 0.8887, 🤖\n",
      "epoch 121: loss 0.3518, time 1156.54ms, mfu -100.00%\n",
      "epoch: 122, step: 738 train/loss: 0.1110, train/tfacc: 0.9733, val/loss: 0.2904, val/tfacc: 0.8816, 🤖\n",
      "epoch 122: loss 0.3162, time 1244.29ms, mfu -100.00%\n",
      "epoch: 123, step: 744 train/loss: 0.0893, train/tfacc: 0.9848, val/loss: 0.2623, val/tfacc: 0.8945, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 123: loss 0.3250, time 1389.29ms, mfu -100.00%\n",
      "epoch: 124, step: 750 train/loss: 0.1028, train/tfacc: 0.9776, val/loss: 0.2840, val/tfacc: 0.8833, 🤖\n",
      "epoch 124: loss 0.3177, time 1233.91ms, mfu -100.00%\n",
      "epoch: 125, step: 756 train/loss: 0.1006, train/tfacc: 0.9780, val/loss: 0.2781, val/tfacc: 0.8877, 🤖\n",
      "epoch 125: loss 0.3061, time 1247.63ms, mfu -100.00%\n",
      "epoch: 126, step: 762 train/loss: 0.0986, train/tfacc: 0.9775, val/loss: 0.2790, val/tfacc: 0.8862, 🤖\n",
      "epoch 126: loss 0.3019, time 1175.24ms, mfu -100.00%\n",
      "epoch: 127, step: 768 train/loss: 0.0860, train/tfacc: 0.9848, val/loss: 0.2551, val/tfacc: 0.8968, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 127: loss 0.3762, time 1269.82ms, mfu -100.00%\n",
      "epoch: 128, step: 774 train/loss: 0.0993, train/tfacc: 0.9767, val/loss: 0.2801, val/tfacc: 0.8844, 🤖\n",
      "epoch 128: loss 0.3178, time 1224.56ms, mfu -100.00%\n",
      "epoch: 129, step: 780 train/loss: 0.0897, train/tfacc: 0.9839, val/loss: 0.2678, val/tfacc: 0.8918, 🤖\n",
      "epoch 129: loss 0.3208, time 1210.46ms, mfu -100.00%\n",
      "epoch: 130, step: 786 train/loss: 0.0949, train/tfacc: 0.9802, val/loss: 0.2759, val/tfacc: 0.8877, 🤖\n",
      "epoch 130: loss 0.3512, time 1204.38ms, mfu -100.00%\n",
      "epoch: 131, step: 792 train/loss: 0.0844, train/tfacc: 0.9845, val/loss: 0.2575, val/tfacc: 0.8964, 🤖\n",
      "epoch 131: loss 0.3010, time 1140.60ms, mfu -100.00%\n",
      "epoch: 132, step: 798 train/loss: 0.0914, train/tfacc: 0.9814, val/loss: 0.2710, val/tfacc: 0.8893, 🤖\n",
      "epoch 132: loss 0.2809, time 1251.95ms, mfu -100.00%\n",
      "epoch: 133, step: 804 train/loss: 0.0834, train/tfacc: 0.9854, val/loss: 0.2623, val/tfacc: 0.8940, 🤖\n",
      "epoch 133: loss 0.2885, time 1225.77ms, mfu -100.00%\n",
      "epoch: 134, step: 810 train/loss: 0.0797, train/tfacc: 0.9873, val/loss: 0.2513, val/tfacc: 0.8994, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 134: loss 0.2829, time 1350.30ms, mfu -100.00%\n",
      "epoch: 135, step: 816 train/loss: 0.0823, train/tfacc: 0.9856, val/loss: 0.2579, val/tfacc: 0.8953, 🤖\n",
      "epoch 135: loss 0.2978, time 1385.93ms, mfu -100.00%\n",
      "epoch: 136, step: 822 train/loss: 0.0773, train/tfacc: 0.9873, val/loss: 0.2534, val/tfacc: 0.8978, 🤖\n",
      "epoch 136: loss 0.2681, time 1425.80ms, mfu -100.00%\n",
      "epoch: 137, step: 828 train/loss: 0.0678, train/tfacc: 0.9918, val/loss: 0.2362, val/tfacc: 0.9060, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 137: loss 0.3151, time 1576.78ms, mfu -100.00%\n",
      "epoch: 138, step: 834 train/loss: 0.0905, train/tfacc: 0.9812, val/loss: 0.2724, val/tfacc: 0.8892, 🤖\n",
      "epoch 138: loss 0.3271, time 1613.17ms, mfu -100.00%\n",
      "epoch: 139, step: 840 train/loss: 0.0790, train/tfacc: 0.9860, val/loss: 0.2593, val/tfacc: 0.8950, 🤖\n",
      "epoch 139: loss 0.2759, time 1376.09ms, mfu -100.00%\n",
      "epoch: 140, step: 846 train/loss: 0.0790, train/tfacc: 0.9867, val/loss: 0.2647, val/tfacc: 0.8920, 🤖\n",
      "epoch 140: loss 0.3118, time 1479.47ms, mfu -100.00%\n",
      "epoch: 141, step: 852 train/loss: 0.0770, train/tfacc: 0.9864, val/loss: 0.2547, val/tfacc: 0.8976, 🤖\n",
      "epoch 141: loss 0.3189, time 1296.93ms, mfu -100.00%\n",
      "epoch: 142, step: 858 train/loss: 0.0768, train/tfacc: 0.9879, val/loss: 0.2446, val/tfacc: 0.9013, 🤖\n",
      "epoch 142: loss 0.2826, time 1232.21ms, mfu -100.00%\n",
      "epoch: 143, step: 864 train/loss: 0.0746, train/tfacc: 0.9883, val/loss: 0.2482, val/tfacc: 0.8989, 🤖\n",
      "epoch 143: loss 0.2769, time 1310.75ms, mfu -100.00%\n",
      "epoch: 144, step: 870 train/loss: 0.0830, train/tfacc: 0.9850, val/loss: 0.2627, val/tfacc: 0.8932, 🤖\n",
      "epoch 144: loss 0.2595, time 1312.39ms, mfu -100.00%\n",
      "epoch: 145, step: 876 train/loss: 0.0708, train/tfacc: 0.9914, val/loss: 0.2537, val/tfacc: 0.8961, 🤖\n",
      "epoch 145: loss 0.3116, time 1344.33ms, mfu -100.00%\n",
      "epoch: 146, step: 882 train/loss: 0.0801, train/tfacc: 0.9846, val/loss: 0.2665, val/tfacc: 0.8917, 🤖\n",
      "epoch 146: loss 0.2615, time 1448.40ms, mfu -100.00%\n",
      "epoch: 147, step: 888 train/loss: 0.0631, train/tfacc: 0.9918, val/loss: 0.2374, val/tfacc: 0.9041, 🤖\n",
      "epoch 147: loss 0.2818, time 1408.12ms, mfu -100.00%\n",
      "epoch: 148, step: 894 train/loss: 0.0761, train/tfacc: 0.9855, val/loss: 0.2605, val/tfacc: 0.8930, 🤖\n",
      "epoch 148: loss 0.2672, time 1289.03ms, mfu -100.00%\n",
      "epoch: 149, step: 900 train/loss: 0.0737, train/tfacc: 0.9873, val/loss: 0.2520, val/tfacc: 0.8988, 🤖\n",
      "epoch 149: loss 0.2864, time 1300.70ms, mfu -100.00%\n",
      "epoch: 150, step: 906 train/loss: 0.0673, train/tfacc: 0.9907, val/loss: 0.2435, val/tfacc: 0.9035, 🤖\n",
      "epoch 150: loss 0.2589, time 1283.61ms, mfu -100.00%\n",
      "epoch: 151, step: 912 train/loss: 0.0683, train/tfacc: 0.9897, val/loss: 0.2440, val/tfacc: 0.9010, 🤖\n",
      "epoch 151: loss 0.2545, time 1387.94ms, mfu -100.00%\n",
      "epoch: 152, step: 918 train/loss: 0.0625, train/tfacc: 0.9924, val/loss: 0.2325, val/tfacc: 0.9063, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 152: loss 0.2544, time 1488.73ms, mfu -100.00%\n",
      "epoch: 153, step: 924 train/loss: 0.0697, train/tfacc: 0.9889, val/loss: 0.2496, val/tfacc: 0.8989, 🤖\n",
      "epoch 153: loss 0.2725, time 1251.39ms, mfu -100.00%\n",
      "epoch: 154, step: 930 train/loss: 0.0680, train/tfacc: 0.9896, val/loss: 0.2399, val/tfacc: 0.9037, 🤖\n",
      "epoch 154: loss 0.2760, time 1221.78ms, mfu -100.00%\n",
      "epoch: 155, step: 936 train/loss: 0.0662, train/tfacc: 0.9888, val/loss: 0.2458, val/tfacc: 0.8996, 🤖\n",
      "epoch 155: loss 0.2760, time 1203.32ms, mfu -100.00%\n",
      "epoch: 156, step: 942 train/loss: 0.0676, train/tfacc: 0.9888, val/loss: 0.2479, val/tfacc: 0.8989, 🤖\n",
      "epoch 156: loss 0.2658, time 1378.69ms, mfu -100.00%\n",
      "epoch: 157, step: 948 train/loss: 0.0638, train/tfacc: 0.9894, val/loss: 0.2438, val/tfacc: 0.9020, 🤖\n",
      "epoch 157: loss 0.2947, time 1331.54ms, mfu -100.00%\n",
      "epoch: 158, step: 954 train/loss: 0.0727, train/tfacc: 0.9869, val/loss: 0.2576, val/tfacc: 0.8947, 🤖\n",
      "epoch 158: loss 0.2713, time 1217.54ms, mfu -100.00%\n",
      "epoch: 159, step: 960 train/loss: 0.0654, train/tfacc: 0.9894, val/loss: 0.2515, val/tfacc: 0.8978, 🤖\n",
      "epoch 159: loss 0.2441, time 1261.43ms, mfu -100.00%\n",
      "epoch: 160, step: 966 train/loss: 0.0672, train/tfacc: 0.9864, val/loss: 0.2556, val/tfacc: 0.8961, 🤖\n",
      "epoch 160: loss 0.2584, time 1232.59ms, mfu -100.00%\n",
      "epoch: 161, step: 972 train/loss: 0.0644, train/tfacc: 0.9906, val/loss: 0.2463, val/tfacc: 0.8993, 🤖\n",
      "epoch 161: loss 0.3087, time 1237.00ms, mfu -100.00%\n",
      "epoch: 162, step: 978 train/loss: 0.0755, train/tfacc: 0.9831, val/loss: 0.2561, val/tfacc: 0.8967, 🤖\n",
      "epoch 162: loss 0.2469, time 1106.99ms, mfu -100.00%\n",
      "epoch: 163, step: 984 train/loss: 0.0639, train/tfacc: 0.9893, val/loss: 0.2416, val/tfacc: 0.9022, 🤖\n",
      "epoch 163: loss 0.3084, time 1246.63ms, mfu -100.00%\n",
      "epoch: 164, step: 990 train/loss: 0.0708, train/tfacc: 0.9877, val/loss: 0.2524, val/tfacc: 0.8978, 🤖\n",
      "epoch 164: loss 0.2636, time 1196.59ms, mfu -100.00%\n",
      "epoch: 165, step: 996 train/loss: 0.0682, train/tfacc: 0.9868, val/loss: 0.2521, val/tfacc: 0.8987, 🤖\n",
      "epoch 165: loss 0.2628, time 1292.46ms, mfu -100.00%\n",
      "epoch: 166, step: 1002 train/loss: 0.0605, train/tfacc: 0.9907, val/loss: 0.2394, val/tfacc: 0.9029, 🤖\n",
      "epoch 166: loss 0.2419, time 1295.80ms, mfu -100.00%\n",
      "epoch: 167, step: 1008 train/loss: 0.0585, train/tfacc: 0.9895, val/loss: 0.2367, val/tfacc: 0.9046, 🤖\n",
      "epoch 167: loss 0.3002, time 1248.32ms, mfu -100.00%\n",
      "epoch: 168, step: 1014 train/loss: 0.0548, train/tfacc: 0.9926, val/loss: 0.2331, val/tfacc: 0.9050, 🤖\n",
      "epoch 168: loss 0.2435, time 1292.56ms, mfu -100.00%\n",
      "epoch: 169, step: 1020 train/loss: 0.0596, train/tfacc: 0.9926, val/loss: 0.2424, val/tfacc: 0.9033, 🤖\n",
      "epoch 169: loss 0.2395, time 1342.22ms, mfu -100.00%\n",
      "epoch: 170, step: 1026 train/loss: 0.0628, train/tfacc: 0.9887, val/loss: 0.2511, val/tfacc: 0.8988, 🤖\n",
      "epoch 170: loss 0.2795, time 1188.99ms, mfu -100.00%\n",
      "epoch: 171, step: 1032 train/loss: 0.0601, train/tfacc: 0.9912, val/loss: 0.2431, val/tfacc: 0.9016, 🤖\n",
      "epoch 171: loss 0.2210, time 1164.28ms, mfu -100.00%\n",
      "epoch: 172, step: 1038 train/loss: 0.0554, train/tfacc: 0.9915, val/loss: 0.2423, val/tfacc: 0.9028, 🤖\n",
      "epoch 172: loss 0.2388, time 1199.21ms, mfu -100.00%\n",
      "epoch: 173, step: 1044 train/loss: 0.0508, train/tfacc: 0.9961, val/loss: 0.2338, val/tfacc: 0.9056, 🤖\n",
      "epoch 173: loss 0.2515, time 1195.21ms, mfu -100.00%\n",
      "epoch: 174, step: 1050 train/loss: 0.0615, train/tfacc: 0.9895, val/loss: 0.2505, val/tfacc: 0.8982, 🤖\n",
      "epoch 174: loss 0.2348, time 1190.53ms, mfu -100.00%\n",
      "epoch: 175, step: 1056 train/loss: 0.0563, train/tfacc: 0.9916, val/loss: 0.2419, val/tfacc: 0.9022, 🤖\n",
      "epoch 175: loss 0.2419, time 1374.70ms, mfu -100.00%\n",
      "epoch: 176, step: 1062 train/loss: 0.0585, train/tfacc: 0.9895, val/loss: 0.2520, val/tfacc: 0.8979, 🤖\n",
      "epoch 176: loss 0.2541, time 1322.58ms, mfu -100.00%\n",
      "epoch: 177, step: 1068 train/loss: 0.0592, train/tfacc: 0.9900, val/loss: 0.2476, val/tfacc: 0.8991, 🤖\n",
      "epoch 177: loss 0.2636, time 1313.95ms, mfu -100.00%\n",
      "epoch: 178, step: 1074 train/loss: 0.0546, train/tfacc: 0.9921, val/loss: 0.2353, val/tfacc: 0.9043, 🤖\n",
      "epoch 178: loss 0.2480, time 1474.81ms, mfu -100.00%\n",
      "epoch: 179, step: 1080 train/loss: 0.0628, train/tfacc: 0.9887, val/loss: 0.2490, val/tfacc: 0.9000, 🤖\n",
      "epoch 179: loss 0.2534, time 1232.59ms, mfu -100.00%\n",
      "epoch: 180, step: 1086 train/loss: 0.0526, train/tfacc: 0.9901, val/loss: 0.2371, val/tfacc: 0.9041, 🤖\n",
      "epoch 180: loss 0.2537, time 1275.04ms, mfu -100.00%\n",
      "epoch: 181, step: 1092 train/loss: 0.0518, train/tfacc: 0.9927, val/loss: 0.2330, val/tfacc: 0.9044, 🤖\n",
      "epoch 181: loss 0.2415, time 1244.51ms, mfu -100.00%\n",
      "epoch: 182, step: 1098 train/loss: 0.0492, train/tfacc: 0.9928, val/loss: 0.2255, val/tfacc: 0.9085, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 182: loss 0.2610, time 1433.99ms, mfu -100.00%\n",
      "epoch: 183, step: 1104 train/loss: 0.0553, train/tfacc: 0.9915, val/loss: 0.2387, val/tfacc: 0.9047, 🤖\n",
      "epoch 183: loss 0.2136, time 1164.42ms, mfu -100.00%\n",
      "epoch: 184, step: 1110 train/loss: 0.0496, train/tfacc: 0.9947, val/loss: 0.2351, val/tfacc: 0.9056, 🤖\n",
      "epoch 184: loss 0.2145, time 1201.02ms, mfu -100.00%\n",
      "epoch: 185, step: 1116 train/loss: 0.0462, train/tfacc: 0.9958, val/loss: 0.2280, val/tfacc: 0.9074, 🤖\n",
      "epoch 185: loss 0.2103, time 1343.15ms, mfu -100.00%\n",
      "epoch: 186, step: 1122 train/loss: 0.0438, train/tfacc: 0.9963, val/loss: 0.2184, val/tfacc: 0.9119, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 186: loss 0.2259, time 2192.43ms, mfu -100.00%\n",
      "epoch: 187, step: 1128 train/loss: 0.0486, train/tfacc: 0.9936, val/loss: 0.2286, val/tfacc: 0.9080, 🤖\n",
      "epoch 187: loss 0.2507, time 1385.74ms, mfu -100.00%\n",
      "epoch: 188, step: 1134 train/loss: 0.0494, train/tfacc: 0.9933, val/loss: 0.2259, val/tfacc: 0.9084, 🤖\n",
      "epoch 188: loss 0.2080, time 1250.93ms, mfu -100.00%\n",
      "epoch: 189, step: 1140 train/loss: 0.0436, train/tfacc: 0.9947, val/loss: 0.2260, val/tfacc: 0.9094, 🤖\n",
      "epoch 189: loss 0.2223, time 2107.45ms, mfu -100.00%\n",
      "epoch: 190, step: 1146 train/loss: 0.0424, train/tfacc: 0.9959, val/loss: 0.2219, val/tfacc: 0.9108, 🤖\n",
      "epoch 190: loss 0.2309, time 1448.45ms, mfu -100.00%\n",
      "epoch: 191, step: 1152 train/loss: 0.0503, train/tfacc: 0.9938, val/loss: 0.2366, val/tfacc: 0.9046, 🤖\n",
      "epoch 191: loss 0.2470, time 1196.09ms, mfu -100.00%\n",
      "epoch: 192, step: 1158 train/loss: 0.0440, train/tfacc: 0.9949, val/loss: 0.2217, val/tfacc: 0.9103, 🤖\n",
      "epoch 192: loss 0.2362, time 1105.44ms, mfu -100.00%\n",
      "epoch: 193, step: 1164 train/loss: 0.0453, train/tfacc: 0.9954, val/loss: 0.2296, val/tfacc: 0.9061, 🤖\n",
      "epoch 193: loss 0.2260, time 1659.83ms, mfu -100.00%\n",
      "epoch: 194, step: 1170 train/loss: 0.0494, train/tfacc: 0.9914, val/loss: 0.2394, val/tfacc: 0.9035, 🤖\n",
      "epoch 194: loss 0.2245, time 1140.40ms, mfu -100.00%\n",
      "epoch: 195, step: 1176 train/loss: 0.0588, train/tfacc: 0.9868, val/loss: 0.2596, val/tfacc: 0.8962, 🤖\n",
      "epoch 195: loss 0.2357, time 1105.02ms, mfu -100.00%\n",
      "epoch: 196, step: 1182 train/loss: 0.0543, train/tfacc: 0.9893, val/loss: 0.2402, val/tfacc: 0.9034, 🤖\n",
      "epoch 196: loss 0.2414, time 1194.42ms, mfu -100.00%\n",
      "epoch: 197, step: 1188 train/loss: 0.0548, train/tfacc: 0.9894, val/loss: 0.2411, val/tfacc: 0.9010, 🤖\n",
      "epoch 197: loss 0.2718, time 1147.73ms, mfu -100.00%\n",
      "epoch: 198, step: 1194 train/loss: 0.0435, train/tfacc: 0.9941, val/loss: 0.2253, val/tfacc: 0.9104, 🤖\n",
      "epoch 198: loss 0.2210, time 1167.06ms, mfu -100.00%\n",
      "epoch: 199, step: 1200 train/loss: 0.0430, train/tfacc: 0.9951, val/loss: 0.2311, val/tfacc: 0.9067, 🤖\n",
      "epoch 199: loss 0.2085, time 1342.25ms, mfu -100.00%\n",
      "epoch: 200, step: 1206 train/loss: 0.0462, train/tfacc: 0.9937, val/loss: 0.2412, val/tfacc: 0.9043, 🤖\n",
      "epoch 200: loss 0.2205, time 1152.73ms, mfu -100.00%\n",
      "epoch: 201, step: 1212 train/loss: 0.0424, train/tfacc: 0.9946, val/loss: 0.2267, val/tfacc: 0.9099, 🤖\n",
      "epoch 201: loss 0.2397, time 1110.40ms, mfu -100.00%\n",
      "epoch: 202, step: 1218 train/loss: 0.0486, train/tfacc: 0.9917, val/loss: 0.2392, val/tfacc: 0.9030, 🤖\n",
      "epoch 202: loss 0.2413, time 1168.33ms, mfu -100.00%\n",
      "epoch: 203, step: 1224 train/loss: 0.0473, train/tfacc: 0.9941, val/loss: 0.2352, val/tfacc: 0.9059, 🤖\n",
      "epoch 203: loss 0.2366, time 1181.65ms, mfu -100.00%\n",
      "epoch: 204, step: 1230 train/loss: 0.0477, train/tfacc: 0.9926, val/loss: 0.2481, val/tfacc: 0.9003, 🤖\n",
      "epoch 204: loss 0.2192, time 1120.31ms, mfu -100.00%\n",
      "epoch: 205, step: 1236 train/loss: 0.0462, train/tfacc: 0.9927, val/loss: 0.2425, val/tfacc: 0.9021, 🤖\n",
      "epoch 205: loss 0.2580, time 1180.34ms, mfu -100.00%\n",
      "epoch: 206, step: 1242 train/loss: 0.0418, train/tfacc: 0.9955, val/loss: 0.2264, val/tfacc: 0.9097, 🤖\n",
      "epoch 206: loss 0.2240, time 1086.11ms, mfu -100.00%\n",
      "epoch: 207, step: 1248 train/loss: 0.0520, train/tfacc: 0.9893, val/loss: 0.2428, val/tfacc: 0.9030, 🤖\n",
      "epoch 207: loss 0.2030, time 1186.24ms, mfu -100.00%\n",
      "epoch: 208, step: 1254 train/loss: 0.0389, train/tfacc: 0.9959, val/loss: 0.2209, val/tfacc: 0.9107, 🤖\n",
      "epoch 208: loss 0.2483, time 1091.03ms, mfu -100.00%\n",
      "epoch: 209, step: 1260 train/loss: 0.0452, train/tfacc: 0.9926, val/loss: 0.2284, val/tfacc: 0.9092, 🤖\n",
      "epoch 209: loss 0.2065, time 1198.80ms, mfu -100.00%\n",
      "epoch: 210, step: 1266 train/loss: 0.0403, train/tfacc: 0.9938, val/loss: 0.2288, val/tfacc: 0.9094, 🤖\n",
      "epoch 210: loss 0.1914, time 1108.83ms, mfu -100.00%\n",
      "epoch: 211, step: 1272 train/loss: 0.0389, train/tfacc: 0.9953, val/loss: 0.2240, val/tfacc: 0.9108, 🤖\n",
      "epoch 211: loss 0.2082, time 1172.18ms, mfu -100.00%\n",
      "epoch: 212, step: 1278 train/loss: 0.0363, train/tfacc: 0.9971, val/loss: 0.2189, val/tfacc: 0.9125, 🤖\n",
      "epoch 212: loss 0.2278, time 1119.47ms, mfu -100.00%\n",
      "epoch: 213, step: 1284 train/loss: 0.0445, train/tfacc: 0.9916, val/loss: 0.2341, val/tfacc: 0.9073, 🤖\n",
      "epoch 213: loss 0.2130, time 1187.71ms, mfu -100.00%\n",
      "epoch: 214, step: 1290 train/loss: 0.0369, train/tfacc: 0.9961, val/loss: 0.2172, val/tfacc: 0.9131, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 214: loss 0.2169, time 1325.16ms, mfu -100.00%\n",
      "epoch: 215, step: 1296 train/loss: 0.0420, train/tfacc: 0.9949, val/loss: 0.2307, val/tfacc: 0.9070, 🤖\n",
      "epoch 215: loss 0.2211, time 1094.06ms, mfu -100.00%\n",
      "epoch: 216, step: 1302 train/loss: 0.0455, train/tfacc: 0.9926, val/loss: 0.2412, val/tfacc: 0.9025, 🤖\n",
      "epoch 216: loss 0.2219, time 1069.05ms, mfu -100.00%\n",
      "epoch: 217, step: 1308 train/loss: 0.0412, train/tfacc: 0.9946, val/loss: 0.2255, val/tfacc: 0.9095, 🤖\n",
      "epoch 217: loss 0.2366, time 1113.71ms, mfu -100.00%\n",
      "epoch: 218, step: 1314 train/loss: 0.0323, train/tfacc: 0.9970, val/loss: 0.2076, val/tfacc: 0.9157, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 218: loss 0.1858, time 1263.06ms, mfu -100.00%\n",
      "epoch: 219, step: 1320 train/loss: 0.0349, train/tfacc: 0.9969, val/loss: 0.2183, val/tfacc: 0.9124, 🤖\n",
      "epoch 219: loss 0.1891, time 1102.42ms, mfu -100.00%\n",
      "epoch: 220, step: 1326 train/loss: 0.0333, train/tfacc: 0.9976, val/loss: 0.2102, val/tfacc: 0.9159, 🤖\n",
      "epoch 220: loss 0.1972, time 1170.15ms, mfu -100.00%\n",
      "epoch: 221, step: 1332 train/loss: 0.0319, train/tfacc: 0.9975, val/loss: 0.2078, val/tfacc: 0.9166, 🤖\n",
      "epoch 221: loss 0.1996, time 1163.77ms, mfu -100.00%\n",
      "epoch: 222, step: 1338 train/loss: 0.0375, train/tfacc: 0.9947, val/loss: 0.2214, val/tfacc: 0.9112, 🤖\n",
      "epoch 222: loss 0.2033, time 1187.58ms, mfu -100.00%\n",
      "epoch: 223, step: 1344 train/loss: 0.0374, train/tfacc: 0.9955, val/loss: 0.2225, val/tfacc: 0.9122, 🤖\n",
      "epoch 223: loss 0.2268, time 1174.11ms, mfu -100.00%\n",
      "epoch: 224, step: 1350 train/loss: 0.0465, train/tfacc: 0.9905, val/loss: 0.2420, val/tfacc: 0.9035, 🤖\n",
      "epoch 224: loss 0.2301, time 1094.46ms, mfu -100.00%\n",
      "epoch: 225, step: 1356 train/loss: 0.0364, train/tfacc: 0.9952, val/loss: 0.2158, val/tfacc: 0.9140, 🤖\n",
      "epoch 225: loss 0.2072, time 1167.18ms, mfu -100.00%\n",
      "epoch: 226, step: 1362 train/loss: 0.0379, train/tfacc: 0.9963, val/loss: 0.2246, val/tfacc: 0.9097, 🤖\n",
      "epoch 226: loss 0.1883, time 1229.29ms, mfu -100.00%\n",
      "epoch: 227, step: 1368 train/loss: 0.0321, train/tfacc: 0.9980, val/loss: 0.2137, val/tfacc: 0.9142, 🤖\n",
      "epoch 227: loss 0.2158, time 1364.65ms, mfu -100.00%\n",
      "epoch: 228, step: 1374 train/loss: 0.0326, train/tfacc: 0.9965, val/loss: 0.2107, val/tfacc: 0.9151, 🤖\n",
      "epoch 228: loss 0.2074, time 1432.15ms, mfu -100.00%\n",
      "epoch: 229, step: 1380 train/loss: 0.0383, train/tfacc: 0.9941, val/loss: 0.2199, val/tfacc: 0.9114, 🤖\n",
      "epoch 229: loss 0.2141, time 1242.89ms, mfu -100.00%\n",
      "epoch: 230, step: 1386 train/loss: 0.0285, train/tfacc: 0.9984, val/loss: 0.2076, val/tfacc: 0.9176, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 230: loss 0.1910, time 1328.86ms, mfu -100.00%\n",
      "epoch: 231, step: 1392 train/loss: 0.0350, train/tfacc: 0.9958, val/loss: 0.2267, val/tfacc: 0.9092, 🤖\n",
      "epoch 231: loss 0.2051, time 1169.64ms, mfu -100.00%\n",
      "epoch: 232, step: 1398 train/loss: 0.0346, train/tfacc: 0.9960, val/loss: 0.2193, val/tfacc: 0.9118, 🤖\n",
      "epoch 232: loss 0.2071, time 1116.98ms, mfu -100.00%\n",
      "epoch: 233, step: 1404 train/loss: 0.0332, train/tfacc: 0.9969, val/loss: 0.2136, val/tfacc: 0.9134, 🤖\n",
      "epoch 233: loss 0.2019, time 1138.05ms, mfu -100.00%\n",
      "epoch: 234, step: 1410 train/loss: 0.0337, train/tfacc: 0.9953, val/loss: 0.2176, val/tfacc: 0.9121, 🤖\n",
      "epoch 234: loss 0.2104, time 1154.31ms, mfu -100.00%\n",
      "epoch: 235, step: 1416 train/loss: 0.0296, train/tfacc: 0.9976, val/loss: 0.2109, val/tfacc: 0.9159, 🤖\n",
      "epoch 235: loss 0.1926, time 1207.51ms, mfu -100.00%\n",
      "epoch: 236, step: 1422 train/loss: 0.0384, train/tfacc: 0.9954, val/loss: 0.2325, val/tfacc: 0.9060, 🤖\n",
      "epoch 236: loss 0.1913, time 1236.01ms, mfu -100.00%\n",
      "epoch: 237, step: 1428 train/loss: 0.0294, train/tfacc: 0.9980, val/loss: 0.2082, val/tfacc: 0.9163, 🤖\n",
      "epoch 237: loss 0.1903, time 1162.64ms, mfu -100.00%\n",
      "epoch: 238, step: 1434 train/loss: 0.0267, train/tfacc: 0.9989, val/loss: 0.1973, val/tfacc: 0.9220, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 238: loss 0.2100, time 1260.71ms, mfu -100.00%\n",
      "epoch: 239, step: 1440 train/loss: 0.0302, train/tfacc: 0.9974, val/loss: 0.2119, val/tfacc: 0.9147, 🤖\n",
      "epoch 239: loss 0.1873, time 1261.14ms, mfu -100.00%\n",
      "epoch: 240, step: 1446 train/loss: 0.0376, train/tfacc: 0.9932, val/loss: 0.2269, val/tfacc: 0.9100, 🤖\n",
      "epoch 240: loss 0.1881, time 1231.87ms, mfu -100.00%\n",
      "epoch: 241, step: 1452 train/loss: 0.0335, train/tfacc: 0.9958, val/loss: 0.2254, val/tfacc: 0.9112, 🤖\n",
      "epoch 241: loss 0.2142, time 1165.59ms, mfu -100.00%\n",
      "epoch: 242, step: 1458 train/loss: 0.0351, train/tfacc: 0.9951, val/loss: 0.2214, val/tfacc: 0.9116, 🤖\n",
      "epoch 242: loss 0.1722, time 1180.40ms, mfu -100.00%\n",
      "epoch: 243, step: 1464 train/loss: 0.0316, train/tfacc: 0.9969, val/loss: 0.2148, val/tfacc: 0.9149, 🤖\n",
      "epoch 243: loss 0.2001, time 1414.82ms, mfu -100.00%\n",
      "epoch: 244, step: 1470 train/loss: 0.0401, train/tfacc: 0.9920, val/loss: 0.2346, val/tfacc: 0.9063, 🤖\n",
      "epoch 244: loss 0.2031, time 1087.13ms, mfu -100.00%\n",
      "epoch: 245, step: 1476 train/loss: 0.0338, train/tfacc: 0.9955, val/loss: 0.2132, val/tfacc: 0.9150, 🤖\n",
      "epoch 245: loss 0.2202, time 1485.58ms, mfu -100.00%\n",
      "epoch: 246, step: 1482 train/loss: 0.0375, train/tfacc: 0.9946, val/loss: 0.2222, val/tfacc: 0.9106, 🤖\n",
      "epoch 246: loss 0.2506, time 1336.97ms, mfu -100.00%\n",
      "epoch: 247, step: 1488 train/loss: 0.0328, train/tfacc: 0.9960, val/loss: 0.2190, val/tfacc: 0.9116, 🤖\n",
      "epoch 247: loss 0.1804, time 1209.88ms, mfu -100.00%\n",
      "epoch: 248, step: 1494 train/loss: 0.0309, train/tfacc: 0.9955, val/loss: 0.2121, val/tfacc: 0.9151, 🤖\n",
      "epoch 248: loss 0.2079, time 1121.88ms, mfu -100.00%\n",
      "epoch: 249, step: 1500 train/loss: 0.0294, train/tfacc: 0.9969, val/loss: 0.2023, val/tfacc: 0.9190, 🤖\n",
      "epoch 249: loss 0.1956, time 1224.79ms, mfu -100.00%\n",
      "epoch: 250, step: 1506 train/loss: 0.0286, train/tfacc: 0.9977, val/loss: 0.2070, val/tfacc: 0.9184, 🤖\n",
      "epoch 250: loss 0.2025, time 1196.47ms, mfu -100.00%\n",
      "epoch: 251, step: 1512 train/loss: 0.0266, train/tfacc: 0.9983, val/loss: 0.2044, val/tfacc: 0.9186, 🤖\n",
      "epoch 251: loss 0.1993, time 1150.92ms, mfu -100.00%\n",
      "epoch: 252, step: 1518 train/loss: 0.0296, train/tfacc: 0.9969, val/loss: 0.2110, val/tfacc: 0.9152, 🤖\n",
      "epoch 252: loss 0.1863, time 1145.18ms, mfu -100.00%\n",
      "epoch: 253, step: 1524 train/loss: 0.0270, train/tfacc: 0.9980, val/loss: 0.2034, val/tfacc: 0.9189, 🤖\n",
      "epoch 253: loss 0.1556, time 1113.77ms, mfu -100.00%\n",
      "epoch: 254, step: 1530 train/loss: 0.0307, train/tfacc: 0.9959, val/loss: 0.2188, val/tfacc: 0.9128, 🤖\n",
      "epoch 254: loss 0.1774, time 1141.01ms, mfu -100.00%\n",
      "epoch: 255, step: 1536 train/loss: 0.0241, train/tfacc: 0.9984, val/loss: 0.2022, val/tfacc: 0.9197, 🤖\n",
      "epoch 255: loss 0.1947, time 1207.55ms, mfu -100.00%\n",
      "epoch: 256, step: 1542 train/loss: 0.0300, train/tfacc: 0.9974, val/loss: 0.2135, val/tfacc: 0.9138, 🤖\n",
      "epoch 256: loss 0.1896, time 1145.18ms, mfu -100.00%\n",
      "epoch: 257, step: 1548 train/loss: 0.0305, train/tfacc: 0.9963, val/loss: 0.2195, val/tfacc: 0.9129, 🤖\n",
      "epoch 257: loss 0.1993, time 1093.12ms, mfu -100.00%\n",
      "epoch: 258, step: 1554 train/loss: 0.0234, train/tfacc: 0.9984, val/loss: 0.2031, val/tfacc: 0.9191, 🤖\n",
      "epoch 258: loss 0.2100, time 1170.88ms, mfu -100.00%\n",
      "epoch: 259, step: 1560 train/loss: 0.0276, train/tfacc: 0.9972, val/loss: 0.2129, val/tfacc: 0.9139, 🤖\n",
      "epoch 259: loss 0.1877, time 1225.69ms, mfu -100.00%\n",
      "epoch: 260, step: 1566 train/loss: 0.0232, train/tfacc: 0.9987, val/loss: 0.1955, val/tfacc: 0.9210, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 260: loss 0.2102, time 1208.56ms, mfu -100.00%\n",
      "epoch: 261, step: 1572 train/loss: 0.0257, train/tfacc: 0.9982, val/loss: 0.2076, val/tfacc: 0.9172, 🤖\n",
      "epoch 261: loss 0.1924, time 1118.63ms, mfu -100.00%\n",
      "epoch: 262, step: 1578 train/loss: 0.0254, train/tfacc: 0.9979, val/loss: 0.2004, val/tfacc: 0.9199, 🤖\n",
      "epoch 262: loss 0.1907, time 1115.56ms, mfu -100.00%\n",
      "epoch: 263, step: 1584 train/loss: 0.0278, train/tfacc: 0.9967, val/loss: 0.2048, val/tfacc: 0.9171, 🤖\n",
      "epoch 263: loss 0.1742, time 1123.87ms, mfu -100.00%\n",
      "epoch: 264, step: 1590 train/loss: 0.0225, train/tfacc: 0.9984, val/loss: 0.1970, val/tfacc: 0.9218, 🤖\n",
      "epoch 264: loss 0.1868, time 1191.44ms, mfu -100.00%\n",
      "epoch: 265, step: 1596 train/loss: 0.0240, train/tfacc: 0.9988, val/loss: 0.1984, val/tfacc: 0.9210, 🤖\n",
      "epoch 265: loss 0.2179, time 1191.75ms, mfu -100.00%\n",
      "epoch: 266, step: 1602 train/loss: 0.0251, train/tfacc: 0.9975, val/loss: 0.2041, val/tfacc: 0.9186, 🤖\n",
      "epoch 266: loss 0.1937, time 1122.15ms, mfu -100.00%\n",
      "epoch: 267, step: 1608 train/loss: 0.0291, train/tfacc: 0.9959, val/loss: 0.2192, val/tfacc: 0.9135, 🤖\n",
      "epoch 267: loss 0.1874, time 1079.29ms, mfu -100.00%\n",
      "epoch: 268, step: 1614 train/loss: 0.0253, train/tfacc: 0.9984, val/loss: 0.2059, val/tfacc: 0.9179, 🤖\n",
      "epoch 268: loss 0.2074, time 1261.63ms, mfu -100.00%\n",
      "epoch: 269, step: 1620 train/loss: 0.0260, train/tfacc: 0.9966, val/loss: 0.2012, val/tfacc: 0.9208, 🤖\n",
      "epoch 269: loss 0.1839, time 1226.19ms, mfu -100.00%\n",
      "epoch: 270, step: 1626 train/loss: 0.0263, train/tfacc: 0.9969, val/loss: 0.2050, val/tfacc: 0.9180, 🤖\n",
      "epoch 270: loss 0.2208, time 1147.54ms, mfu -100.00%\n",
      "epoch: 271, step: 1632 train/loss: 0.0272, train/tfacc: 0.9971, val/loss: 0.2102, val/tfacc: 0.9157, 🤖\n",
      "epoch 271: loss 0.1868, time 1261.75ms, mfu -100.00%\n",
      "epoch: 272, step: 1638 train/loss: 0.0269, train/tfacc: 0.9965, val/loss: 0.2108, val/tfacc: 0.9165, 🤖\n",
      "epoch 272: loss 0.1705, time 1127.78ms, mfu -100.00%\n",
      "epoch: 273, step: 1644 train/loss: 0.0234, train/tfacc: 0.9974, val/loss: 0.2040, val/tfacc: 0.9204, 🤖\n",
      "epoch 273: loss 0.1988, time 1185.58ms, mfu -100.00%\n",
      "epoch: 274, step: 1650 train/loss: 0.0245, train/tfacc: 0.9985, val/loss: 0.2122, val/tfacc: 0.9165, 🤖\n",
      "epoch 274: loss 0.1764, time 1086.39ms, mfu -100.00%\n",
      "epoch: 275, step: 1656 train/loss: 0.0333, train/tfacc: 0.9949, val/loss: 0.2282, val/tfacc: 0.9080, 🤖\n",
      "epoch 275: loss 0.1782, time 1199.61ms, mfu -100.00%\n",
      "epoch: 276, step: 1662 train/loss: 0.0244, train/tfacc: 0.9974, val/loss: 0.2021, val/tfacc: 0.9205, 🤖\n",
      "epoch 276: loss 0.1832, time 1133.20ms, mfu -100.00%\n",
      "epoch: 277, step: 1668 train/loss: 0.0277, train/tfacc: 0.9961, val/loss: 0.2141, val/tfacc: 0.9144, 🤖\n",
      "epoch 277: loss 0.2230, time 1118.89ms, mfu -100.00%\n",
      "epoch: 278, step: 1674 train/loss: 0.0244, train/tfacc: 0.9980, val/loss: 0.2102, val/tfacc: 0.9161, 🤖\n",
      "epoch 278: loss 0.1994, time 1126.99ms, mfu -100.00%\n",
      "epoch: 279, step: 1680 train/loss: 0.0212, train/tfacc: 0.9986, val/loss: 0.2005, val/tfacc: 0.9197, 🤖\n",
      "epoch 279: loss 0.1899, time 1106.94ms, mfu -100.00%\n",
      "epoch: 280, step: 1686 train/loss: 0.0276, train/tfacc: 0.9967, val/loss: 0.2175, val/tfacc: 0.9142, 🤖\n",
      "epoch 280: loss 0.2074, time 1170.62ms, mfu -100.00%\n",
      "epoch: 281, step: 1692 train/loss: 0.0245, train/tfacc: 0.9977, val/loss: 0.2108, val/tfacc: 0.9153, 🤖\n",
      "epoch 281: loss 0.1645, time 1134.71ms, mfu -100.00%\n",
      "epoch: 282, step: 1698 train/loss: 0.0222, train/tfacc: 0.9976, val/loss: 0.1989, val/tfacc: 0.9212, 🤖\n",
      "epoch 282: loss 0.1555, time 1099.38ms, mfu -100.00%\n",
      "epoch: 283, step: 1704 train/loss: 0.0262, train/tfacc: 0.9975, val/loss: 0.2116, val/tfacc: 0.9146, 🤖\n",
      "epoch 283: loss 0.1888, time 1091.48ms, mfu -100.00%\n",
      "epoch: 284, step: 1710 train/loss: 0.0233, train/tfacc: 0.9975, val/loss: 0.2050, val/tfacc: 0.9189, 🤖\n",
      "epoch 284: loss 0.1710, time 1167.64ms, mfu -100.00%\n",
      "epoch: 285, step: 1716 train/loss: 0.0253, train/tfacc: 0.9968, val/loss: 0.2088, val/tfacc: 0.9174, 🤖\n",
      "epoch 285: loss 0.1727, time 1168.48ms, mfu -100.00%\n",
      "epoch: 286, step: 1722 train/loss: 0.0204, train/tfacc: 0.9983, val/loss: 0.1935, val/tfacc: 0.9230, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 286: loss 0.1378, time 1168.97ms, mfu -100.00%\n",
      "epoch: 287, step: 1728 train/loss: 0.0225, train/tfacc: 0.9977, val/loss: 0.2097, val/tfacc: 0.9172, 🤖\n",
      "epoch 287: loss 0.1676, time 1149.57ms, mfu -100.00%\n",
      "epoch: 288, step: 1734 train/loss: 0.0192, train/tfacc: 0.9995, val/loss: 0.1951, val/tfacc: 0.9221, 🤖\n",
      "epoch 288: loss 0.1732, time 1285.76ms, mfu -100.00%\n",
      "epoch: 289, step: 1740 train/loss: 0.0194, train/tfacc: 0.9990, val/loss: 0.1928, val/tfacc: 0.9240, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 289: loss 0.1641, time 1171.89ms, mfu -100.00%\n",
      "epoch: 290, step: 1746 train/loss: 0.0226, train/tfacc: 0.9982, val/loss: 0.2110, val/tfacc: 0.9154, 🤖\n",
      "epoch 290: loss 0.1712, time 1155.66ms, mfu -100.00%\n",
      "epoch: 291, step: 1752 train/loss: 0.0190, train/tfacc: 0.9991, val/loss: 0.1957, val/tfacc: 0.9223, 🤖\n",
      "epoch 291: loss 0.1592, time 1171.73ms, mfu -100.00%\n",
      "epoch: 292, step: 1758 train/loss: 0.0205, train/tfacc: 0.9991, val/loss: 0.2031, val/tfacc: 0.9193, 🤖\n",
      "epoch 292: loss 0.1667, time 1118.44ms, mfu -100.00%\n",
      "epoch: 293, step: 1764 train/loss: 0.0219, train/tfacc: 0.9983, val/loss: 0.2089, val/tfacc: 0.9175, 🤖\n",
      "epoch 293: loss 0.1572, time 1112.10ms, mfu -100.00%\n",
      "epoch: 294, step: 1770 train/loss: 0.0204, train/tfacc: 0.9987, val/loss: 0.2038, val/tfacc: 0.9185, 🤖\n",
      "epoch 294: loss 0.1763, time 1155.65ms, mfu -100.00%\n",
      "epoch: 295, step: 1776 train/loss: 0.0210, train/tfacc: 0.9979, val/loss: 0.2098, val/tfacc: 0.9176, 🤖\n",
      "epoch 295: loss 0.1773, time 1142.38ms, mfu -100.00%\n",
      "epoch: 296, step: 1782 train/loss: 0.0190, train/tfacc: 0.9987, val/loss: 0.2028, val/tfacc: 0.9202, 🤖\n",
      "epoch 296: loss 0.1739, time 1179.59ms, mfu -100.00%\n",
      "epoch: 297, step: 1788 train/loss: 0.0243, train/tfacc: 0.9972, val/loss: 0.2175, val/tfacc: 0.9137, 🤖\n",
      "epoch 297: loss 0.1659, time 1110.73ms, mfu -100.00%\n",
      "epoch: 298, step: 1794 train/loss: 0.0205, train/tfacc: 0.9976, val/loss: 0.2063, val/tfacc: 0.9191, 🤖\n",
      "epoch 298: loss 0.1514, time 1099.75ms, mfu -100.00%\n",
      "epoch: 299, step: 1800 train/loss: 0.0173, train/tfacc: 0.9991, val/loss: 0.1974, val/tfacc: 0.9210, 🤖\n",
      "epoch 299: loss 0.1766, time 1141.73ms, mfu -100.00%\n",
      "epoch: 300, step: 1806 train/loss: 0.0198, train/tfacc: 0.9984, val/loss: 0.2052, val/tfacc: 0.9188, 🤖\n",
      "epoch 300: loss 0.1677, time 1081.80ms, mfu -100.00%\n",
      "epoch: 301, step: 1812 train/loss: 0.0212, train/tfacc: 0.9980, val/loss: 0.2063, val/tfacc: 0.9185, 🤖\n",
      "epoch 301: loss 0.1471, time 1098.95ms, mfu -100.00%\n",
      "epoch: 302, step: 1818 train/loss: 0.0195, train/tfacc: 0.9984, val/loss: 0.1933, val/tfacc: 0.9231, 🤖\n",
      "epoch 302: loss 0.1581, time 1150.66ms, mfu -100.00%\n",
      "epoch: 303, step: 1824 train/loss: 0.0169, train/tfacc: 0.9989, val/loss: 0.1930, val/tfacc: 0.9234, 🤖\n",
      "epoch 303: loss 0.1859, time 1159.45ms, mfu -100.00%\n",
      "epoch: 304, step: 1830 train/loss: 0.0169, train/tfacc: 0.9993, val/loss: 0.1951, val/tfacc: 0.9227, 🤖\n",
      "epoch 304: loss 0.1569, time 1230.38ms, mfu -100.00%\n",
      "epoch: 305, step: 1836 train/loss: 0.0175, train/tfacc: 0.9993, val/loss: 0.1949, val/tfacc: 0.9225, 🤖\n",
      "epoch 305: loss 0.1673, time 1119.22ms, mfu -100.00%\n",
      "epoch: 306, step: 1842 train/loss: 0.0162, train/tfacc: 0.9992, val/loss: 0.1949, val/tfacc: 0.9233, 🤖\n",
      "epoch 306: loss 0.1818, time 1198.86ms, mfu -100.00%\n",
      "epoch: 307, step: 1848 train/loss: 0.0198, train/tfacc: 0.9982, val/loss: 0.2112, val/tfacc: 0.9172, 🤖\n",
      "epoch 307: loss 0.1851, time 1333.94ms, mfu -100.00%\n",
      "epoch: 308, step: 1854 train/loss: 0.0184, train/tfacc: 0.9988, val/loss: 0.2047, val/tfacc: 0.9178, 🤖\n",
      "epoch 308: loss 0.1732, time 1126.11ms, mfu -100.00%\n",
      "epoch: 309, step: 1860 train/loss: 0.0251, train/tfacc: 0.9964, val/loss: 0.2238, val/tfacc: 0.9118, 🤖\n",
      "epoch 309: loss 0.1340, time 1186.57ms, mfu -100.00%\n",
      "epoch: 310, step: 1866 train/loss: 0.0191, train/tfacc: 0.9986, val/loss: 0.2053, val/tfacc: 0.9202, 🤖\n",
      "epoch 310: loss 0.1485, time 1077.29ms, mfu -100.00%\n",
      "epoch: 311, step: 1872 train/loss: 0.0190, train/tfacc: 0.9982, val/loss: 0.2005, val/tfacc: 0.9205, 🤖\n",
      "epoch 311: loss 0.1662, time 1156.52ms, mfu -100.00%\n",
      "epoch: 312, step: 1878 train/loss: 0.0186, train/tfacc: 0.9988, val/loss: 0.2026, val/tfacc: 0.9198, 🤖\n",
      "epoch 312: loss 0.1617, time 1192.85ms, mfu -100.00%\n",
      "epoch: 313, step: 1884 train/loss: 0.0193, train/tfacc: 0.9987, val/loss: 0.2063, val/tfacc: 0.9183, 🤖\n",
      "epoch 313: loss 0.1764, time 1109.29ms, mfu -100.00%\n",
      "epoch: 314, step: 1890 train/loss: 0.0153, train/tfacc: 0.9993, val/loss: 0.1953, val/tfacc: 0.9231, 🤖\n",
      "epoch 314: loss 0.1551, time 1181.38ms, mfu -100.00%\n",
      "epoch: 315, step: 1896 train/loss: 0.0174, train/tfacc: 0.9992, val/loss: 0.1973, val/tfacc: 0.9224, 🤖\n",
      "epoch 315: loss 0.1693, time 1055.09ms, mfu -100.00%\n",
      "epoch: 316, step: 1902 train/loss: 0.0225, train/tfacc: 0.9974, val/loss: 0.2067, val/tfacc: 0.9183, 🤖\n",
      "epoch 316: loss 0.1597, time 1362.03ms, mfu -100.00%\n",
      "epoch: 317, step: 1908 train/loss: 0.0157, train/tfacc: 0.9995, val/loss: 0.1908, val/tfacc: 0.9250, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 317: loss 0.1747, time 1392.58ms, mfu -100.00%\n",
      "epoch: 318, step: 1914 train/loss: 0.0210, train/tfacc: 0.9982, val/loss: 0.2057, val/tfacc: 0.9184, 🤖\n",
      "epoch 318: loss 0.1819, time 1175.04ms, mfu -100.00%\n",
      "epoch: 319, step: 1920 train/loss: 0.0177, train/tfacc: 0.9987, val/loss: 0.1998, val/tfacc: 0.9213, 🤖\n",
      "epoch 319: loss 0.1578, time 1151.64ms, mfu -100.00%\n",
      "epoch: 320, step: 1926 train/loss: 0.0155, train/tfacc: 0.9992, val/loss: 0.1924, val/tfacc: 0.9239, 🤖\n",
      "epoch 320: loss 0.1747, time 1249.96ms, mfu -100.00%\n",
      "epoch: 321, step: 1932 train/loss: 0.0188, train/tfacc: 0.9988, val/loss: 0.2052, val/tfacc: 0.9185, 🤖\n",
      "epoch 321: loss 0.1618, time 1183.78ms, mfu -100.00%\n",
      "epoch: 322, step: 1938 train/loss: 0.0161, train/tfacc: 0.9985, val/loss: 0.2006, val/tfacc: 0.9220, 🤖\n",
      "epoch 322: loss 0.1406, time 1223.55ms, mfu -100.00%\n",
      "epoch: 323, step: 1944 train/loss: 0.0168, train/tfacc: 0.9988, val/loss: 0.1948, val/tfacc: 0.9238, 🤖\n",
      "epoch 323: loss 0.1291, time 1266.09ms, mfu -100.00%\n",
      "epoch: 324, step: 1950 train/loss: 0.0180, train/tfacc: 0.9982, val/loss: 0.2114, val/tfacc: 0.9175, 🤖\n",
      "epoch 324: loss 0.1528, time 1237.79ms, mfu -100.00%\n",
      "epoch: 325, step: 1956 train/loss: 0.0190, train/tfacc: 0.9983, val/loss: 0.2090, val/tfacc: 0.9176, 🤖\n",
      "epoch 325: loss 0.1695, time 1239.99ms, mfu -100.00%\n",
      "epoch: 326, step: 1962 train/loss: 0.0167, train/tfacc: 0.9990, val/loss: 0.1987, val/tfacc: 0.9217, 🤖\n",
      "epoch 326: loss 0.1634, time 1158.95ms, mfu -100.00%\n",
      "epoch: 327, step: 1968 train/loss: 0.0170, train/tfacc: 0.9988, val/loss: 0.2105, val/tfacc: 0.9182, 🤖\n",
      "epoch 327: loss 0.1449, time 1129.91ms, mfu -100.00%\n",
      "epoch: 328, step: 1974 train/loss: 0.0196, train/tfacc: 0.9978, val/loss: 0.2188, val/tfacc: 0.9135, 🤖\n",
      "epoch 328: loss 0.1846, time 1255.62ms, mfu -100.00%\n",
      "epoch: 329, step: 1980 train/loss: 0.0159, train/tfacc: 0.9988, val/loss: 0.1936, val/tfacc: 0.9235, 🤖\n",
      "epoch 329: loss 0.1749, time 1442.54ms, mfu -100.00%\n",
      "epoch: 330, step: 1986 train/loss: 0.0157, train/tfacc: 0.9992, val/loss: 0.1911, val/tfacc: 0.9246, 🤖\n",
      "epoch 330: loss 0.1582, time 1372.40ms, mfu -100.00%\n",
      "epoch: 331, step: 1992 train/loss: 0.0161, train/tfacc: 0.9991, val/loss: 0.1962, val/tfacc: 0.9229, 🤖\n",
      "epoch 331: loss 0.1427, time 1334.15ms, mfu -100.00%\n",
      "epoch: 332, step: 1998 train/loss: 0.0176, train/tfacc: 0.9985, val/loss: 0.2018, val/tfacc: 0.9204, 🤖\n",
      "epoch 332: loss 0.1572, time 1302.45ms, mfu -100.00%\n",
      "epoch: 333, step: 2004 train/loss: 0.0203, train/tfacc: 0.9972, val/loss: 0.2062, val/tfacc: 0.9195, 🤖\n",
      "epoch 333: loss 0.1390, time 1279.46ms, mfu -100.00%\n",
      "epoch: 334, step: 2010 train/loss: 0.0166, train/tfacc: 0.9990, val/loss: 0.1970, val/tfacc: 0.9225, 🤖\n",
      "epoch 334: loss 0.1901, time 1357.99ms, mfu -100.00%\n",
      "epoch: 335, step: 2016 train/loss: 0.0187, train/tfacc: 0.9987, val/loss: 0.2061, val/tfacc: 0.9194, 🤖\n",
      "epoch 335: loss 0.1844, time 1467.88ms, mfu -100.00%\n",
      "epoch: 336, step: 2022 train/loss: 0.0160, train/tfacc: 0.9988, val/loss: 0.1940, val/tfacc: 0.9234, 🤖\n",
      "epoch 336: loss 0.1620, time 1340.24ms, mfu -100.00%\n",
      "epoch: 337, step: 2028 train/loss: 0.0164, train/tfacc: 0.9990, val/loss: 0.2021, val/tfacc: 0.9204, 🤖\n",
      "epoch 337: loss 0.1763, time 1388.67ms, mfu -100.00%\n",
      "epoch: 338, step: 2034 train/loss: 0.0156, train/tfacc: 0.9991, val/loss: 0.1945, val/tfacc: 0.9249, 🤖\n",
      "epoch 338: loss 0.1532, time 1213.16ms, mfu -100.00%\n",
      "epoch: 339, step: 2040 train/loss: 0.0177, train/tfacc: 0.9983, val/loss: 0.1967, val/tfacc: 0.9223, 🤖\n",
      "epoch 339: loss 0.1640, time 1263.62ms, mfu -100.00%\n",
      "epoch: 340, step: 2046 train/loss: 0.0172, train/tfacc: 0.9980, val/loss: 0.1992, val/tfacc: 0.9208, 🤖\n",
      "epoch 340: loss 0.1455, time 1245.69ms, mfu -100.00%\n",
      "epoch: 341, step: 2052 train/loss: 0.0151, train/tfacc: 0.9992, val/loss: 0.1951, val/tfacc: 0.9240, 🤖\n",
      "epoch 341: loss 0.1612, time 1329.54ms, mfu -100.00%\n",
      "epoch: 342, step: 2058 train/loss: 0.0143, train/tfacc: 0.9992, val/loss: 0.1933, val/tfacc: 0.9238, 🤖\n",
      "epoch 342: loss 0.1494, time 1377.80ms, mfu -100.00%\n",
      "epoch: 343, step: 2064 train/loss: 0.0166, train/tfacc: 0.9985, val/loss: 0.1941, val/tfacc: 0.9237, 🤖\n",
      "epoch 343: loss 0.1449, time 1251.83ms, mfu -100.00%\n",
      "epoch: 344, step: 2070 train/loss: 0.0146, train/tfacc: 0.9988, val/loss: 0.1919, val/tfacc: 0.9251, 🤖\n",
      "epoch 344: loss 0.1273, time 1435.86ms, mfu -100.00%\n",
      "epoch: 345, step: 2076 train/loss: 0.0151, train/tfacc: 0.9993, val/loss: 0.1992, val/tfacc: 0.9210, 🤖\n",
      "epoch 345: loss 0.1435, time 1341.69ms, mfu -100.00%\n",
      "epoch: 346, step: 2082 train/loss: 0.0149, train/tfacc: 0.9988, val/loss: 0.1896, val/tfacc: 0.9253, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 346: loss 0.1195, time 1276.31ms, mfu -100.00%\n",
      "epoch: 347, step: 2088 train/loss: 0.0153, train/tfacc: 0.9991, val/loss: 0.1965, val/tfacc: 0.9234, 🤖\n",
      "epoch 347: loss 0.1597, time 1300.54ms, mfu -100.00%\n",
      "epoch: 348, step: 2094 train/loss: 0.0144, train/tfacc: 0.9992, val/loss: 0.1920, val/tfacc: 0.9247, 🤖\n",
      "epoch 348: loss 0.1599, time 1365.86ms, mfu -100.00%\n",
      "epoch: 349, step: 2100 train/loss: 0.0159, train/tfacc: 0.9989, val/loss: 0.2013, val/tfacc: 0.9203, 🤖\n",
      "epoch 349: loss 0.1557, time 1386.67ms, mfu -100.00%\n",
      "epoch: 350, step: 2106 train/loss: 0.0139, train/tfacc: 0.9995, val/loss: 0.1919, val/tfacc: 0.9246, 🤖\n",
      "epoch 350: loss 0.1364, time 1300.99ms, mfu -100.00%\n",
      "epoch: 351, step: 2112 train/loss: 0.0169, train/tfacc: 0.9989, val/loss: 0.2069, val/tfacc: 0.9190, 🤖\n",
      "epoch 351: loss 0.1539, time 1250.42ms, mfu -100.00%\n",
      "epoch: 352, step: 2118 train/loss: 0.0144, train/tfacc: 0.9991, val/loss: 0.1942, val/tfacc: 0.9242, 🤖\n",
      "epoch 352: loss 0.1335, time 1230.70ms, mfu -100.00%\n",
      "epoch: 353, step: 2124 train/loss: 0.0172, train/tfacc: 0.9984, val/loss: 0.2029, val/tfacc: 0.9206, 🤖\n",
      "epoch 353: loss 0.1337, time 1311.18ms, mfu -100.00%\n",
      "epoch: 354, step: 2130 train/loss: 0.0167, train/tfacc: 0.9987, val/loss: 0.2011, val/tfacc: 0.9206, 🤖\n",
      "epoch 354: loss 0.1726, time 1256.69ms, mfu -100.00%\n",
      "epoch: 355, step: 2136 train/loss: 0.0146, train/tfacc: 0.9992, val/loss: 0.1975, val/tfacc: 0.9219, 🤖\n",
      "epoch 355: loss 0.1440, time 1353.85ms, mfu -100.00%\n",
      "epoch: 356, step: 2142 train/loss: 0.0137, train/tfacc: 0.9996, val/loss: 0.1927, val/tfacc: 0.9234, 🤖\n",
      "epoch 356: loss 0.1396, time 1332.19ms, mfu -100.00%\n",
      "epoch: 357, step: 2148 train/loss: 0.0156, train/tfacc: 0.9984, val/loss: 0.1962, val/tfacc: 0.9227, 🤖\n",
      "epoch 357: loss 0.1391, time 1273.01ms, mfu -100.00%\n",
      "epoch: 358, step: 2154 train/loss: 0.0152, train/tfacc: 0.9991, val/loss: 0.1976, val/tfacc: 0.9232, 🤖\n",
      "epoch 358: loss 0.1466, time 1332.80ms, mfu -100.00%\n",
      "epoch: 359, step: 2160 train/loss: 0.0157, train/tfacc: 0.9988, val/loss: 0.2007, val/tfacc: 0.9216, 🤖\n",
      "epoch 359: loss 0.1606, time 1403.76ms, mfu -100.00%\n",
      "epoch: 360, step: 2166 train/loss: 0.0131, train/tfacc: 0.9996, val/loss: 0.1866, val/tfacc: 0.9264, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 360: loss 0.1636, time 1573.86ms, mfu -100.00%\n",
      "epoch: 361, step: 2172 train/loss: 0.0141, train/tfacc: 0.9995, val/loss: 0.1982, val/tfacc: 0.9219, 🤖\n",
      "epoch 361: loss 0.1374, time 1426.07ms, mfu -100.00%\n",
      "epoch: 362, step: 2178 train/loss: 0.0154, train/tfacc: 0.9985, val/loss: 0.1987, val/tfacc: 0.9227, 🤖\n",
      "epoch 362: loss 0.1530, time 1305.41ms, mfu -100.00%\n",
      "epoch: 363, step: 2184 train/loss: 0.0131, train/tfacc: 0.9995, val/loss: 0.1864, val/tfacc: 0.9282, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 363: loss 0.1462, time 1257.89ms, mfu -100.00%\n",
      "epoch: 364, step: 2190 train/loss: 0.0137, train/tfacc: 0.9991, val/loss: 0.1906, val/tfacc: 0.9241, 🤖\n",
      "epoch 364: loss 0.1670, time 1365.51ms, mfu -100.00%\n",
      "epoch: 365, step: 2196 train/loss: 0.0141, train/tfacc: 0.9990, val/loss: 0.1940, val/tfacc: 0.9241, 🤖\n",
      "epoch 365: loss 0.1141, time 1188.72ms, mfu -100.00%\n",
      "epoch: 366, step: 2202 train/loss: 0.0128, train/tfacc: 0.9991, val/loss: 0.1864, val/tfacc: 0.9262, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 366: loss 0.1572, time 1367.76ms, mfu -100.00%\n",
      "epoch: 367, step: 2208 train/loss: 0.0130, train/tfacc: 0.9992, val/loss: 0.1937, val/tfacc: 0.9236, 🤖\n",
      "epoch 367: loss 0.1695, time 1192.94ms, mfu -100.00%\n",
      "epoch: 368, step: 2214 train/loss: 0.0136, train/tfacc: 0.9995, val/loss: 0.2046, val/tfacc: 0.9203, 🤖\n",
      "epoch 368: loss 0.1618, time 1441.73ms, mfu -100.00%\n",
      "epoch: 369, step: 2220 train/loss: 0.0118, train/tfacc: 0.9996, val/loss: 0.1905, val/tfacc: 0.9253, 🤖\n",
      "epoch 369: loss 0.1297, time 1360.11ms, mfu -100.00%\n",
      "epoch: 370, step: 2226 train/loss: 0.0176, train/tfacc: 0.9977, val/loss: 0.2128, val/tfacc: 0.9180, 🤖\n",
      "epoch 370: loss 0.1392, time 1270.28ms, mfu -100.00%\n",
      "epoch: 371, step: 2232 train/loss: 0.0146, train/tfacc: 0.9988, val/loss: 0.2016, val/tfacc: 0.9219, 🤖\n",
      "epoch 371: loss 0.1348, time 1391.65ms, mfu -100.00%\n",
      "epoch: 372, step: 2238 train/loss: 0.0113, train/tfacc: 0.9993, val/loss: 0.1795, val/tfacc: 0.9292, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 372: loss 0.1467, time 1594.50ms, mfu -100.00%\n",
      "epoch: 373, step: 2244 train/loss: 0.0117, train/tfacc: 0.9995, val/loss: 0.1883, val/tfacc: 0.9256, 🤖\n",
      "epoch 373: loss 0.1808, time 1351.26ms, mfu -100.00%\n",
      "epoch: 374, step: 2250 train/loss: 0.0125, train/tfacc: 0.9995, val/loss: 0.1950, val/tfacc: 0.9242, 🤖\n",
      "epoch 374: loss 0.1373, time 1280.42ms, mfu -100.00%\n",
      "epoch: 375, step: 2256 train/loss: 0.0111, train/tfacc: 0.9996, val/loss: 0.1856, val/tfacc: 0.9265, 🤖\n",
      "epoch 375: loss 0.1355, time 1289.95ms, mfu -100.00%\n",
      "epoch: 376, step: 2262 train/loss: 0.0115, train/tfacc: 0.9995, val/loss: 0.1826, val/tfacc: 0.9279, 🤖\n",
      "epoch 376: loss 0.1377, time 1344.06ms, mfu -100.00%\n",
      "epoch: 377, step: 2268 train/loss: 0.0134, train/tfacc: 0.9991, val/loss: 0.1942, val/tfacc: 0.9236, 🤖\n",
      "epoch 377: loss 0.1461, time 1267.18ms, mfu -100.00%\n",
      "epoch: 378, step: 2274 train/loss: 0.0132, train/tfacc: 0.9992, val/loss: 0.1987, val/tfacc: 0.9236, 🤖\n",
      "epoch 378: loss 0.1404, time 1444.08ms, mfu -100.00%\n",
      "epoch: 379, step: 2280 train/loss: 0.0135, train/tfacc: 0.9990, val/loss: 0.2000, val/tfacc: 0.9222, 🤖\n",
      "epoch 379: loss 0.1438, time 1223.61ms, mfu -100.00%\n",
      "epoch: 380, step: 2286 train/loss: 0.0108, train/tfacc: 0.9991, val/loss: 0.1849, val/tfacc: 0.9289, 🤖\n",
      "epoch 380: loss 0.1352, time 1247.32ms, mfu -100.00%\n",
      "epoch: 381, step: 2292 train/loss: 0.0130, train/tfacc: 0.9991, val/loss: 0.1966, val/tfacc: 0.9243, 🤖\n",
      "epoch 381: loss 0.1175, time 1238.24ms, mfu -100.00%\n",
      "epoch: 382, step: 2298 train/loss: 0.0123, train/tfacc: 0.9991, val/loss: 0.1964, val/tfacc: 0.9235, 🤖\n",
      "epoch 382: loss 0.1707, time 1347.26ms, mfu -100.00%\n",
      "epoch: 383, step: 2304 train/loss: 0.0118, train/tfacc: 1.0000, val/loss: 0.1881, val/tfacc: 0.9262, 🤖\n",
      "epoch 383: loss 0.1492, time 1285.34ms, mfu -100.00%\n",
      "epoch: 384, step: 2310 train/loss: 0.0146, train/tfacc: 0.9990, val/loss: 0.1967, val/tfacc: 0.9235, 🤖\n",
      "epoch 384: loss 0.1220, time 1287.62ms, mfu -100.00%\n",
      "epoch: 385, step: 2316 train/loss: 0.0114, train/tfacc: 0.9993, val/loss: 0.1893, val/tfacc: 0.9268, 🤖\n",
      "epoch 385: loss 0.1329, time 1181.95ms, mfu -100.00%\n",
      "epoch: 386, step: 2322 train/loss: 0.0120, train/tfacc: 0.9991, val/loss: 0.1911, val/tfacc: 0.9255, 🤖\n",
      "epoch 386: loss 0.1472, time 1440.58ms, mfu -100.00%\n",
      "epoch: 387, step: 2328 train/loss: 0.0113, train/tfacc: 0.9995, val/loss: 0.1856, val/tfacc: 0.9276, 🤖\n",
      "epoch 387: loss 0.1306, time 1301.03ms, mfu -100.00%\n",
      "epoch: 388, step: 2334 train/loss: 0.0098, train/tfacc: 0.9999, val/loss: 0.1845, val/tfacc: 0.9276, 🤖\n",
      "epoch 388: loss 0.1439, time 1333.85ms, mfu -100.00%\n",
      "epoch: 389, step: 2340 train/loss: 0.0110, train/tfacc: 0.9993, val/loss: 0.1863, val/tfacc: 0.9281, 🤖\n",
      "epoch 389: loss 0.1504, time 1435.46ms, mfu -100.00%\n",
      "epoch: 390, step: 2346 train/loss: 0.0112, train/tfacc: 0.9996, val/loss: 0.1803, val/tfacc: 0.9293, 🤖\n",
      "epoch 390: loss 0.1684, time 1397.62ms, mfu -100.00%\n",
      "epoch: 391, step: 2352 train/loss: 0.0131, train/tfacc: 0.9988, val/loss: 0.2016, val/tfacc: 0.9223, 🤖\n",
      "epoch 391: loss 0.1445, time 1458.76ms, mfu -100.00%\n",
      "epoch: 392, step: 2358 train/loss: 0.0115, train/tfacc: 0.9999, val/loss: 0.1920, val/tfacc: 0.9256, 🤖\n",
      "epoch 392: loss 0.1363, time 1263.72ms, mfu -100.00%\n",
      "epoch: 393, step: 2364 train/loss: 0.0110, train/tfacc: 0.9997, val/loss: 0.1842, val/tfacc: 0.9265, 🤖\n",
      "epoch 393: loss 0.1262, time 1296.45ms, mfu -100.00%\n",
      "epoch: 394, step: 2370 train/loss: 0.0095, train/tfacc: 0.9992, val/loss: 0.1798, val/tfacc: 0.9299, 🤖\n",
      "epoch 394: loss 0.1193, time 1242.54ms, mfu -100.00%\n",
      "epoch: 395, step: 2376 train/loss: 0.0106, train/tfacc: 0.9991, val/loss: 0.1841, val/tfacc: 0.9285, 🤖\n",
      "epoch 395: loss 0.1179, time 1208.31ms, mfu -100.00%\n",
      "epoch: 396, step: 2382 train/loss: 0.0122, train/tfacc: 0.9992, val/loss: 0.1950, val/tfacc: 0.9237, 🤖\n",
      "epoch 396: loss 0.1181, time 1168.69ms, mfu -100.00%\n",
      "epoch: 397, step: 2388 train/loss: 0.0121, train/tfacc: 0.9996, val/loss: 0.1983, val/tfacc: 0.9222, 🤖\n",
      "epoch 397: loss 0.1133, time 1108.21ms, mfu -100.00%\n",
      "epoch: 398, step: 2394 train/loss: 0.0104, train/tfacc: 0.9995, val/loss: 0.1922, val/tfacc: 0.9254, 🤖\n",
      "epoch 398: loss 0.1272, time 1180.79ms, mfu -100.00%\n",
      "epoch: 399, step: 2400 train/loss: 0.0121, train/tfacc: 0.9992, val/loss: 0.1946, val/tfacc: 0.9234, 🤖\n",
      "epoch 399: loss 0.1325, time 1066.36ms, mfu -100.00%\n",
      "epoch: 400, step: 2406 train/loss: 0.0099, train/tfacc: 0.9996, val/loss: 0.1818, val/tfacc: 0.9295, 🤖\n",
      "epoch 400: loss 0.1659, time 1132.17ms, mfu -100.00%\n",
      "epoch: 401, step: 2412 train/loss: 0.0098, train/tfacc: 0.9997, val/loss: 0.1897, val/tfacc: 0.9267, 🤖\n",
      "epoch 401: loss 0.1324, time 1120.06ms, mfu -100.00%\n",
      "epoch: 402, step: 2418 train/loss: 0.0135, train/tfacc: 0.9987, val/loss: 0.2019, val/tfacc: 0.9217, 🤖\n",
      "epoch 402: loss 0.1262, time 1152.10ms, mfu -100.00%\n",
      "epoch: 403, step: 2424 train/loss: 0.0108, train/tfacc: 0.9992, val/loss: 0.1897, val/tfacc: 0.9271, 🤖\n",
      "epoch 403: loss 0.1456, time 1159.39ms, mfu -100.00%\n",
      "epoch: 404, step: 2430 train/loss: 0.0104, train/tfacc: 0.9992, val/loss: 0.1879, val/tfacc: 0.9275, 🤖\n",
      "epoch 404: loss 0.1481, time 1305.56ms, mfu -100.00%\n",
      "epoch: 405, step: 2436 train/loss: 0.0098, train/tfacc: 0.9999, val/loss: 0.1821, val/tfacc: 0.9283, 🤖\n",
      "epoch 405: loss 0.1274, time 1147.96ms, mfu -100.00%\n",
      "epoch: 406, step: 2442 train/loss: 0.0091, train/tfacc: 0.9999, val/loss: 0.1785, val/tfacc: 0.9304, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 406: loss 0.1316, time 1248.14ms, mfu -100.00%\n",
      "epoch: 407, step: 2448 train/loss: 0.0105, train/tfacc: 0.9996, val/loss: 0.1881, val/tfacc: 0.9263, 🤖\n",
      "epoch 407: loss 0.1261, time 1186.68ms, mfu -100.00%\n",
      "epoch: 408, step: 2454 train/loss: 0.0089, train/tfacc: 0.9999, val/loss: 0.1823, val/tfacc: 0.9293, 🤖\n",
      "epoch 408: loss 0.1240, time 1243.64ms, mfu -100.00%\n",
      "epoch: 409, step: 2460 train/loss: 0.0133, train/tfacc: 0.9982, val/loss: 0.1988, val/tfacc: 0.9239, 🤖\n",
      "epoch 409: loss 0.1049, time 1115.96ms, mfu -100.00%\n",
      "epoch: 410, step: 2466 train/loss: 0.0105, train/tfacc: 0.9996, val/loss: 0.1891, val/tfacc: 0.9273, 🤖\n",
      "epoch 410: loss 0.1469, time 1100.48ms, mfu -100.00%\n",
      "epoch: 411, step: 2472 train/loss: 0.0118, train/tfacc: 0.9993, val/loss: 0.1897, val/tfacc: 0.9267, 🤖\n",
      "epoch 411: loss 0.1247, time 1100.09ms, mfu -100.00%\n",
      "epoch: 412, step: 2478 train/loss: 0.0108, train/tfacc: 0.9991, val/loss: 0.1884, val/tfacc: 0.9269, 🤖\n",
      "epoch 412: loss 0.1352, time 1140.05ms, mfu -100.00%\n",
      "epoch: 413, step: 2484 train/loss: 0.0098, train/tfacc: 0.9996, val/loss: 0.1898, val/tfacc: 0.9269, 🤖\n",
      "epoch 413: loss 0.1294, time 1132.87ms, mfu -100.00%\n",
      "epoch: 414, step: 2490 train/loss: 0.0096, train/tfacc: 0.9995, val/loss: 0.1791, val/tfacc: 0.9306, 🤖\n",
      "epoch 414: loss 0.1343, time 1116.74ms, mfu -100.00%\n",
      "epoch: 415, step: 2496 train/loss: 0.0115, train/tfacc: 0.9993, val/loss: 0.1851, val/tfacc: 0.9286, 🤖\n",
      "epoch 415: loss 0.1229, time 1162.68ms, mfu -100.00%\n",
      "epoch: 416, step: 2502 train/loss: 0.0104, train/tfacc: 0.9992, val/loss: 0.1863, val/tfacc: 0.9287, 🤖\n",
      "epoch 416: loss 0.1316, time 1160.29ms, mfu -100.00%\n",
      "epoch: 417, step: 2508 train/loss: 0.0083, train/tfacc: 0.9999, val/loss: 0.1797, val/tfacc: 0.9310, 🤖\n",
      "epoch 417: loss 0.1399, time 1127.81ms, mfu -100.00%\n",
      "epoch: 418, step: 2514 train/loss: 0.0098, train/tfacc: 0.9995, val/loss: 0.1854, val/tfacc: 0.9278, 🤖\n",
      "epoch 418: loss 0.1134, time 1125.85ms, mfu -100.00%\n",
      "epoch: 419, step: 2520 train/loss: 0.0082, train/tfacc: 0.9999, val/loss: 0.1758, val/tfacc: 0.9313, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 419: loss 0.1230, time 1233.48ms, mfu -100.00%\n",
      "epoch: 420, step: 2526 train/loss: 0.0101, train/tfacc: 0.9996, val/loss: 0.1870, val/tfacc: 0.9276, 🤖\n",
      "epoch 420: loss 0.1171, time 1170.97ms, mfu -100.00%\n",
      "epoch: 421, step: 2532 train/loss: 0.0091, train/tfacc: 0.9999, val/loss: 0.1901, val/tfacc: 0.9260, 🤖\n",
      "epoch 421: loss 0.1243, time 1236.45ms, mfu -100.00%\n",
      "epoch: 422, step: 2538 train/loss: 0.0089, train/tfacc: 0.9999, val/loss: 0.1866, val/tfacc: 0.9292, 🤖\n",
      "epoch 422: loss 0.1239, time 1382.21ms, mfu -100.00%\n",
      "epoch: 423, step: 2544 train/loss: 0.0129, train/tfacc: 0.9988, val/loss: 0.1955, val/tfacc: 0.9235, 🤖\n",
      "epoch 423: loss 0.1279, time 1225.00ms, mfu -100.00%\n",
      "epoch: 424, step: 2550 train/loss: 0.0076, train/tfacc: 1.0000, val/loss: 0.1753, val/tfacc: 0.9315, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 424: loss 0.1319, time 1459.40ms, mfu -100.00%\n",
      "epoch: 425, step: 2556 train/loss: 0.0134, train/tfacc: 0.9975, val/loss: 0.2164, val/tfacc: 0.9198, 🤖\n",
      "epoch 425: loss 0.1400, time 1516.11ms, mfu -100.00%\n",
      "epoch: 426, step: 2562 train/loss: 0.0117, train/tfacc: 0.9990, val/loss: 0.1941, val/tfacc: 0.9254, 🤖\n",
      "epoch 426: loss 0.1224, time 1195.77ms, mfu -100.00%\n",
      "epoch: 427, step: 2568 train/loss: 0.0128, train/tfacc: 0.9982, val/loss: 0.1962, val/tfacc: 0.9245, 🤖\n",
      "epoch 427: loss 0.1187, time 1374.95ms, mfu -100.00%\n",
      "epoch: 428, step: 2574 train/loss: 0.0113, train/tfacc: 0.9990, val/loss: 0.1993, val/tfacc: 0.9228, 🤖\n",
      "epoch 428: loss 0.1352, time 1337.77ms, mfu -100.00%\n",
      "epoch: 429, step: 2580 train/loss: 0.0089, train/tfacc: 0.9996, val/loss: 0.1857, val/tfacc: 0.9293, 🤖\n",
      "epoch 429: loss 0.1464, time 1294.51ms, mfu -100.00%\n",
      "epoch: 430, step: 2586 train/loss: 0.0093, train/tfacc: 0.9997, val/loss: 0.1818, val/tfacc: 0.9304, 🤖\n",
      "epoch 430: loss 0.1343, time 1263.04ms, mfu -100.00%\n",
      "epoch: 431, step: 2592 train/loss: 0.0109, train/tfacc: 0.9989, val/loss: 0.1992, val/tfacc: 0.9238, 🤖\n",
      "epoch 431: loss 0.1259, time 1255.45ms, mfu -100.00%\n",
      "epoch: 432, step: 2598 train/loss: 0.0091, train/tfacc: 0.9996, val/loss: 0.1862, val/tfacc: 0.9291, 🤖\n",
      "epoch 432: loss 0.1380, time 1150.87ms, mfu -100.00%\n",
      "epoch: 433, step: 2604 train/loss: 0.0110, train/tfacc: 0.9991, val/loss: 0.1960, val/tfacc: 0.9250, 🤖\n",
      "epoch 433: loss 0.1385, time 1301.80ms, mfu -100.00%\n",
      "epoch: 434, step: 2610 train/loss: 0.0124, train/tfacc: 0.9989, val/loss: 0.2098, val/tfacc: 0.9198, 🤖\n",
      "epoch 434: loss 0.1488, time 1173.81ms, mfu -100.00%\n",
      "epoch: 435, step: 2616 train/loss: 0.0132, train/tfacc: 0.9993, val/loss: 0.2156, val/tfacc: 0.9192, 🤖\n",
      "epoch 435: loss 0.1538, time 1200.20ms, mfu -100.00%\n",
      "epoch: 436, step: 2622 train/loss: 0.0087, train/tfacc: 0.9999, val/loss: 0.1921, val/tfacc: 0.9260, 🤖\n",
      "epoch 436: loss 0.1394, time 1145.57ms, mfu -100.00%\n",
      "epoch: 437, step: 2628 train/loss: 0.0085, train/tfacc: 0.9997, val/loss: 0.1881, val/tfacc: 0.9275, 🤖\n",
      "epoch 437: loss 0.1349, time 1460.06ms, mfu -100.00%\n",
      "epoch: 438, step: 2634 train/loss: 0.0095, train/tfacc: 0.9996, val/loss: 0.1925, val/tfacc: 0.9268, 🤖\n",
      "epoch 438: loss 0.1288, time 1348.17ms, mfu -100.00%\n",
      "epoch: 439, step: 2640 train/loss: 0.0088, train/tfacc: 0.9993, val/loss: 0.1838, val/tfacc: 0.9289, 🤖\n",
      "epoch 439: loss 0.1231, time 1232.16ms, mfu -100.00%\n",
      "epoch: 440, step: 2646 train/loss: 0.0076, train/tfacc: 0.9995, val/loss: 0.1811, val/tfacc: 0.9305, 🤖\n",
      "epoch 440: loss 0.1262, time 1225.81ms, mfu -100.00%\n",
      "epoch: 441, step: 2652 train/loss: 0.0098, train/tfacc: 0.9995, val/loss: 0.1952, val/tfacc: 0.9253, 🤖\n",
      "epoch 441: loss 0.1206, time 1262.67ms, mfu -100.00%\n",
      "epoch: 442, step: 2658 train/loss: 0.0118, train/tfacc: 0.9993, val/loss: 0.2025, val/tfacc: 0.9222, 🤖\n",
      "epoch 442: loss 0.1280, time 1189.43ms, mfu -100.00%\n",
      "epoch: 443, step: 2664 train/loss: 0.0098, train/tfacc: 0.9996, val/loss: 0.1842, val/tfacc: 0.9292, 🤖\n",
      "epoch 443: loss 0.0937, time 1370.22ms, mfu -100.00%\n",
      "epoch: 444, step: 2670 train/loss: 0.0097, train/tfacc: 0.9993, val/loss: 0.1885, val/tfacc: 0.9284, 🤖\n",
      "epoch 444: loss 0.1379, time 1147.98ms, mfu -100.00%\n",
      "epoch: 445, step: 2676 train/loss: 0.0073, train/tfacc: 1.0000, val/loss: 0.1808, val/tfacc: 0.9308, 🤖\n",
      "epoch 445: loss 0.1288, time 1064.83ms, mfu -100.00%\n",
      "epoch: 446, step: 2682 train/loss: 0.0093, train/tfacc: 0.9995, val/loss: 0.1888, val/tfacc: 0.9281, 🤖\n",
      "epoch 446: loss 0.1222, time 1175.93ms, mfu -100.00%\n",
      "epoch: 447, step: 2688 train/loss: 0.0086, train/tfacc: 0.9999, val/loss: 0.1837, val/tfacc: 0.9289, 🤖\n",
      "epoch 447: loss 0.1315, time 1126.20ms, mfu -100.00%\n",
      "epoch: 448, step: 2694 train/loss: 0.0073, train/tfacc: 0.9999, val/loss: 0.1734, val/tfacc: 0.9328, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 448: loss 0.1181, time 1193.95ms, mfu -100.00%\n",
      "epoch: 449, step: 2700 train/loss: 0.0074, train/tfacc: 0.9997, val/loss: 0.1745, val/tfacc: 0.9329, 🤖\n",
      "epoch 449: loss 0.1103, time 1161.90ms, mfu -100.00%\n",
      "epoch: 450, step: 2706 train/loss: 0.0073, train/tfacc: 0.9997, val/loss: 0.1756, val/tfacc: 0.9331, 🤖\n",
      "epoch 450: loss 0.1243, time 1148.56ms, mfu -100.00%\n",
      "epoch: 451, step: 2712 train/loss: 0.0072, train/tfacc: 0.9999, val/loss: 0.1784, val/tfacc: 0.9312, 🤖\n",
      "epoch 451: loss 0.1126, time 1162.78ms, mfu -100.00%\n",
      "epoch: 452, step: 2718 train/loss: 0.0080, train/tfacc: 0.9999, val/loss: 0.1831, val/tfacc: 0.9306, 🤖\n",
      "epoch 452: loss 0.1394, time 1221.98ms, mfu -100.00%\n",
      "epoch: 453, step: 2724 train/loss: 0.0087, train/tfacc: 0.9997, val/loss: 0.1862, val/tfacc: 0.9290, 🤖\n",
      "epoch 453: loss 0.1043, time 1168.96ms, mfu -100.00%\n",
      "epoch: 454, step: 2730 train/loss: 0.0093, train/tfacc: 0.9995, val/loss: 0.1858, val/tfacc: 0.9285, 🤖\n",
      "epoch 454: loss 0.1524, time 1146.45ms, mfu -100.00%\n",
      "epoch: 455, step: 2736 train/loss: 0.0077, train/tfacc: 0.9997, val/loss: 0.1875, val/tfacc: 0.9280, 🤖\n",
      "epoch 455: loss 0.1213, time 1149.66ms, mfu -100.00%\n",
      "epoch: 456, step: 2742 train/loss: 0.0084, train/tfacc: 0.9997, val/loss: 0.1914, val/tfacc: 0.9261, 🤖\n",
      "epoch 456: loss 0.1240, time 1231.79ms, mfu -100.00%\n",
      "epoch: 457, step: 2748 train/loss: 0.0076, train/tfacc: 0.9999, val/loss: 0.1923, val/tfacc: 0.9271, 🤖\n",
      "epoch 457: loss 0.1050, time 1131.87ms, mfu -100.00%\n",
      "epoch: 458, step: 2754 train/loss: 0.0076, train/tfacc: 0.9997, val/loss: 0.1840, val/tfacc: 0.9288, 🤖\n",
      "epoch 458: loss 0.1101, time 1164.78ms, mfu -100.00%\n",
      "epoch: 459, step: 2760 train/loss: 0.0074, train/tfacc: 0.9996, val/loss: 0.1830, val/tfacc: 0.9298, 🤖\n",
      "epoch 459: loss 0.1109, time 1172.49ms, mfu -100.00%\n",
      "epoch: 460, step: 2766 train/loss: 0.0072, train/tfacc: 0.9996, val/loss: 0.1810, val/tfacc: 0.9300, 🤖\n",
      "epoch 460: loss 0.1155, time 1155.05ms, mfu -100.00%\n",
      "epoch: 461, step: 2772 train/loss: 0.0064, train/tfacc: 1.0000, val/loss: 0.1813, val/tfacc: 0.9299, 🤖\n",
      "epoch 461: loss 0.1146, time 1240.42ms, mfu -100.00%\n",
      "epoch: 462, step: 2778 train/loss: 0.0070, train/tfacc: 0.9997, val/loss: 0.1855, val/tfacc: 0.9285, 🤖\n",
      "epoch 462: loss 0.1135, time 1253.68ms, mfu -100.00%\n",
      "epoch: 463, step: 2784 train/loss: 0.0076, train/tfacc: 0.9997, val/loss: 0.1857, val/tfacc: 0.9295, 🤖\n",
      "epoch 463: loss 0.1094, time 1132.00ms, mfu -100.00%\n",
      "epoch: 464, step: 2790 train/loss: 0.0066, train/tfacc: 1.0000, val/loss: 0.1737, val/tfacc: 0.9325, 🤖\n",
      "epoch 464: loss 0.1113, time 1280.62ms, mfu -100.00%\n",
      "epoch: 465, step: 2796 train/loss: 0.0089, train/tfacc: 0.9995, val/loss: 0.1855, val/tfacc: 0.9287, 🤖\n",
      "epoch 465: loss 0.1363, time 1151.38ms, mfu -100.00%\n",
      "epoch: 466, step: 2802 train/loss: 0.0075, train/tfacc: 1.0000, val/loss: 0.1885, val/tfacc: 0.9269, 🤖\n",
      "epoch 466: loss 0.1073, time 1220.15ms, mfu -100.00%\n",
      "epoch: 467, step: 2808 train/loss: 0.0071, train/tfacc: 1.0000, val/loss: 0.1805, val/tfacc: 0.9311, 🤖\n",
      "epoch 467: loss 0.1104, time 1171.90ms, mfu -100.00%\n",
      "epoch: 468, step: 2814 train/loss: 0.0066, train/tfacc: 1.0000, val/loss: 0.1717, val/tfacc: 0.9337, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 468: loss 0.1307, time 1241.04ms, mfu -100.00%\n",
      "epoch: 469, step: 2820 train/loss: 0.0066, train/tfacc: 0.9997, val/loss: 0.1732, val/tfacc: 0.9327, 🤖\n",
      "epoch 469: loss 0.0986, time 1109.21ms, mfu -100.00%\n",
      "epoch: 470, step: 2826 train/loss: 0.0068, train/tfacc: 0.9999, val/loss: 0.1775, val/tfacc: 0.9315, 🤖\n",
      "epoch 470: loss 0.1095, time 1132.06ms, mfu -100.00%\n",
      "epoch: 471, step: 2832 train/loss: 0.0074, train/tfacc: 0.9999, val/loss: 0.1781, val/tfacc: 0.9315, 🤖\n",
      "epoch 471: loss 0.0820, time 1154.46ms, mfu -100.00%\n",
      "epoch: 472, step: 2838 train/loss: 0.0080, train/tfacc: 0.9993, val/loss: 0.1866, val/tfacc: 0.9289, 🤖\n",
      "epoch 472: loss 0.1303, time 1149.84ms, mfu -100.00%\n",
      "epoch: 473, step: 2844 train/loss: 0.0062, train/tfacc: 0.9999, val/loss: 0.1813, val/tfacc: 0.9308, 🤖\n",
      "epoch 473: loss 0.1150, time 1207.48ms, mfu -100.00%\n",
      "epoch: 474, step: 2850 train/loss: 0.0070, train/tfacc: 0.9997, val/loss: 0.1783, val/tfacc: 0.9317, 🤖\n",
      "epoch 474: loss 0.1088, time 1191.41ms, mfu -100.00%\n",
      "epoch: 475, step: 2856 train/loss: 0.0094, train/tfacc: 0.9997, val/loss: 0.1872, val/tfacc: 0.9282, 🤖\n",
      "epoch 475: loss 0.1198, time 1157.50ms, mfu -100.00%\n",
      "epoch: 476, step: 2862 train/loss: 0.0070, train/tfacc: 0.9999, val/loss: 0.1835, val/tfacc: 0.9290, 🤖\n",
      "epoch 476: loss 0.1389, time 1170.29ms, mfu -100.00%\n",
      "epoch: 477, step: 2868 train/loss: 0.0097, train/tfacc: 0.9993, val/loss: 0.2032, val/tfacc: 0.9226, 🤖\n",
      "epoch 477: loss 0.1115, time 1156.49ms, mfu -100.00%\n",
      "epoch: 478, step: 2874 train/loss: 0.0101, train/tfacc: 0.9991, val/loss: 0.2103, val/tfacc: 0.9223, 🤖\n",
      "epoch 478: loss 0.1593, time 1125.66ms, mfu -100.00%\n",
      "epoch: 479, step: 2880 train/loss: 0.0084, train/tfacc: 0.9992, val/loss: 0.1974, val/tfacc: 0.9250, 🤖\n",
      "epoch 479: loss 0.1084, time 1128.76ms, mfu -100.00%\n",
      "epoch: 480, step: 2886 train/loss: 0.0089, train/tfacc: 0.9997, val/loss: 0.1938, val/tfacc: 0.9255, 🤖\n",
      "epoch 480: loss 0.1199, time 1164.93ms, mfu -100.00%\n",
      "epoch: 481, step: 2892 train/loss: 0.0065, train/tfacc: 0.9997, val/loss: 0.1803, val/tfacc: 0.9311, 🤖\n",
      "epoch 481: loss 0.1364, time 1137.87ms, mfu -100.00%\n",
      "epoch: 482, step: 2898 train/loss: 0.0061, train/tfacc: 0.9999, val/loss: 0.1784, val/tfacc: 0.9324, 🤖\n",
      "epoch 482: loss 0.1026, time 1085.48ms, mfu -100.00%\n",
      "epoch: 483, step: 2904 train/loss: 0.0090, train/tfacc: 0.9993, val/loss: 0.1948, val/tfacc: 0.9262, 🤖\n",
      "epoch 483: loss 0.1026, time 1103.54ms, mfu -100.00%\n",
      "epoch: 484, step: 2910 train/loss: 0.0071, train/tfacc: 0.9996, val/loss: 0.1860, val/tfacc: 0.9280, 🤖\n",
      "epoch 484: loss 0.1103, time 1153.53ms, mfu -100.00%\n",
      "epoch: 485, step: 2916 train/loss: 0.0069, train/tfacc: 0.9999, val/loss: 0.1828, val/tfacc: 0.9304, 🤖\n",
      "epoch 485: loss 0.1006, time 1174.18ms, mfu -100.00%\n",
      "epoch: 486, step: 2922 train/loss: 0.0062, train/tfacc: 0.9997, val/loss: 0.1810, val/tfacc: 0.9318, 🤖\n",
      "epoch 486: loss 0.0947, time 1109.75ms, mfu -100.00%\n",
      "epoch: 487, step: 2928 train/loss: 0.0062, train/tfacc: 0.9997, val/loss: 0.1841, val/tfacc: 0.9293, 🤖\n",
      "epoch 487: loss 0.1088, time 1185.57ms, mfu -100.00%\n",
      "epoch: 488, step: 2934 train/loss: 0.0087, train/tfacc: 0.9991, val/loss: 0.1838, val/tfacc: 0.9300, 🤖\n",
      "epoch 488: loss 0.0988, time 1110.99ms, mfu -100.00%\n",
      "epoch: 489, step: 2940 train/loss: 0.0073, train/tfacc: 0.9997, val/loss: 0.1870, val/tfacc: 0.9305, 🤖\n",
      "epoch 489: loss 0.1252, time 1116.77ms, mfu -100.00%\n",
      "epoch: 490, step: 2946 train/loss: 0.0067, train/tfacc: 0.9996, val/loss: 0.1933, val/tfacc: 0.9268, 🤖\n",
      "epoch 490: loss 0.1222, time 1155.05ms, mfu -100.00%\n",
      "epoch: 491, step: 2952 train/loss: 0.0067, train/tfacc: 0.9997, val/loss: 0.1932, val/tfacc: 0.9281, 🤖\n",
      "epoch 491: loss 0.1090, time 1158.47ms, mfu -100.00%\n",
      "epoch: 492, step: 2958 train/loss: 0.0077, train/tfacc: 0.9996, val/loss: 0.1926, val/tfacc: 0.9266, 🤖\n",
      "epoch 492: loss 0.1221, time 1162.12ms, mfu -100.00%\n",
      "epoch: 493, step: 2964 train/loss: 0.0094, train/tfacc: 0.9995, val/loss: 0.1990, val/tfacc: 0.9251, 🤖\n",
      "epoch 493: loss 0.1028, time 1202.56ms, mfu -100.00%\n",
      "epoch: 494, step: 2970 train/loss: 0.0086, train/tfacc: 0.9993, val/loss: 0.1957, val/tfacc: 0.9258, 🤖\n",
      "epoch 494: loss 0.1129, time 1111.09ms, mfu -100.00%\n",
      "epoch: 495, step: 2976 train/loss: 0.0060, train/tfacc: 0.9999, val/loss: 0.1804, val/tfacc: 0.9301, 🤖\n",
      "epoch 495: loss 0.1178, time 1214.73ms, mfu -100.00%\n",
      "epoch: 496, step: 2982 train/loss: 0.0090, train/tfacc: 0.9992, val/loss: 0.2001, val/tfacc: 0.9245, 🤖\n",
      "epoch 496: loss 0.1213, time 1154.29ms, mfu -100.00%\n",
      "epoch: 497, step: 2988 train/loss: 0.0062, train/tfacc: 0.9997, val/loss: 0.1797, val/tfacc: 0.9305, 🤖\n",
      "epoch 497: loss 0.0959, time 1250.65ms, mfu -100.00%\n",
      "epoch: 498, step: 2994 train/loss: 0.0062, train/tfacc: 0.9998, val/loss: 0.1807, val/tfacc: 0.9319, 🤖\n",
      "epoch 498: loss 0.1246, time 1126.46ms, mfu -100.00%\n",
      "epoch: 499, step: 3000 train/loss: 0.0052, train/tfacc: 0.9997, val/loss: 0.1700, val/tfacc: 0.9347, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 499: loss 0.1204, time 1250.05ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "train_model(**train_kwargs) # set norm_first = False in this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test, target_test, labels_test = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 85.31%\n",
      "full sequence accuracy: 42.97%\n",
      "teacher-forcing accuracy:  92.73%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'elementwise_accuracy': 0.8531250357627869,\n",
       " 'full_sequence_accuracy': 0.4296875,\n",
       " 'teacher_forcing_accuracy': 0.9273437857627869,\n",
       " 'acc_by_position': [0.9765625,\n",
       "  0.921875,\n",
       "  0.8828125,\n",
       "  0.8359375,\n",
       "  0.796875,\n",
       "  0.7578125,\n",
       "  0.7109375,\n",
       "  0.78125,\n",
       "  0.8984375,\n",
       "  0.96875]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq2seq_model(model, source_test, target_test, labels_test, start_token, print_=True, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project_name = 'abstract_transformer--object_sorting'\n",
    "num_trials = 1\n",
    "train_sizes = [250, 500, 1000, 1500, 2000, 2500, 3000]\n",
    "start_trial = 0\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee, ea, de, da = 0, 2, 2, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_abstransformer_model(ee, ea, de, da):\n",
    "\n",
    "    model_args = dict(\n",
    "        input_spec=dict(type='vector', dim=8), output_spec=dict(type='token', vocab_size=10+1),\n",
    "        symbol_retrieval='positional_symbols', symbol_retrieval_kwargs=dict(symbol_dim=64, max_symbols=10),\n",
    "        d_model=64, out_dim=10, n_layers_enc=2, n_layers_dec=2,\n",
    "        encoder_kwargs=dict(n_heads_enc=ee, n_heads_abs=ea, dff=128, activation='relu', norm_first=True, dropout_rate=0.1, causal=False, rel_mask_diag=False),\n",
    "        decoder_kwargs=dict(n_heads_enc=de, n_heads_abs=da, n_heads_cross=2, dff=128, activation='relu', norm_first=True, dropout_rate=0.1, causal=True, rel_mask_diag=False),\n",
    "        in_block_size=10, out_block_size=10)\n",
    "    seq2seqabstransformer = Seq2SeqAbstractTransformer(**model_args)#.to(device)\n",
    "    return seq2seqabstransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return create_abstransformer_model(ee, ea, de, da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_learning_curves\u001b[39m(\n\u001b[1;32m      2\u001b[0m     create_model,\n\u001b[1;32m      3\u001b[0m     wandb_project_name, group_name,\n\u001b[0;32m----> 4\u001b[0m     train_sizes\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_sizes\u001b[49m, num_trials\u001b[38;5;241m=\u001b[39mnum_trials):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_size \u001b[38;5;129;01min\u001b[39;00m tqdm(train_sizes, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain size\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m trange(start_trial, start_trial \u001b[38;5;241m+\u001b[39m num_trials, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;66;03m# run = wandb.init(project=wandb_project_name, group=group_name, name=f'train size = {train_size}; trial = {trial}',\u001b[39;00m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m#                 config={'train size': train_size, 'trial': trial, 'group': group_name})\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m# TODO: add model args to config?\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_learning_curves(\n",
    "    create_model,\n",
    "    wandb_project_name, group_name,\n",
    "    train_sizes=train_sizes, num_trials=num_trials):\n",
    "\n",
    "    for train_size in tqdm(train_sizes, desc='train size'):\n",
    "\n",
    "        for trial in trange(start_trial, start_trial + num_trials, desc='trial', leave=False):\n",
    "            # run = wandb.init(project=wandb_project_name, group=group_name, name=f'train size = {train_size}; trial = {trial}',\n",
    "            #                 config={'train size': train_size, 'trial': trial, 'group': group_name})\n",
    "            # TODO: add model args to config?\n",
    "\n",
    "            model = create_model().to(device)\n",
    "\n",
    "            scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "            # optimizer\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "            # optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type=device)\n",
    "\n",
    "            train_dl = get_train_dl(train_size)\n",
    "\n",
    "            # TODO: make training loop support pre-initiated wanbd runs\n",
    "            train_kwargs = dict(\n",
    "                model=model, train_dl=train_dl, eval_model=eval_model, n_epochs=n_epochs,\n",
    "                optimizer=optimizer, scaler=scaler, get_lr=get_lr,\n",
    "                compile=True, grad_clip=0,\n",
    "                eval_main_metric='val/loss',\n",
    "                always_save_checkpoint=always_save_checkpoint,\n",
    "                # ckpt_dict=dict(model_args=model_args), \n",
    "                out_dir=out_dir,\n",
    "                wandb_log=False, wandb_init_kwargs=dict(project=wandb_project, group=group_name, name=f'{group_name}--trial={trial}'),\n",
    "                track_mfu=True,\n",
    "                ddp=False, device_type='cuda')\n",
    "            train_utils.train_model(**train_kwargs)\n",
    "\n",
    "            source_test, target_test, labels_test = test_ds.tensors\n",
    "            eval_dict = evaluate_seq2seq_model(model, source_test, target_test, labels_test, start_token, print_=True, ctx=ctx)\n",
    "\n",
    "            # wandb.log(eval_dict)\n",
    "            # wandb.finish(quiet=True)\n",
    "\n",
    "            del model\n",
    "\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 2 train/loss: 2.4196, train/tfacc: 0.1109, val/loss: 2.4156, val/tfacc: 0.1127, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.4996, time 31263.68ms, mfu -100.00%\n",
      "epoch: 1, step: 4 train/loss: 2.3761, train/tfacc: 0.1036, val/loss: 2.3727, val/tfacc: 0.1108, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 2.3970, time 1199.03ms, mfu -100.00%\n",
      "epoch: 2, step: 6 train/loss: 2.2999, train/tfacc: 0.1197, val/loss: 2.2974, val/tfacc: 0.1235, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 2.3223, time 1316.90ms, mfu -100.00%\n",
      "epoch: 3, step: 8 train/loss: 2.2637, train/tfacc: 0.1348, val/loss: 2.2621, val/tfacc: 0.1352, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 2.2540, time 1215.06ms, mfu -100.00%\n",
      "epoch: 4, step: 10 train/loss: 2.2459, train/tfacc: 0.1503, val/loss: 2.2451, val/tfacc: 0.1489, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 2.2274, time 1212.45ms, mfu -100.00%\n",
      "epoch: 5, step: 12 train/loss: 2.2218, train/tfacc: 0.1675, val/loss: 2.2220, val/tfacc: 0.1624, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 2.1906, time 1249.38ms, mfu -100.00%\n",
      "epoch: 6, step: 14 train/loss: 2.1965, train/tfacc: 0.1722, val/loss: 2.1980, val/tfacc: 0.1685, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 2.1604, time 1285.63ms, mfu -100.00%\n",
      "epoch: 7, step: 16 train/loss: 2.1751, train/tfacc: 0.1805, val/loss: 2.1776, val/tfacc: 0.1780, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 2.1295, time 1211.50ms, mfu -100.00%\n",
      "epoch: 8, step: 18 train/loss: 2.1583, train/tfacc: 0.1891, val/loss: 2.1616, val/tfacc: 0.1858, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 2.1077, time 1589.96ms, mfu -100.00%\n",
      "epoch: 9, step: 20 train/loss: 2.1384, train/tfacc: 0.1922, val/loss: 2.1420, val/tfacc: 0.1919, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 2.0786, time 1373.41ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  14%|█▍        | 1/7 [00:48<04:49, 48.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 9.84%\n",
      "full sequence accuracy: 0.00%\n",
      "teacher-forcing accuracy:  18.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 4 train/loss: 2.3567, train/tfacc: 0.1200, val/loss: 2.3605, val/tfacc: 0.1170, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.3820, time 31078.46ms, mfu -100.00%\n",
      "epoch: 1, step: 8 train/loss: 2.2654, train/tfacc: 0.1387, val/loss: 2.2674, val/tfacc: 0.1385, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 2.2617, time 1609.61ms, mfu -100.00%\n",
      "epoch: 2, step: 12 train/loss: 2.2119, train/tfacc: 0.1634, val/loss: 2.2120, val/tfacc: 0.1590, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 2.2001, time 1601.12ms, mfu -100.00%\n",
      "epoch: 3, step: 16 train/loss: 2.1700, train/tfacc: 0.1665, val/loss: 2.1690, val/tfacc: 0.1681, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 2.1578, time 2182.98ms, mfu -100.00%\n",
      "epoch: 4, step: 20 train/loss: 2.1237, train/tfacc: 0.1955, val/loss: 2.1229, val/tfacc: 0.1962, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 2.1035, time 2452.11ms, mfu -100.00%\n",
      "epoch: 5, step: 24 train/loss: 2.0729, train/tfacc: 0.2363, val/loss: 2.0732, val/tfacc: 0.2337, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 2.0631, time 2309.66ms, mfu -100.00%\n",
      "epoch: 6, step: 28 train/loss: 2.0203, train/tfacc: 0.2521, val/loss: 2.0217, val/tfacc: 0.2529, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 2.0081, time 2528.76ms, mfu -100.00%\n",
      "epoch: 7, step: 32 train/loss: 1.9667, train/tfacc: 0.2653, val/loss: 1.9693, val/tfacc: 0.2660, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 1.9443, time 1896.86ms, mfu -100.00%\n",
      "epoch: 8, step: 36 train/loss: 1.9156, train/tfacc: 0.2785, val/loss: 1.9183, val/tfacc: 0.2732, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 1.8947, time 1233.20ms, mfu -100.00%\n",
      "epoch: 9, step: 40 train/loss: 1.8635, train/tfacc: 0.2809, val/loss: 1.8653, val/tfacc: 0.2827, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 1.8465, time 1293.16ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  29%|██▊       | 2/7 [01:42<04:18, 51.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 10.29%\n",
      "full sequence accuracy: 0.00%\n",
      "teacher-forcing accuracy:  28.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 8 train/loss: 2.2773, train/tfacc: 0.1335, val/loss: 2.2818, val/tfacc: 0.1264, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.2982, time 27287.14ms, mfu -100.00%\n",
      "epoch: 1, step: 16 train/loss: 2.1743, train/tfacc: 0.1809, val/loss: 2.1779, val/tfacc: 0.1779, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 2.1800, time 1463.14ms, mfu -100.00%\n",
      "epoch: 2, step: 24 train/loss: 2.0826, train/tfacc: 0.2285, val/loss: 2.0851, val/tfacc: 0.2272, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 2.0898, time 1454.96ms, mfu -100.00%\n",
      "epoch: 3, step: 32 train/loss: 1.9839, train/tfacc: 0.2645, val/loss: 1.9865, val/tfacc: 0.2620, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 2.0029, time 1364.19ms, mfu -100.00%\n",
      "epoch: 4, step: 40 train/loss: 1.8838, train/tfacc: 0.2777, val/loss: 1.8878, val/tfacc: 0.2775, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 1.9006, time 1359.98ms, mfu -100.00%\n",
      "epoch: 5, step: 48 train/loss: 1.7941, train/tfacc: 0.2848, val/loss: 1.7988, val/tfacc: 0.2854, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 1.8146, time 1465.61ms, mfu -100.00%\n",
      "epoch: 6, step: 56 train/loss: 1.7217, train/tfacc: 0.3039, val/loss: 1.7242, val/tfacc: 0.3009, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 1.7457, time 1455.64ms, mfu -100.00%\n",
      "epoch: 7, step: 64 train/loss: 1.6542, train/tfacc: 0.3184, val/loss: 1.6547, val/tfacc: 0.3173, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 1.6663, time 1399.27ms, mfu -100.00%\n",
      "epoch: 8, step: 72 train/loss: 1.5927, train/tfacc: 0.3330, val/loss: 1.5947, val/tfacc: 0.3287, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 1.6052, time 1277.08ms, mfu -100.00%\n",
      "epoch: 9, step: 80 train/loss: 1.5439, train/tfacc: 0.3581, val/loss: 1.5476, val/tfacc: 0.3505, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 1.5321, time 1450.25ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  43%|████▎     | 3/7 [02:28<03:16, 49.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 14.34%\n",
      "full sequence accuracy: 0.00%\n",
      "teacher-forcing accuracy:  35.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 12 train/loss: 2.2288, train/tfacc: 0.1542, val/loss: 2.2273, val/tfacc: 0.1583, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.2477, time 26540.96ms, mfu -100.00%\n",
      "epoch: 1, step: 24 train/loss: 2.1060, train/tfacc: 0.2095, val/loss: 2.1080, val/tfacc: 0.2033, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 2.1008, time 1467.04ms, mfu -100.00%\n",
      "epoch: 2, step: 36 train/loss: 1.9700, train/tfacc: 0.2723, val/loss: 1.9729, val/tfacc: 0.2620, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 1.9841, time 1491.36ms, mfu -100.00%\n",
      "epoch: 3, step: 48 train/loss: 1.8269, train/tfacc: 0.2885, val/loss: 1.8300, val/tfacc: 0.2827, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 1.8582, time 1519.06ms, mfu -100.00%\n",
      "epoch: 4, step: 60 train/loss: 1.7072, train/tfacc: 0.2975, val/loss: 1.7121, val/tfacc: 0.2943, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 1.7388, time 1546.02ms, mfu -100.00%\n",
      "epoch: 5, step: 72 train/loss: 1.6135, train/tfacc: 0.3190, val/loss: 1.6199, val/tfacc: 0.3146, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 1.6449, time 1485.10ms, mfu -100.00%\n",
      "epoch: 6, step: 84 train/loss: 1.5439, train/tfacc: 0.3547, val/loss: 1.5500, val/tfacc: 0.3502, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 1.5786, time 1529.73ms, mfu -100.00%\n",
      "epoch: 7, step: 96 train/loss: 1.4760, train/tfacc: 0.3959, val/loss: 1.4806, val/tfacc: 0.3931, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 1.4929, time 1507.01ms, mfu -100.00%\n",
      "epoch: 8, step: 108 train/loss: 1.4325, train/tfacc: 0.4168, val/loss: 1.4347, val/tfacc: 0.4139, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 1.4338, time 1563.47ms, mfu -100.00%\n",
      "epoch: 9, step: 120 train/loss: 1.3742, train/tfacc: 0.4447, val/loss: 1.3739, val/tfacc: 0.4449, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 1.3824, time 1723.82ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  57%|█████▋    | 4/7 [03:14<02:24, 48.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 20.20%\n",
      "full sequence accuracy: 0.00%\n",
      "teacher-forcing accuracy:  44.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 16 train/loss: 2.1509, train/tfacc: 0.1898, val/loss: 2.1518, val/tfacc: 0.1861, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.1634, time 26247.43ms, mfu -100.00%\n",
      "epoch: 1, step: 32 train/loss: 1.9588, train/tfacc: 0.2586, val/loss: 1.9622, val/tfacc: 0.2526, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 1.9982, time 1570.01ms, mfu -100.00%\n",
      "epoch: 2, step: 48 train/loss: 1.8069, train/tfacc: 0.2842, val/loss: 1.8116, val/tfacc: 0.2796, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 1.8532, time 1669.52ms, mfu -100.00%\n",
      "epoch: 3, step: 64 train/loss: 1.6832, train/tfacc: 0.3164, val/loss: 1.6848, val/tfacc: 0.3133, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 1.7336, time 1492.06ms, mfu -100.00%\n",
      "epoch: 4, step: 80 train/loss: 1.5763, train/tfacc: 0.3599, val/loss: 1.5780, val/tfacc: 0.3598, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 1.6322, time 1558.21ms, mfu -100.00%\n",
      "epoch: 5, step: 96 train/loss: 1.4624, train/tfacc: 0.4128, val/loss: 1.4616, val/tfacc: 0.4140, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 1.4956, time 1760.36ms, mfu -100.00%\n",
      "epoch: 6, step: 112 train/loss: 1.2690, train/tfacc: 0.4968, val/loss: 1.2710, val/tfacc: 0.4951, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 1.3366, time 1715.59ms, mfu -100.00%\n",
      "epoch: 7, step: 128 train/loss: 1.0920, train/tfacc: 0.5686, val/loss: 1.0949, val/tfacc: 0.5658, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 1.1802, time 1578.49ms, mfu -100.00%\n",
      "epoch: 8, step: 144 train/loss: 0.9539, train/tfacc: 0.6294, val/loss: 0.9544, val/tfacc: 0.6301, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 1.0732, time 1762.55ms, mfu -100.00%\n",
      "epoch: 9, step: 160 train/loss: 0.9012, train/tfacc: 0.6418, val/loss: 0.9029, val/tfacc: 0.6424, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 0.9605, time 1573.08ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  71%|███████▏  | 5/7 [04:01<01:35, 47.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 38.68%\n",
      "full sequence accuracy: 0.38%\n",
      "teacher-forcing accuracy:  64.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 20 train/loss: 2.1053, train/tfacc: 0.2101, val/loss: 2.1051, val/tfacc: 0.2109, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.1382, time 26348.18ms, mfu -100.00%\n",
      "epoch: 1, step: 40 train/loss: 1.8719, train/tfacc: 0.2753, val/loss: 1.8725, val/tfacc: 0.2781, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 1.9096, time 1763.87ms, mfu -100.00%\n",
      "epoch: 2, step: 60 train/loss: 1.6740, train/tfacc: 0.3191, val/loss: 1.6756, val/tfacc: 0.3196, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 1.7244, time 1842.89ms, mfu -100.00%\n",
      "epoch: 3, step: 80 train/loss: 1.5187, train/tfacc: 0.3747, val/loss: 1.5236, val/tfacc: 0.3684, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 1.5904, time 1875.92ms, mfu -100.00%\n",
      "epoch: 4, step: 100 train/loss: 1.3967, train/tfacc: 0.4431, val/loss: 1.3959, val/tfacc: 0.4427, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 1.4544, time 1720.68ms, mfu -100.00%\n",
      "epoch: 5, step: 120 train/loss: 1.2200, train/tfacc: 0.5125, val/loss: 1.2232, val/tfacc: 0.5120, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 1.2814, time 1827.51ms, mfu -100.00%\n",
      "epoch: 6, step: 140 train/loss: 1.0653, train/tfacc: 0.5791, val/loss: 1.0654, val/tfacc: 0.5780, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 1.1064, time 1624.67ms, mfu -100.00%\n",
      "epoch: 7, step: 160 train/loss: 0.9258, train/tfacc: 0.6359, val/loss: 0.9199, val/tfacc: 0.6354, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 0.9916, time 1730.93ms, mfu -100.00%\n",
      "epoch: 8, step: 180 train/loss: 0.8579, train/tfacc: 0.6510, val/loss: 0.8593, val/tfacc: 0.6467, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 0.9295, time 1566.65ms, mfu -100.00%\n",
      "epoch: 9, step: 200 train/loss: 0.7219, train/tfacc: 0.7186, val/loss: 0.7205, val/tfacc: 0.7179, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 0.7962, time 1785.60ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  86%|████████▌ | 6/7 [04:49<00:47, 47.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 49.62%\n",
      "full sequence accuracy: 1.81%\n",
      "teacher-forcing accuracy:  71.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "WARNING: no abstract attention heads in this block; consider using a standard block instead.\n",
      "compiling model... done compiling.\n",
      "starting training loop...\n",
      "epoch: 0, step: 24 train/loss: 2.0660, train/tfacc: 0.2462, val/loss: 2.0685, val/tfacc: 0.2451, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 0: loss 2.0755, time 28061.02ms, mfu -100.00%\n",
      "epoch: 1, step: 48 train/loss: 1.7881, train/tfacc: 0.2956, val/loss: 1.7908, val/tfacc: 0.2945, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 1: loss 1.8357, time 1992.52ms, mfu -100.00%\n",
      "epoch: 2, step: 72 train/loss: 1.5935, train/tfacc: 0.3345, val/loss: 1.5926, val/tfacc: 0.3334, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 2: loss 1.6511, time 2048.97ms, mfu -100.00%\n",
      "epoch: 3, step: 96 train/loss: 1.4810, train/tfacc: 0.3809, val/loss: 1.4815, val/tfacc: 0.3765, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 3: loss 1.5479, time 1653.12ms, mfu -100.00%\n",
      "epoch: 4, step: 120 train/loss: 1.3636, train/tfacc: 0.4442, val/loss: 1.3630, val/tfacc: 0.4438, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 4: loss 1.4169, time 2055.84ms, mfu -100.00%\n",
      "epoch: 5, step: 144 train/loss: 1.2194, train/tfacc: 0.5065, val/loss: 1.2230, val/tfacc: 0.5046, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 5: loss 1.3210, time 2063.83ms, mfu -100.00%\n",
      "epoch: 6, step: 168 train/loss: 1.0345, train/tfacc: 0.5875, val/loss: 1.0344, val/tfacc: 0.5855, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 6: loss 1.1278, time 2134.20ms, mfu -100.00%\n",
      "epoch: 7, step: 192 train/loss: 0.8450, train/tfacc: 0.6612, val/loss: 0.8448, val/tfacc: 0.6601, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 7: loss 0.9738, time 1932.98ms, mfu -100.00%\n",
      "epoch: 8, step: 216 train/loss: 0.6830, train/tfacc: 0.7277, val/loss: 0.6775, val/tfacc: 0.7290, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 8: loss 0.8409, time 1948.55ms, mfu -100.00%\n",
      "epoch: 9, step: 240 train/loss: 0.5881, train/tfacc: 0.7619, val/loss: 0.5849, val/tfacc: 0.7622, 🤖\n",
      "saving checkpoint to ../out/object_sorting\n",
      "epoch 9: loss 0.7192, time 2248.14ms, mfu -100.00%\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size: 100%|██████████| 7/7 [05:40<00:00, 48.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise accuracy: 56.42%\n",
      "full sequence accuracy: 4.30%\n",
      "teacher-forcing accuracy:  76.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_learning_curves(\n",
    "    create_model, wandb_project_name='abstract_transformer--object_sorting', group_name='abstract_transformer_',\n",
    "    train_sizes=train_sizes, num_trials=num_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstract_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
