\section{Conclusion}\label{sec:discussion}

\textbf{Summary.} The standard attention mechanism of Transformers provides a versatile mechanism for retrieval of sensory information from a given context, but does not explicitly support retrieval of relational information. In this work, we presented an extension of the Transformer architecture that disentangles and integrates sensory and relational information through a variant of multi-head attention with two distinct types of attention heads: standard self-attention for sensory information and a novel \textit{relational attention} mechanism for relational information. We empirically evaluate this architecture and find that it yields performance improvements across a range of tasks and modalities.

\textbf{Limitations \& Future Work.} The proposed architecture introduces several hyperparameters and possible configurations. Although we carried out ablations on the major configuration choices (e.g., composition of head types, symmetry, symbol assignment mechanism), an expanded empirical examination would help develop an improved understanding of the behavior of this architecture under different configurations. Such a systematic study may also reveal further modifications that could improve the architecture. We also note that our implementation of the Dual-Attention Transformer currently lacks the hardware-aware optimizations available for standard Transformers (e.g., Flash-Attention~\citep{dao2022flashattention}), which results in slower performance. However, we expect similar optimizations to be possible for dual attention and \textit{DAT}. An important direction for future work is the mechanistic interpretability~\citep{elhage2021mathematical,olsson2022context,wang2023interpretability} of \textit{DAT} models, focusing on identifying specific circuits that perform key computations to better understand the performance improvements observed in complex domains like language modeling.