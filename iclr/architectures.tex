% \section{Orthos: The Dual Head-Type Transformer Architecture}
% \section{The Dual Attention Transformer Architecture}
\section{Integrating Attention over Sensory and Relational Information}

\subsection{Dual Attention}

One of the keys to the success of the Transformer architecture is the use of so-called \textit{multi-head} attention. This involves learning and computing multiple attention operations in parallel at each layer and concatenating the output. This enables learning multiple useful criteria for routing information between objects. However, in standard Transformers, these attention heads only support routing sensory information---there is no explicit support for routing \textit{relational} information between objects.

We posit that both sensory and relational information are crucial for robust and flexible learning over sequences or collections of objects. To this end, we propose an extension of multi-head attention with two distinct types of attention heads: sensory attention (i.e., standard self-attention), and relational attention.  Our hypothesis is that by having both kinds of computations available to the model, it can learn to select between them depending on the current task or context, as well as compose them to yield highly-expressive and flexible computational circuits.

\aanote{Emphasize: Both types of attention mechanisms at every layer of computation to dynamically route both sensory and relational information.}

\Cref{alg:dual_head_attn} describes the proposed module, which we refer to as \textit{dual-attention}. The number of sensory-attention heads $\nhsa$ and number of relational attention heads $\nhra$ are hyperparameters. The sensory-attention heads attend to and retrieve sensory information while the relational attention heads attend to and retrieve relational information. The $n_h \coloneq \nhsa + \nhra$ heads are then concatenated to produce the output. The result is a representation of contextual information with integrated sensory and relational components.

\begin{algorithm}[ht!]
	\caption{Dual Attention}\label{alg:dual_head_attn}
	\SetKwInput{Hyperparameters}{Hyperparameters}
	\SetKwInput{Input}{Input}
	\SetKwInOut{Output}{Output}
    % \Hyperparameters{$\nhsa, \nhra$, symbol assignment mechanism, (optional: \texttt{symmetric\_RA = False}, $d_r = \nhra$, ...)}
	\Input{$\x = (x_1, \ldots, x_n) \in \reals^{n \times d}$ }

    \vspace{1em}

    \texttt{Compute self-attention heads}
    \begin{align*}
        \bm{\alpha}^{(h)} &\gets \Softmax\bigparen{{(\x \, \Wqattnn{h})} {(\x \, \Wkattnn{h})}^\intercal}, \qquad && h \in [\nhsa] \\
        e_i^{(h)} &\gets \sum_{j} \alpha_{ij}^{(h)} x_j \, W_v^h, \qquad && i \in [n], h \in [\nhsa]\\
        e_i &\gets \concat\bigparen{e_i^{(1)}, \ldots, e_i^{(\nhsa)}} \, W_o^{sa}, && i \in [n]
    \end{align*}

    \texttt{Assign symbols:} $\s = (s_1, \ldots, s_n) \gets \SymbolRetriever(\x;\,\Slib)$
    \vspace{0.5em}

    \texttt{Compute relational attention heads}
    \begin{align*}
        \bm{\alpha}^{(h)} &\gets \Softmax\bigparen{{(\x \, \Wqattnn{h})} {(\x \, \Wkattnn{h})}^\intercal}, \qquad && h \in [\nhra] \\
        \bm{r}_{ij} &\gets \bigparen{\iprod{x_i \, \Wqrell{\ell}}{x_j \, \Wkrell{\ell}}}_{\ell \in [d_r]} \qquad && i,j \in [n] \\
        a_i^{(h)} &\gets \sum_{j} \alpha_{ij}^{(h)} \bigparen{\bm{r}_{ij}\,W_r^h + s_j \, W_s^{h}}, \qquad && i \in [n],\, h \in [\nhra]\\
        a_i &\gets \concat\bigparen{a_i^{(1)}, \ldots, a_i^{(\nhra)}} W_o^{ra}, && i \in [n]
    \end{align*}

    \Output{$\bigparen{\concat(e_i, a_i)}_{i=1}^{n}$}

\end{algorithm}

\textbf{Symmetric relations.} A symmetry inductive bias can be injected into the relations $\bm{r}_{ij}$ by imposing that $W_{q}^{\rel} = W_k^{\rel}$.

\textbf{Attention Masks \& Causality.} Any type of attention mask (e.g., causal mask for autoregressive language modeling) can be implemented in relational attention in the same way as for standard self-attention (i.e., mask is added to $\alpha_{ij}^h$ pre-softmax).

\textbf{Positional Encoding.} There exists different methods in the literature on encoding positional information in the Transformer architecture. For example,~\citet{vaswani2017attention} propose adding positional embeddings to the input,~\citet{shawSelfAttentionRelativePosition2018b} propose adding relative-positional embeddings at each attention operation, and~\citet{suRoFormerEnhancedTransformer2023} propose rotary positional embeddings (RoPE) which apply a position-dependent map to the queries and keys pre-softmax. These methods are compatible with dual attention and are configurable options in our public implementation.
\aanote{we use \texttt{$\backslash$citet} here. Think about whether we want to adjust wording if using a numeric citation style.}

\textbf{Computational complexity.} The computational complexity of relational attention scales the same as standard self-attention with a $O(n^2)$ dependence on sequence length. Like standard attention, relational attention can be computed in parallel via efficient matrix multiplication operations.

\aanote{attention as soft selection of elements at the previous level of computation (in architecture?)}

\subsection{The Dual Attention Transformer Architecture}

A standard Transformer implements a procedure of iterative information retrieval (attention) followed by local processing (MLP). We define our Dual Attention Transformer in the same way, except that multi-head self-attention is replaced by dual attention.~\Cref{alg:dh_encoder,alg:dh_decoder} defines an encoder and decoder block with dual attention. Composing these blocks yields the \textit{Dual Attention Transformer}.

\begin{remark}
    In our implementation, the symbol library $\Slib$ is shared across layers to reduce the number of parameters.
    % The interpretation is that although a particular symbol may have different ``semantic meanings'' in different layers, since its role is to merely act as an abstract code, it can be remapped at each layer (e.g., a variable can be reassigned to point to a different object).
    The rationale is that, since the role of symbols is merely to act as abstract pointer variables, they can be remapped at each layer if needed.% (e.g., a variable can be reassigned to point to a different object).
    % Moreover, $\Slib$ can be randomly initialized and frozen to further reduce the number of trainable parameters. This is because, since the symbols' role is to act as abstract codes, they only need to be well-separated and random Gaussian vectors are approximately orthogonal in high dimensions. That is, randomly initialized and frozen symbols can be thought of as random codes.
\end{remark}
\aanote{Note --- removed mention of the possibility of freezing symbols. Probably distracting and not crucial.}

\begin{figure}[ht]
    \begin{minipage}{0.48\textwidth}
        \begin{algorithm}[H]
            \caption{Dual Attention Encoder Block}\label{alg:dh_encoder}
            \SetKwInOut{Input}{Input}
            % \SetKwInOut{Hyperparameters}{Hyperparameters}
            % \Hyperparameters{$\nhsa, \nhra$, symbol assignment mechanism, (optional: \texttt{symmetric\_RA = False}, $d_r = \nhra$, ...)}
            \Input{$\x \in \reals^{n \times d}$}
            \vspace{0.5em}
            $\x \gets \mathrm{Norm}(\x + \mathrm{DualAttn}(\x))$

            $\x \gets \mathrm{Norm}(\x + \MLP(\x))$

            $\hphantom{\x \gets \mathrm{Norm}(\x + \MLP(\x))}$

            \textbf{Output:} $\x$
        \end{algorithm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \begin{algorithm}[H]
            \caption{Dual Attention Decoder Block}\label{alg:dh_decoder}
            \SetKwInOut{Input}{Input}
            \SetKwInOut{Output}{Output}
            \Input{$\x, \y \in \reals^{n \times d}$}
            \vspace{0.5em}

            $\x \gets \mathrm{Norm}(\x + \mathrm{DualAttn}(\x))$

            $\x \gets \mathrm{Norm}(\x + \mathrm{CrossAttn}(\x, \y))$

            $\x \gets \mathrm{Norm}(\x + \MLP(\x))$

            \Output{$\x$}
        \end{algorithm}
    \end{minipage}
\end{figure}

An encoder-decoder architecture with causal dual-head attention in the decoder can be applied to sequence-to-sequence tasks, as in the original Transformer paper~\citep{vaswani2017attention}. An encoder-only architecture can be used for a BERT-style language embedding model~\citep{devlinBERTPretrainingDeep2019} or a Vision Transformer-style vision model~\citep{dosovitskiyImageWorth16x162020}. A decoder-only architecture with causal dual-head attention can be used for autoregressive language modeling.
% \footnote{``Decoder-only'' is the commonly used term for this type of architecture~\citep{radfordImprovingLanguageUnderstanding2018}, but it can also be viewed as an encoder-only architecture with causal attention}. 
We note that a standard Transformer is a special case of this architecture where $\nhsa = n_h, \nhra = 0$.

\Cref{sec:appendix_implementation} provides further discussion on the details of the architecture and its implementation.