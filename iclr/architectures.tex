% \section{Orthos: The Dual Head-Type Transformer Architecture}
% \section{The Dual Attention Transformer Architecture}
\section{Integrating Attention over Sensory and Relational Information}

\subsection{Dual Attention}

One of the keys to the success of the Transformer architecture is the use of so-called \textit{multi-head} attention. This involves computing multiple attention operations in parallel at each layer and concatenating the output, enabling the model to learn multiple useful criteria for routing information between objects.  However, in standard Transformers, these attention heads focus solely on routing sensory information, lacking explicit support for routing \textit{relational} information between objects.

We posit that both sensory and relational information are crucial for robust and flexible learning over sequences or collections of objects. To this end, we propose an extension of multi-head attention comprising two distinct types of attention heads: sensory attention (i.e., standard self-attention), and relational attention. This yields a powerful mechanism for dynamically routing both sensory and relational information in parallel. Our hypothesis is that by having access to both computational mechanism, the model can learn to select between them based on the current task or context, as well as compose them to create highly-expressive and flexible computational circuits.

\Cref{alg:dual_head_attn} describes the proposed module, referred to as \textit{dual-attention}. The number of sensory-attention heads $\nhsa$ and number of relational attention heads $\nhra$ are hyperparameters. The sensory-attention heads attend to sensory information while the relational attention heads attend to relational information. The combined $n_h \coloneq \nhsa + \nhra$ heads are then concatenated to produce the output. The result is a representation of contextual information with integrated sensory and relational components.

\begin{algorithm}[ht!]
	\caption{Dual Attention}\label{alg:dual_head_attn}
	\SetKwInput{Hyperparameters}{Hyperparameters}
	\SetKwInput{Input}{Input}
	\SetKwInOut{Output}{Output}
    % \Hyperparameters{$\nhsa, \nhra$, symbol assignment mechanism, (optional: \texttt{symmetric\_RA = False}, $d_r = \nhra$, ...)}
	\Input{$\x = (x_1, \ldots, x_n) \in \reals^{n \times d}$ }

    \vspace{1em}

    \texttt{Compute self-attention heads}
    \begin{align*}
        \bm{\alpha}^{(h)} &\gets \Softmax\bigparen{{(\x \, \Wqattnn{h})} {(\x \, \Wkattnn{h})}^\intercal}, \qquad && h \in [\nhsa] \\
        e_i^{(h)} &\gets \sum_{j} \alpha_{ij}^{(h)} x_j \, W_v^h, \qquad && i \in [n], h \in [\nhsa]\\
        e_i &\gets \concat\bigparen{e_i^{(1)}, \ldots, e_i^{(\nhsa)}} \, W_o^{sa}, && i \in [n]
    \end{align*}

    \texttt{Assign symbols:} $\s = (s_1, \ldots, s_n) \gets \SymbolRetriever(\x;\,\Slib)$
    \vspace{0.5em}

    \texttt{Compute relational attention heads}
    \begin{align*}
        \bm{\alpha}^{(h)} &\gets \Softmax\bigparen{{(\x \, \Wqattnn{h})} {(\x \, \Wkattnn{h})}^\intercal}, \qquad && h \in [\nhra] \\
        \bm{r}_{ij} &\gets \bigparen{\iprod{x_i \, \Wqrell{\ell}}{x_j \, \Wkrell{\ell}}}_{\ell \in [d_r]} \qquad && i,j \in [n] \\
        a_i^{(h)} &\gets \sum_{j} \alpha_{ij}^{(h)} \bigparen{\bm{r}_{ij}\,W_r^h + s_j \, W_s^{h}}, \qquad && i \in [n],\, h \in [\nhra]\\
        a_i &\gets \concat\bigparen{a_i^{(1)}, \ldots, a_i^{(\nhra)}} W_o^{ra}, && i \in [n]
    \end{align*}

    \Output{$\bigparen{\concat(e_i, a_i)}_{i=1}^{n}$}

\end{algorithm}

% We briefly make a few remarks.

\textbf{Attention Masks \& Causality.} Any type of attention mask (e.g., causal mask for autoregressive language modeling) can be implemented in relational attention in the same way as for standard self-attention (i.e., mask is added to $\alpha_{ij}^h$ pre-softmax).

\textbf{Positional Encoding.} There exists different methods in the literature for encoding positional information in the Transformer architecture. For example,~\citep{vaswani2017attention} propose adding positional embeddings to the input,~\citep{shawSelfAttentionRelativePosition2018b} propose adding relative-positional embeddings at each attention operation, and~\citep{suRoFormerEnhancedTransformer2023} propose rotary positional embeddings (RoPE) which apply a position-dependent map to the queries and keys pre-softmax. These methods are compatible with dual attention and are configurable options in our public implementation.
\aanote{we use \texttt{$\backslash$citet} here. Think about whether we want to adjust wording if using a numeric citation style.}

\textbf{Computational complexity.} The computational complexity of relational attention scales similarly to standard self-attention with a $O(n^2)$ dependence on sequence length. Like standard attention, relational attention can be computed in parallel via efficient matrix multiplication operations.

\textbf{Symmetric relations.} A symmetry constraint can be injected into the relations $\bm{r}_{ij}$ by imposing that $W_{q}^{\rel} = W_k^{\rel}$, which is a useful inductive bias when the task-relevant relations have this structure.


\subsection{The Dual Attention Transformer Architecture}

The standard Transformer architecture is composed of repeated blocks of attention (information retrieval) followed by an (local processing). Our proposed \textit{Dual Attention Transformer} follows this same structure, but replaces multi-head self-attention with dual attention.  At each layer, dual attention dynamically retrieves both sensory and relational information from the previous level of computation, which is then processed locally by an MLP.~\Cref{alg:dh_encoder,alg:dh_decoder} defines an encoder and decoder block with dual attention. Composing these blocks yields the Dual Attention Transformer architecture.

\begin{figure}[ht]
    \begin{minipage}{0.48\textwidth}
        \begin{algorithm}[H]
            \caption{Dual Attention Encoder Block}\label{alg:dh_encoder}
            \SetKwInOut{Input}{Input}
            % \SetKwInOut{Hyperparameters}{Hyperparameters}
            % \Hyperparameters{$\nhsa, \nhra$, symbol assignment mechanism, (optional: \texttt{symmetric\_RA = False}, $d_r = \nhra$, ...)}
            \Input{$\x \in \reals^{n \times d}$}
            \vspace{0.5em}
            $\x \gets \mathrm{Norm}(\x + \mathrm{DualAttn}(\x))$

            $\x \gets \mathrm{Norm}(\x + \MLP(\x))$

            $\hphantom{\x \gets \mathrm{Norm}(\x + \MLP(\x))}$

            \textbf{Output:} $\x$
        \end{algorithm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \begin{algorithm}[H]
            \caption{Dual Attention Decoder Block}\label{alg:dh_decoder}
            \SetKwInOut{Input}{Input}
            \SetKwInOut{Output}{Output}
            \Input{$\x, \y \in \reals^{n \times d}$}
            \vspace{0.5em}

            $\x \gets \mathrm{Norm}(\x + \mathrm{DualAttn}(\x))$

            $\x \gets \mathrm{Norm}(\x + \mathrm{CrossAttn}(\x, \y))$

            $\x \gets \mathrm{Norm}(\x + \MLP(\x))$

            \Output{$\x$}
        \end{algorithm}
    \end{minipage}
\end{figure}

The Dual Attention Transformer framework supports all architectural variants of the standard Transformer, making it applicable to a wide range of task paradigms. An encoder-decoder architecture with causal dual-head attention in the decoder can be applied to sequence-to-sequence tasks, as in the original Transformer paper~\citep{vaswani2017attention}. An encoder-only architecture can be used for a BERT-style language embedding model~\citep{devlinBERTPretrainingDeep2019} or a \textit{ViT}-style vision model~\citep{dosovitskiyImageWorth16x162020}. A decoder-only architecture with causal dual-head attention can be used for autoregressive language modeling.
% \footnote{``Decoder-only'' is the commonly used term for this type of architecture~\citep{radfordImprovingLanguageUnderstanding2018}, but it can also be viewed as an encoder-only architecture with causal attention}. 
% We note that a standard Transformer is a special case of this architecture where $\nhsa = n_h, \nhra = 0$.

\Cref{sec:appendix_implementation} provides further discussion on the details of the architecture and its implementation.