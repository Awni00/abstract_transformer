\section*{Code and Reproducibility}

% Our implementation of the Dual Attention Transformer architecture is open-sourced at \url{https://github.com/Awni00/dual-attention} and published as a Python package. Pre-trained model weights, including the 1.3B-parameter \textit{DAT} language model, are made publicly available and can be loaded directly using the package. Additionally, we provide code for running the experiments described in this paper, along with instructions for reproducing our results and access to the experimental logs. Links to all this can be found through the github repository.
Our implementation of the Dual Attention Transformer architecture is open-sourced and published as a Python package. Pre-trained model weights, including the 1.3B-parameter \textit{DAT} language model, are made publicly available and can be loaded directly using the package. Additionally, we provide code for running the experiments described in this paper, along with instructions for reproducing our results and access to the experimental logs. Links will be included in the de-anonymized camera-ready version.

% \aanote{add mention of visualization app included in package and hosted online.}