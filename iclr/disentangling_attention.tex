\section{Disentangling Attention over Sensory and Relational Information}

\subsection{Standard Attention: Attention over Sensory Information}

The attention mechanism of standard Transformers can be understood as a form of neural message-passing that performs selective information retrieval over the sensory information in the context. An object emits a query that is compared against the keys of each object in its context via an inner product. A match occurs when the inner product is large, causing an encoding of the features of the attended object to be retrieved and added to the residual stream of the receiver. Formally, attention between an object $x$ and a context $\y = \ys$ takes the form
\begin{equation}\label{eq:self_attn}
  \begin{split}
    \Attn(x, \ys) &= \sum_{i=1}^{n} \alpha_i(x, \y) \phi_v(y_i), \ \text{where}, \\
    \alpha(x, \y) &= \Softmax\bigparen{\bra{\iprod{\phiqattn(x)}{\phikattn(y_i)}}_{i=1}^{n}},
  \end{split}
\end{equation}
where $\phiqattn,\phikattn$ are learnable query and key maps controlling the selection criterion, and $\phi_v$ is a learnable value map controlling what information about $y_i$ is sent. The attention scores $\alpha(x, \y) \in \Delta^n$ are used to retrieve a convex combination of the values, where $\alpha_i(x, \y)$ denotes the $i$-th component.

Here, the retrieved information is \textit{sensory}, comprising the features and attributes of individual objects. Standard neural attention does not explicitly capture information about the \textit{relationship} between the sender and the receiver, making relational learning in standard Transformers inefficient.

% The attention scores $\alpha_i(x, \y)$ can perhaps be thought of as (normalized) relations between objects, but these are merely computed as an intermediate step in an information-retrieval operation, and are ultimately entangled with the object-level sensory features of the sender\footnote{In principle, it is also possible that the MLP compute a relation between the sender and receiver in the local processing step by separating their representations and computing a comparison. However, this is difficult to do since the representations of the objects will be additively mixed, and there is no inductive bias pressuring the computed function to be a relation.}. 

\subsection{Relational Attention: Attention over Relational Information}

We propose \textit{relational attention}, an attention mechanism for dynamically routing relational information. Under the message-passing view of \cref{eq:intro_message_passing}, relational attention represents an operation where the message from one object to another encodes the relationship between the sender and the receiver.

Mirroring standard attention, this operation begins with each object emitting a query and a key, which are compared via an inner product to compute attention scores determining which objects to attend to. Next, instead of retrieving the sensory features of the selected object, relational attention retrieves the \textit{relation} between the two objects---defined as a series of comparisons between the two objects under different feature subspaces. In addition, a symbolic identifier is transmitted to signal the identity of the sender to the receiver. Mathematically, this operation is computed as follows.
\begin{equation}\label{eq:rel_attn}
  \begin{split}
    \RelAttn(x, \ys) &= \sum_{i=1}^{n} \alpha_i(x, \y) \bigparen{r(x, y_i) W_r + s_i W_s}, \ \text{where},\\
    \alpha(x, \y) &= \Softmax\bigparen{\bra{\iprod{\phiqattn(x)}{\phikattn(y_i)}}_{i=1}^{n}}, \\
    % r(x, y) &= \bigparen{\iprod{\phi_{q,1}^{\rel}(x)}{\phi_{k,1}^{\rel}(y)}, \ldots, \iprod{\phi_{q, d_r}^{\rel}(x)}{\phi_{k, d_r}^{\rel}(y)}} \in \reals^{d_r}, \\
    r(x, y_i) &= \bigparen{\iprod{\phiqrell{\ell}(x)}{\phikrell{\ell}(y_i)}}_{\ell \in [d_r]}, \\
    (s_1, \ldots, s_n) &= \SymbolRetriever(\y;\,\Slib)
  \end{split}
\end{equation}

Thus, relational attention between the object $x$ and the context $\y = \ys$ retrieves a convex combination of the vectors $\{r(x, y_i)\}_{i=1}^{n}$, representing $x$'s relations with each object in the context. The relations sent are tagged with a symbol $s_i$ that identifies the object the relation is with. The role and implementation of the symbols will be discussed further in the next subsection. As before, $\phiqattn, \phikattn$ are learned feature maps that control the selection criterion for which object(s) in the context to attend to. Another set of query and key feature maps, $\phiqrell{\ell}, \phikrell{\ell}, \ell \in [d_r]$, are learned to represent the relation between the sender and the receiver. For each $\ell \in [d_r]$, the feature maps $\phiqrell{\ell}, \phikrell{\ell}$ extract specific attributes from the object pair which are compared by an inner product, producing a $d_r$-dimensional relation vector representing a fine-grained series of comparisons $\pparen{\iiprod{\phiqrell{\ell}(x)}{\phikrell{\ell}(y_i)}}_{\ell \in [d_r]}$ across different feature subspaces.

In certain tasks~\citep{kergNeuralArchitectureInductive2022,altabaa2024abstractors,altabaaLearningHierarchicalRelational2024}, a useful inductive bias on the relations function $r(\cdot, \cdot)$ is symmetry; i.e., $r(x, y) = r(y, x),\, \forall x, y$. This corresponds to using the same feature filter for the query and key maps, $\phiqrel = \phikrel$. This adds structure to the relation function, transforming it into a positive semi-definite kernel that defines a pseudometric on the object space, along with a corresponding geometry. %We explore this and expand on this discussion in our experiments.

\subsection{Symbol Assignment Mechanisms}

% For the purposes of processing relations, all the receiver needs to know is: 1) the relation between itself and the objects in its context, and 2) the identity of the object corresponding to each relation.
For the purposes of processing relations, the receiver needs to know: 1) the relation between itself and the objects in its context, and 2) the identity of the object corresponding to each relation.
The symbols $s_i$ in relational attention are used to tag each relation with the identity of the sender. %  (i.e., the object the relation involves).
% Without this information, the result of relational attention would only be an aggregated representation of the relations between the receiver and the selected object(s).

A symbol identifies or ``points to'' an object, but, importantly, does not fully encode the features of the object. The second point is key to making relational attention disentangled from sensory or object-level features. The sensory features of individual objects are high-dimensional and have high variability. By contrast, relational information is low-dimensional and more abstract. If sensory features are mixed with relational information, they may overwhelm the relational information, preventing abstraction and generalization. Instead, symbols act as abstract references to objects, which can be viewed as a connectionist analog of pointers in traditional symbolic systems.

The notion of the ``identity'' of an object can vary depending on context. In this work, we consider three possible types of identifiers: 1) position, 2) relative position, or 3) an equivalence class over features. For each type, we model a corresponding symbol assignment mechanism~\citep{altabaa2024abstractors}.

\textbf{Positional Symbols.} In some applications, it is sufficient to identify objects through their position in the input sequence. We maintain a library of symbols $\Slib = (s_1, \ldots, s_{\mathtt{max\_len}}) \in \reals^{\mathtt{max\_len} \times d_{\mathrm{model}}}$ and assign $s_i$ to the $i$-th object in the sequence. These are essentially learned positional embeddings.

\textbf{Position-Relative Symbols.} Sometimes, the more useful identifier is \textit{relative} position with respect to the receiver, rather than absolute position. This can be implemented with position-relative embeddings. We learn a symbol library $\Slib = (s_{-\Delta}, \ldots, s_{-1}, s_0, s_1, \ldots, s_{\Delta}) \in \reals^{(2 \Delta + 1) \times d_{\mathrm{model}}}$, and relational attention becomes $x_i' \gets \sum_{j} \alpha_{ij}  (r(x_i, x_j) \, W_r + s_{j-i} \,  W_s)$.%, with ``$m_{j \to i} = (r(x_i, x_j), s_{j-i})$''.

\textbf{Symbolic Attention.} In certain domains, some information about the objects' features is necessary for identifying them for the purposes of relational processing. Yet, to maintain a relational inductive bias, we would like to avoid sending a full encoding of object-level features. In symbolic attention, we learn a set of symbol vectors, $\Slib = (s_1, \ldots, s_{n_s}) \in \reals^{n_s \times d_{\mathrm{model}}}$ and a matching set of feature templates $\Flib = (f_1, \ldots, f_{n_s})$. We retrieve a symbol for each object by an attention operation that matches the input vectors $x_i$ against the feature templates $f_j$ and retrieves symbols $s_j$.
\begin{equation}
  \SymbolicAttn(\x) = \Softmax\bigparen{(\x \, W_q)\, \Flib^\top} \Slib.
\end{equation}
Here, $\Slib, \Flib, W_q$ are learned parameters. This can be thought of as implementing a learned differentiable ``equivalence class map'' over feature embeddings. Crucially, the number of symbols (i.e., feature equivalence classes) is \textit{finite}, which enables relational attention to still produce a relation-centric representation while tagging the relations with the necessary identifier.

We find that different symbol assignment mechanisms are more effective in different domains.

\aanote{add further discussion on symbol assignment mechanisms here (or somewhere else?)}

% \subsection{Approximation theory: What Class of Functions can Relational Attention Compute?}\label{ssec:approx}
\subsection{What Class of Functions can Relational Attention Compute?}\label{ssec:approx}

To get some intuition about the type of computation that relational attention can perform, we present the following approximation result. The following theorem states that relational attention can approximate any function on $\calX \times \calY^n$ that 1) selects an element in $\ys$, then 2) computes a relation with it. Both the selection criterion and the relation function are arbitrary, and the selection criterion is query-dependent. The formal statement and proof are given in~\Cref{sec:approx}.
\begin{theorem}[Informal]\label{theorem:func_class}
  Let $\mathrm{Select}: \calX \times \calY^n \to \calY$ be an arbitrary preference selection function, which selects an element among $\ys$ based on a query-dependent preorder relation $\{\preceq_x\}_{x \in \calX}$. Let $\mathrm{Rel}: \calX \times \calY \to \reals^{d_r}$ be an arbitrary continuous relation function on $\calX \times \calY$. There exists a relational attention module that approximates the function $\mathrm{Rel}(x, \mathrm{Select}(x, \y))$ to arbitrary precision.
\end{theorem}
