\begin{abstract}
  Relational reasoning is a central component of generally intelligent systems with robust and data-efficient inductive generalization capabilities. In this work, we distinguish between two types of information: \textit{sensory} information about the properties of individual objects, and \textit{relational} information about the relationships between objects. The neural attention mechanism of Transformers can be understood as controlling the flow of sensory information between objects. However, it does not explicitly model the flow of relational information. Recent empirical evidence has shown that existing neural architectures, including Transformers, struggle with tasks that involve relational reasoning. We propose an architectural extension of the Transformer framework, dubbed the \textit{Dual Attention Transformer (DAT)}, featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate \textit{DAT} on a diverse set of tasks ranging from previously-studied synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that incorporating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains.
\end{abstract}
