\begin{abstract}
  Relational reasoning is a central component of generally intelligent systems with robust and data-efficient inductive generalization capabilities. In this work, we distinguish between two types of information: \textit{sensory} information about the properties of individual objects, and \textit{relational} information about the relationships between objects. The neural attention mechanism of Transformers can be understood as controlling the flow of sensory information between objects. In particular, it does not explicitly model the flow of relational information. Recent empirical evidence has shown that existing neural architectures, including Transformers, struggle with tasks that involve relational reasoning. We propose an architectural extension of the Transformer framework, dubbed the \textit{Dual Attention Transformer (DAT)}, with two distinct forms of attention: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate \textit{DAT} on a diverse set of tasks ranging from previously-studied synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing, finding that the introduction of explicit relational computational mechanisms into the Transformer architecture confers significant performance benefits.
\end{abstract}
