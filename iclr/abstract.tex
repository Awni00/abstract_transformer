\begin{abstract}
  Relational reasoning is a central component of generally intelligent systems, enabling robust and data-efficient inductive generalization. Despite their successes, recent empirical evidence shows that existing neural architectures, including Transformers, struggle with tasks requiring relational reasoning. In this work, we distinguish between two types of information: \textit{sensory} information about the properties of individual objects, and \textit{relational} information about the relationships between objects. While neural attention provides a powerful mechanism for controlling the flow of sensory information between objects, the Transformer lacks an explicit computational mechanism for routing and processing relational information. To address this limitation, we propose an architectural extension of the Transformer framework, dubbed the \textit{Dual Attention Transformer (DAT)}, featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate \textit{DAT} on a diverse set of tasks ranging from synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that integrating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains in terms of data efficiency and parameter efficiency.
\end{abstract}
