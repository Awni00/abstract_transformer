\section{Discussion}\label{sec:discussion}

\textbf{Summary.} The standard attention mechanism of Transformers provides a versatile mechanism for retrieval of sensory information in any given context, but does not explicitly support retrieval of relational information. In this work, we presented an extension of the Transformer architecture that disentangles and integrates sensory and relational information through a variant of multi-head attention with two distinct types of attention heads: standard self-attention for sensory information and a novel \textit{relational attention} mechanism for relational information. We empirically evaluate this architecture and find that it yields performance improvements across a range of tasks and modalities.

\textbf{Limitations.} The proposed architecture introduces several hyperparameters and possible configurations. Although we carried out ablations on the major configuration choices (e.g., composition of head types, symmetry, symbol assignment mechanism), a more thorough empirical examination would help develop an improved understanding of the behavior of this architecture under different configurations. Such a systematic study may also enable the discovery of further modifications to improve the architecture. We also note that our implementation of the Dual-Attention Transformer lacks the hardware-aware optimizations of standard Transformers~\citep{dao2022flashattention}, making it slower. However, we expect that similar optimizations are possible for dual attention and \textit{DAT}.